<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Recommendation System - Long User Sequence Modeling" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a simple review on the evolution of user sequence modeling work, especially long user sequence modeling. Hope this blog could inspire broader exploration ideas for the future." /><meta property="og:description" content="User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a simple review on the evolution of user sequence modeling work, especially long user sequence modeling. Hope this blog could inspire broader exploration ideas for the future." /><link rel="canonical" href="https://pyemma.github.io/Long-User-Sequence-Modeling-In-Recsys/" /><meta property="og:url" content="https://pyemma.github.io/Long-User-Sequence-Modeling-In-Recsys/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-06-28T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Recommendation System - Long User Sequence Modeling" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2025-06-28T00:00:00-07:00","datePublished":"2025-06-28T00:00:00-07:00","description":"User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a simple review on the evolution of user sequence modeling work, especially long user sequence modeling. Hope this blog could inspire broader exploration ideas for the future.","headline":"Recommendation System - Long User Sequence Modeling","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/Long-User-Sequence-Modeling-In-Recsys/"},"url":"https://pyemma.github.io/Long-User-Sequence-Modeling-In-Recsys/"}</script><title>Recommendation System - Long User Sequence Modeling | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Recommendation System - Long User Sequence Modeling</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Recommendation System - Long User Sequence Modeling</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1751094000" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jun 28, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2447 words" > <em>13 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a simple review on the evolution of user sequence modeling work, especially long user sequence modeling. Hope this blog could inspire broader exploration ideas for the future.</p><p>Here is a quick outlets on the papers that we are going to discuss about today</p><ul><li><a href="https://arxiv.org/pdf/2302.02352">TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou</a><li><a href="https://arxiv.org/pdf/2407.16357">TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou</a><li><a href="https://arxiv.org/pdf/2306.00248">TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest</a><li><a href="https://arxiv.org/pdf/2506.02267">TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation</a><li><a href="https://www.arxiv.org/pdf/2505.04421">LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders</a><li><a href="https://arxiv.org/pdf/2506.00450">DV365: Extremely Long User History Modeling at Instagram</a></ul><p>Now, let’s begin our journey with the <em>ancient world</em>.</p><h3 id="ancient-world"><span class="me-2">Ancient World</span><a href="#ancient-world" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>User sequence essentially is a chronologic sequence of collaborative signals collected from certain domain, e.g. the feed posts that a user has viewed, clicked and commented or the ads that a user has clicked. In the past, these signals are usually processed in a <em>reduction</em> format, which means that we would drop the chronologic information and use a simple <em>pooling</em> strategy to fuse the information together, similar to the example below.</p><p><a href="/assets/embedding-lookup.png" class="popup img-link shimmer"><img src="/assets/embedding-lookup.png" alt="User Sequence Reduction" loading="lazy"></a></p><p>There are some issues within this approach:</p><ul><li>The lost of chronologic information leads to the loss of granularity on user’s <em>short term</em> and <em>long term</em> interest. There is some solution such as introducing recency wight to put more emphasis on user’s recent behavior, but still might lead to suboptimal representation of user<li>The simple <em>pooling</em> strategy leads us not able to learn too much useful signals from user’s sequence behavior. For example, after seeing a video of NBA, user might visit sports shop for basketball. Such relationship is implicitly encoded within user’s behavior but could not be learnt once they are aggregated together.</ul><p>There are some work to overcome the <em>reduction</em> style of user sequence handling in the past. For example, in DIN the <strong>attention</strong> mechanism (more formally, it is <strong>target attention</strong>) is introduced to selective find the items most <em>relevant</em> to the current ranking candidates from user’s behavior history. This helps the model to use the most effective portion of information from user’s past behavior, instead of letting those signals being averaged out.</p><h3 id="modern-world"><span class="me-2">Modern World</span><a href="#modern-world" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Transformer</strong> architecture has revolutionized the NLP world. Due to its superiority in terms of modeling relationship among tokens and inference efficiency compared to RNN, researchers also start to apply this technique into user sequence modeling. One of the most representative work is Pinterest’s <a href="https://arxiv.org/pdf/2306.00248">TransAct</a> model.</p><p>Before introducing TransAct, there are some common properties I have summarized from the papers I have read. I plan to use this as an architecture to explain the works that is going to be introduced in this post:</p><ul><li><strong>Token representation</strong>: which refers to how the sequence is represented. For example, is each token a composition of several different embeddings, or there are <em>different types of token</em> (similar to multi modality) in the sequence<li><strong>Sequence length</strong>: the time span of the user past behavior to use. For example, use past 2 years history, or just recent history<li><strong>Attention type</strong>: multi head attention or cross attention<li><strong>Dimension reduction</strong>: mainly adopted for long sequence scenarios to reduce the computation time complexity. For example, use search in the sequence or directly compress the sequence to shorter length</ul><p>The idea in <em>TransAct</em> is relative straight-forward: applying transformer encoder to user’s recent (in their paper, they called it as real-time) sequence and forwarding the output from transformer block to downstream of the model (TransAct is being a module within the entire model, other component adopted is DCNv2).</p><p><a href="/assets/transact.png" class="popup img-link shimmer"><img src="/assets/transact.png" alt="TransAct Module" width="600" height="400" loading="lazy"></a></p><ul><li><strong>Token representation</strong>: The token contains the positive actions from user’s past behaviors, e.g. pins clicked or shared. Each token is a composition of 3 parts, the action embedding, the interacted item embedding and the candidate embedding. The action embedding is learned during the training and the interacted item embedding and candidate embedding is from <em>PinFormer</em>, which could be viewed as <em>static</em>. These 3 embeddings are concatenated to form the final token embeddings (they compared using sum of embeddings and production the concat version).<li><strong>Sequence length</strong>: only the latest 100 user actions is used due to the training &amp; serving cost at the time of the work; padding is used if there is no 100 actions from the user (e.g. cold-start users). Since the sequence is pretty short and to avoid model over-fitting on the last user actions, they introduced a random mask based on the request timestamp (masked attention).<li><strong>Attention type</strong>: self attention is adopted as only the <strong>encoder</strong> of <em>transformer</em> is used; note that the <em>cross</em> part of the candidate item with items user historically interacted with is implicitly handled because of the token representation.<li><strong>Dimension reduction</strong>: no dimension reduction is adopted due to the relative short length used.</ul><p>The last K output of the transformer blocker + max polling of all token’s hidden representation is used as the input to the remaining part of the model; it is flattened and concatenated with other vectors such as embedding lookup output of sparse features.</p><h3 id="towards-longer-sequence"><span class="me-2">Towards Longer Sequence</span><a href="#towards-longer-sequence" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Due to $O(N^2)$ time complexity of the transformer blocker, it is pretty challenging to really scale to longer sequence. To make it work in practice, different solutions from infra or from modeling has been proposed.</p><p><a href="https://arxiv.org/pdf/2302.02352"><em>TWIN</em></a> is one of the pioneer to scale user sequence from $10^2$ to $10^4$ so that we could model user’s lifelong behaviors.</p><ul><li><strong>Token representation</strong>: The feature associated with each item is converted to categorical and go through the embedding lookup to convert to the dense vector format.<li><strong>Sequence length</strong>: There are 2 stages in <em>TWIN</em> framework. The first stage is a <em>general search</em> stage where the input sequence length is $10^4$, and the second stage is <em>exact search</em> stage where the input sequence is $10^2$, which is the top items selected from the <em>general search</em> stage. This is similar to the retrieval-ranking mutli-stage arch in traditional recommendation system.<li><strong>Attention type</strong>: Multi-head target attention is adopted. This is still similar to DIN’s cross attention, but use different projection to transform the QKV into multiple heads so that each head could learn different aspect of the hidden representation.<li><strong>Dimension reduction</strong>: The technique adopted is still search style, which is <em>top-k attention scores</em>. In the <em>general search</em> stage, the candidate item is used as query to perform multi-head cross attention with $10^4$ user history interactions. And the top $10^2$ scores are selected and sent to <em>exact search</em> stage.</ul><p><a href="/assets/twin.png" class="popup img-link shimmer"><img src="/assets/twin.png" alt="TWIN model architecture" loading="lazy"></a></p><p>To make the computation in the <em>general search</em> more efficient, a <em>feature decomposition</em> is adopted to enable precompute &amp; cache</p><ul><li>For each token representation, it is decomposed as a <em>item specific</em> section and a <em>item-user cross</em> section; each section is associated with a projection matrix $W$; the <em>item specific</em> part is still used as normal attention computation, and the <em>item-user cross</em> part is modeled as a bias term to be added to the attention scores<li>After offline training, the project matrix of the <em>item specific</em> could be used to precompute and cached into the <em>offline inference service</em>. This cache is updated with the latest embeddings of items and latest project matrix synchronized from the training system to minimize the staleness of the result.<li>Once there is a request comes in, the offline inference server could return the precomputed result for user’s history sequence and the remaining computation is done on the fly.<li>This removes the major computation bottle neck in <em>TWIN</em> which is the projection of $10^4$ user sequence.</ul><p><a href="/assets/twin-infra.png" class="popup img-link shimmer"><img src="/assets/twin-infra.png" alt="TWIN infra architecture" loading="lazy"></a></p><p><strong>LONGER</strong> is another long user sequence work from Bytedance and it used a different approach to reduce the sequence length. Also the overall architecture is similar to <a href="https://arxiv.org/pdf/2402.17152">HSTU</a>.</p><ul><li><strong>Token representation</strong>: Different type of features going through the same shared embedding layer, and then with the addition of position embedding. To reduce the token length, a <em>token merge</em> strategy is used here to merge adjacency tokens into a single one. <em>InnerTrans</em> block is used for this merging so that local information is still preserved after token merge.<li><strong>Sequence length</strong>: Over $10^3$ length of user history items. The construction of the sequence not only include the user history interactions, but also include the candidate item features and user profile features, which is used for construction as <em>global tokens</em> to interact with all all user history behaviors.<li><strong>Attention type</strong>: In the first layer, <em>causal cross attention</em> is used and regular <em>causal self attention</em> is used for the remaining layers. In <em>causal cross attention</em>, the <em>global token</em> is used as the query, along with several items retrieved from user’s behavior sequence (they find using the most recent k items yield best performance).<li><strong>Dimension reduction</strong>: As mentioned, the primary reduction strategy is through <strong>compression</strong>. <em>Token merge</em> is one layer of compression, <em>cross attention</em> on selective query tokens are another layer of compression to reduce the sequence length.</ul><p><a href="/assets/longer.png" class="popup img-link shimmer"><img src="/assets/longer.png" alt="LONGER model architecture" loading="lazy"></a></p><p>Recently Pinterest also upgrade their <em>TransAct</em> to <a href="https://arxiv.org/pdf/2506.02267"><em>TransAct V2</em></a> to scale user sequence from $10^2$ to $10^4$.</p><ul><li><strong>Token representation</strong>: Static embedding is still leveraged as input (from PinSage) and candidate’s embedding is still append to each user interacted items’ embedding. However, the action embedding is not concatenated but added. Besides action embedding, surface embedding and timestamp embedding (as position) is also introduced, and also added with the item embedding.<li><strong>Sequence length</strong>: 3 sequences are introduced. Lifelong sequence length is 90th percentile of user’s past 2 years history, which is at $10^4$ scale. Realtime sequence which contains user’s latest interaction sequence scales at $10^2$ level. Impression sequence which contains user’s negative interaction (no action from users) scales at $10^2$.<li><strong>Attention type</strong>: Similar to <em>TransAct</em>.<li><strong>Dimension reduction</strong>: Nearest neighbor search against the candidate item is used to reduce the sequence length for all 3 sequences. After NN the sub-sequences are concatenated together to go through the transformer encoder.</ul><p><a href="/assets/transact-v2.png" class="popup img-link shimmer"><img src="/assets/transact-v2.png" alt="TransAct V2 model architecture" loading="lazy"></a></p><p>Another modeling improvement in <em>TransAct v2</em> is to adopt <em>contrastive learning</em> to enhance the representation learning. For the hidden representation for timestamp $t$ (not that due to the causal attention ), the $t+1$ item from the realtime sequence is used as the positive samples and the 2 representation are pushed closer; while random negatives are sampled from the impression sequence to be pushed away.</p><h3 id="sequence-length-"><span class="me-2">Sequence Length ++</span><a href="#sequence-length-" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Although $10^4$ is already a pretty long sequence, researchers do not stop their effort to scale to even longer sequence.</p><p><em>TWIN v2</em> is one upgrade of the <em>TWIN</em> algorithm and scale the sequence from $10^4$ to $10^5$. The majority of the components do not change except for the part of dimension reduction.</p><ul><li><strong>Token representation</strong>: Cluster embedding which is computed through the hierarchy K-means algorithm.<li><strong>Dimension reduction</strong>: Besides the <strong>general search</strong> and <strong>exact search</strong> unit similar in <em>TWIN</em>, one addition <em>clustering</em> based sequence reduction process is adopted to mitigate the scaling challenge. The clustering algorithm is done in 2 stage:<ul><li>In the first stage, item in users’s sequence is grouped via a heuristic approach. In the work, they group the items based on the percentage of the consumption of the video by the user.<li>Within each group, a K-means algorithm is used to cluster the item into several clusters. The newly formed cluster would go through another round of K-means if the number of item within the cluster is higher than certain threshold; once the number of item drops below the threshold, this cluster is finalized and would be moved out from the process and append to global result. The item embedding used for the K-means is from the recommendation model, which means it is using <em>collaborative signals</em>.<li>Eventually, user’s original sequence would be converted to a sequence of clusters. And the mean pooling of all items in the cluster is used as the representation of the cluster (which is going to be the new <em>token representation</em>).</ul></ul><p><a href="/assets/twin-v2.png" class="popup img-link shimmer"><img src="/assets/twin-v2.png" alt="TWIN V2 model architecture" loading="lazy"></a></p><p>The last work is from Instagram, which is called <a href="https://arxiv.org/pdf/2506.00450"><em>DV365</em></a>. The sequence length is also scaled to $10^5$, where the longest one is at 70000 and average is 40000. This work is used as a <em>foundation model</em> to generate high performant user profiling embeddings which is used as input to other downstream models. This is a relative <em>disaggregated</em> view compared to the work we have mentioned above.</p><ul><li><strong>Token representation</strong>: Carefully manual crafted &amp; selected features are used as the token representation. The features are bucketized (categorized) and then converted to dense representation via the embedding lookup.<li><strong>Sequence length</strong>: $10^5$ scale of length. User sequence are constructed in 2 different format, one is explicit sequence which contains users’ action such as click; and the other one is implicit sequence which contains users’ implicit reaction such as video duration &amp; dwell time.<li><strong>Attention type</strong>: Funnel transformer is adopted, which pools in token dimension to reduce the sequence length in later stage of model. Linear compression is also adopted to compress the original sequence input and combined with the final output from funnel transformer.<li><strong>Dimension reduction</strong>: No other reduction technique is used such as item selection or clustering.</ul><p><a href="/assets/dv365.png" class="popup img-link shimmer"><img src="/assets/dv365.png" alt="DV365" loading="lazy"></a></p><h3 id="is-this-end-of-era"><span class="me-2">Is this end of era</span><a href="#is-this-end-of-era" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The landscape of user sequence modeling has been fundamentally changed by transformer architecture and more powerful hardwares. Besides this traditional view of the user sequence modeling where it is treated as a <em>feature processor</em> or <em>feature generator</em>, there is also another disruptive stream in the recommendation area, which is <strong>Generative Recommendation</strong>. In <strong>GR</strong>, the input sequence is already changed from impression level to member level, and consume the member sequence directly as the input to predict the next item that user is likely to interact with. This is an interesting and active area, stay tune for my upcoming post!</p><p>Is <strong>GR</strong> going to be the killer for the traditional user sequence modeling work? Yes and no, and actually these 2 domain synergy pretty well with each other:</p><ul><li>Both needs to handle the scale of the user sequence. Right now in GR the raw sequence is still the primary choice, but we could see that lots of dimension reduction techniques used in user sequence modeling could be applied to GR as well.<li>Item representation is shared. How to synergy collaborative embedding, semantic embedding, and even multi-modal in the sequence representation would still be the key.<li>Inference challenging is the same. Lots of infra optimization work needs to be done to make it for online. Also how to enable <em>incremental training</em> and <em>online training</em> is also a challenging task.</ul><blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote><p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p><h3 id="acknowledgment"><span class="me-2">Acknowledgment</span><a href="#acknowledgment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>This blog is inspired from a group discussion with several Daolao: Yunzhong, Daqi, Zeyu, Lili, Michael, Qiaqia. Appreciate their generous sharing idea and insights.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/machine-learning-design/" class="post-tag no-text-decoration" >machine learning design</a> <a href="/tags/recommendation-system/" class="post-tag no-text-decoration" >recommendation-system</a> <a href="/tags/user-sequence-modeling/" class="post-tag no-text-decoration" >user-sequence-modeling</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Recommendation%20System%20-%20Long%20User%20Sequence%20Modeling%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FLong-User-Sequence-Modeling-In-Recsys%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Recommendation%20System%20-%20Long%20User%20Sequence%20Modeling%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FLong-User-Sequence-Modeling-In-Recsys%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FLong-User-Sequence-Modeling-In-Recsys%2F&text=Recommendation%20System%20-%20Long%20User%20Sequence%20Modeling%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/Book-PyTorch-Training-Optimization/">PyTorch 性能与显存优化手册</a><li class="text-truncate lh-lg"> <a href="/Long-User-Sequence-Modeling-In-Recsys/">Recommendation System - Long User Sequence Modeling</a><li class="text-truncate lh-lg"> <a href="/Machine-Learning-System-Design-Sparse-Features/">Trend of Sparse Features in Recommendation System</a><li class="text-truncate lh-lg"> <a href="/Recsys-Paper-Review-2025-Q1/">Recsys Paper Summary 2025 Q1</a><li class="text-truncate lh-lg"> <a href="/How-to-design-slack/">How to Design Slack</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Recsys-Paper-Review-2025-Q1/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1745046000" data-df="ll" > Apr 19, 2025 </time><h4 class="pt-0 my-2">Recsys Paper Summary 2025 Q1</h4><div class="text-muted"><p>In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here i...</p></div></div></a></article><article class="col"> <a href="/Recsys-2024-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1735977600" data-df="ll" > Jan 4, 2025 </time><h4 class="pt-0 my-2">Recsys 2024 Paper Summary</h4><div class="text-muted"><p>In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research...</p></div></div></a></article><article class="col"> <a href="/Machine-Learning-System-Design-Sparse-Features/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1730617200" data-df="ll" > Nov 3, 2024 </time><h4 class="pt-0 my-2">Trend of Sparse Features in Recommendation System</h4><div class="text-muted"><p>In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as revi...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/Recsys-Paper-Review-2025-Q1/" class="btn btn-outline-primary" aria-label="Older" ><p>Recsys Paper Summary 2025 Q1</p></a> <a href="/Book-PyTorch-Training-Optimization/" class="btn btn-outline-primary" aria-label="Newer" ><p>PyTorch 性能与显存优化手册</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/Long-User-Sequence-Modeling-In-Recsys/'; this.page.identifier = '/Long-User-Sequence-Modeling-In-Recsys/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
