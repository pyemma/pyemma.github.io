<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="KDD 2025 Paper Summary" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="It has been a while since KDD 2025, finally I have had sometime to finish reading all papers that I interested in and summarize some of my learnings in this post :sweat_smile:. My primary focus is still on the work related to recommendation system from industry track, especially the area of user sequence modeling and the integration of LLM in recsys. Below if the follow list of papers covered in this post" /><meta property="og:description" content="It has been a while since KDD 2025, finally I have had sometime to finish reading all papers that I interested in and summarize some of my learnings in this post :sweat_smile:. My primary focus is still on the work related to recommendation system from industry track, especially the area of user sequence modeling and the integration of LLM in recsys. Below if the follow list of papers covered in this post" /><link rel="canonical" href="https://pyemma.github.io/KDD-2025-Paper-Summary/" /><meta property="og:url" content="https://pyemma.github.io/KDD-2025-Paper-Summary/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-10-17T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="KDD 2025 Paper Summary" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2025-10-17T00:00:00-07:00","datePublished":"2025-10-17T00:00:00-07:00","description":"It has been a while since KDD 2025, finally I have had sometime to finish reading all papers that I interested in and summarize some of my learnings in this post :sweat_smile:. My primary focus is still on the work related to recommendation system from industry track, especially the area of user sequence modeling and the integration of LLM in recsys. Below if the follow list of papers covered in this post","headline":"KDD 2025 Paper Summary","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/KDD-2025-Paper-Summary/"},"url":"https://pyemma.github.io/KDD-2025-Paper-Summary/"}</script><title>KDD 2025 Paper Summary | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a>
</h1>
<p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta &amp; LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav">
<li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a>
</li>
<li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a>
</li>
<li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a>
</li>
<li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a>
</li>
<li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a>
</li>
</ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer"> <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href%20=%20'mailto:'%20+%20['pyemma1991','gmail.com'].join('@')" aria-label="email"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss"> <i class="fas fa-rss"></i> </a>
</div></aside><div id="main-wrapper" class="d-flex justify-content-center">
<div class="container d-flex flex-column px-xxl-5">
<header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100">
<nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>KDD 2025 Paper Summary</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div>
<button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
</div></header><div class="row flex-grow-1">
<main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>KDD 2025 Paper Summary</h1>
<div class="post-meta text-muted"> <span> Posted <time data-ts="1760684400" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom"> Oct 17, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2706 words"> <em>15 min</em> read</span>
</div>
</div>
<div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" alt="Views" loading="lazy"></a> </span>
</div>
</div></header><div class="content">
<p>It has been a while since KDD 2025, finally I have had sometime to finish reading all papers that I interested in and summarize some of my learnings in this post <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">. My primary focus is still on the work related to recommendation system from industry track, especially the area of user sequence modeling and the integration of LLM in recsys. Below if the follow list of papers covered in this post</p>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3711896.3736893">Contrastive Text-enhanced Transformer for Cross-Domain Sequential Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2411.15005">Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3711896.3737193">Applying Large Language Model For Relevance Search In Tencent</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3711896.3737204">Complicated Semantic Alignment for Long-Tail Query Rewriting in Taobao Search Based on Large Language Model</a></li>
<li><a href="https://arxiv.org/pdf/2501.08695">Real-time Indexing for Large-scale Recommendation by Streaming Vector Quantization Retriever</a></li>
<li><a href="https://arxiv.org/pdf/2408.05430">HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3711896.3737275">Aligning and Balancing ID and Multimodal Representations for Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2405.12327">Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems</a></li>
<li><a href="https://dl.acm.org/doi/10.1145/3711896.3737003">Improving Long-tail User CTR Prediction via Hierarchical Distribution Alignment</a></li>
<li><a href="https://arxiv.org/pdf/2506.18309">LettinGo: Explore User Profile Generation for Recommendation System</a></li>
<li><a href="https://arxiv.org/pdf/2506.01375">Generative Next POI Recommendation with Semantic ID</a></li>
<li><a href="https://arxiv.org/abs/2506.03699">Scaling Transformers for Discriminative Recommendation via Generative Pretraining</a></li>
<li><a href="https://arxiv.org/abs/2506.04699">Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling</a></li>
<li><a href="https://arxiv.org/abs/2405.18113">MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting</a></li>
</ul>
<h3 id="sequential-modeling">
<span class="me-2">Sequential Modeling</span><a href="#sequential-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="contrastive-text-enhanced-transformer-for-cross-domain-sequential-recommendation">
<span class="me-2">Contrastive Text-enhanced Transformer for Cross-Domain Sequential Recommendation</span><a href="#contrastive-text-enhanced-transformer-for-cross-domain-sequential-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This work is a fusion of combining embedding alignment with cross domain together. In cross domain, there is one source domain which is usually with rich user behavior data and one target domain which is with limited behavior data. The goal is to see if there are certain underlying structure hidden behind these 2 domains so that we could learn user profile in target domain better with the insight we gain from source domain. And this source domain -&gt; target domain flow is similar to what the embedding alignment adopted for multimodal embedding learnings.</p>
<ul>
<li>For each input item, there is one text encoder to convert the rich text information as text embeddings; as well as a native embedding lookup table to convert item id to id embeddings (which would be learnt during training)</li>
<li>A complicated alignment between the text embeddings and id embeddings is adopted. First, a cross attention is used: the source domain’s text embedding is used as <code class="language-plaintext highlighter-rouge">K</code> and id embedding (which is rich in collaboration signal because of more data) is used as <code class="language-plaintext highlighter-rouge">V</code>, the target domain text embedding is used as <code class="language-plaintext highlighter-rouge">Q</code>; Second, self attention is applied to both text embedding and id embedding individually in the target domain; Third, the attention scores of <code class="language-plaintext highlighter-rouge">attn(target_text_embedding, target_text_embedding)</code> and <code class="language-plaintext highlighter-rouge">attn(target_text_embedding, source_text_embedding)</code> is aligned through an operator before aggregation with <code class="language-plaintext highlighter-rouge">V</code>
</li>
<li>The loss adopted is InfoNCE, users’ positive action in different domain is used as positive pairs, and other users in the same domain is used as negative pairs</li>
<li>Eventually, both the user representation learnt from source domain and target domain would be concat together, and use for the traditional recommendation task</li>
</ul>
<p><a href="/assets/kdd-2025-cross-domain-alignment.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-cross-domain-alignment.png" alt="Contrastive Text-enhanced Transformer for Cross-Domain Sequential Recommendation" loading="lazy"></a></p>
<p>The main take away from this work is how the alignment is done: besides the classic cross attention on semantic rich embeddings, contrastive learning, the original attention scores are also used.</p>
<h4 id="multi-granularity-interest-retrieval-and-refinement-network-for-long-term-user-behavior-modeling-in-ctr-prediction">
<span class="me-2">Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction</span><a href="#multi-granularity-interest-retrieval-and-refinement-network-for-long-term-user-behavior-modeling-in-ctr-prediction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>The main problem addressed in this work is related to long user behavior sequence. And the solution proposed is pretty similar to what have been adopted in TransAct V2 or OneRec, where multiple user behavior sequence sampled through different strategy is used as input to the model.</p>
<ul>
<li>Instead of use a single user behavior sequence, 3 sequences are used and these sequences is constructed through a query against user’s behavior sequence: First, the target item is used as query to search across user’s sequence, and <strong>SimHash</strong> is used here for quick similarity computation instead of dot product; user’s recent behavior is converted to a query via pooling of hidden state learnt from GRU; user’s life long sequence is clustered based on DPC algorithm, and use target item to query which cluster is most close</li>
<li>Besides the sequence, fourier transformation is also applied to the sequence dimension to reduce the attention computation time complexity from <code class="language-plaintext highlighter-rouge">O(n^2)</code> to <code class="language-plaintext highlighter-rouge">O(nlogn)</code>; I understand the high level motivation but didn’t quite understand the implementations <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">
</li>
</ul>
<p><a href="/assets/kdd-2025-long-seq.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-long-seq.png" alt="Multi-granularity Interest Retrieval and Refinement Network for Long-Term User Behavior Modeling in CTR Prediction" loading="lazy"></a></p>
<p>Using multiple sequence instead of single sequence to enrich the semantic and context might be a good approach? Instead of use 3N single length sequence, use 3 sequences with length N might offer similar performance but much better compute efficiency.</p>
<h3 id="llm--search--recommendation">
<span class="me-2">LLM &amp; Search | Recommendation</span><a href="#llm--search--recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="applying-large-language-model-for-relevance-search-in-tencent">
<span class="me-2">Applying Large Language Model For Relevance Search In Tencent</span><a href="#applying-large-language-model-for-relevance-search-in-tencent" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This works from Tencent mainly explored how to apply LLM for the relevance task in search engine. This work is integrated into QQ web browser’s default search engine.</p>
<ul>
<li>Given a query and a document, a prompt is provided to LLM to predict the probability of the relevance scores between the query and the document, and the probability is used as weight to compute a final rank scores.</li>
<li>Besides the pure query and document, certain information from the traditional search engine, such as intent and tags from query understanding component or document features, is added as tokens into prompt to improve LLM’s capability on relevance task.</li>
<li>One additional exploration in the work is to use LLM as judge or LLM synergy negative examples to improve the BERT model widely used in search engine.</li>
<li>Knowledge distillation from LLM to BERT is also explored. The way is to transform the <code class="language-plaintext highlighter-rouge">CLS</code> token in BERT via one additional head to match to the output from LLM head, and then do the logits level distillation</li>
</ul>
<h4 id="complicated-semantic-alignment-for-long-tail-query-rewriting-in-taobao-search-based-on-large-language-model">
<span class="me-2">Complicated Semantic Alignment for Long-Tail Query Rewriting in Taobao Search Based on Large Language Model</span><a href="#complicated-semantic-alignment-for-long-tail-query-rewriting-in-taobao-search-based-on-large-language-model" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This works from TaoBao introduced how LLM is leveraged for search query rewrite, especially for the long-tail queries. The main problem in this scenario is that the query from user might be pretty different from merchant’s description of the item. How to write the query so that the language of users and merchants could be on the “same page”, while still perverse users’ original intention, is a challenging problem. The solution in this work proposed is to fine tune LLM for query rewriting and alignment.</p>
<ul>
<li>A high quality SFT dataset for query rewrite is proposed. Starting from the search log, several long tail queries are identified via certain heuristic rules; then leverage LLM to rewrite query and human to annotate the quality of the rewrite; during the generation of rewrite, a RAG-styled prompt technique is used</li>
<li>One alignment dataset for user query and merchant product description is also provided. Based on the SFT LLM, several rewrite query is generated and human to annotate if the rewritten query still captures user’s original intent; and the rewritten query is also send into query engine for retrieval, and the recall volume is treated as the feedback</li>
</ul>
<h4 id="aligning-and-balancing-id-and-multimodal-representations-for-recommendation">
<span class="me-2">Aligning and Balancing ID and Multimodal Representations for Recommendation</span><a href="#aligning-and-balancing-id-and-multimodal-representations-for-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>One work from Kuaishou about multimodal embedding and id embedding alignments. The technique used in multimodal is relative standard. The novel part is how multimodal embedding and id embedding is aligned. Instead of using the traditional cosine distance, Wasserstein distance is used. Another technique they proposed is a gradient module to dynamically adjust the predict power of the id embedding and multimodal embedding, to avoid one overwhelm the other during training.</p>
<h4 id="lettingo-explore-user-profile-generation-for-recommendation-system">
<span class="me-2">LettinGo: Explore User Profile Generation for Recommendation System</span><a href="#lettingo-explore-user-profile-generation-for-recommendation-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>User profiling is one critical parts in recommendation system, it helps the system to understand user’s interest so that more matching items could be delivered to users. Traditionally user profiling is learnt from data and usually captured via embeddings (e.g. an embedding lookup table for user ids).</p>
<p>In this work, LLM is used to summarize a user profile in text based on user’s history. Different LLM is leveraged to generate a diversified user profiles (this is called profile exploration stage). Once the user profiles are generated, they are sent into recommendation model for prediction, along with user’s history and a target item. The model’s prediction and the ground truth could be collected as positive and negative samples for DPO fine tuning.</p>
<h4 id="improving-long-tail-user-ctr-prediction-via-hierarchical-distribution-alignment">
<span class="me-2">Improving Long-tail User CTR Prediction via Hierarchical Distribution Alignment</span><a href="#improving-long-tail-user-ctr-prediction-via-hierarchical-distribution-alignment" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This work from Kuaishou main focus on the long tail user issue in recommendation system. Because of the data sparsity, it is challenging to learn good knowledge/insights about long tail users.</p>
<ul>
<li>In this work, the problem is modeled similar to transfer learning, where the head users and long tail users are treated as 2 different domains, and we leverage the rich knowledge about head users to boost the learnings of long tail users. To extract the shared knowledge among head users and long tail users, a <strong>feature probabilistic mapping</strong> (which transforms the original input feature into another format) is used for feature decoupling, and adversarial learning is adopted for representation learning.</li>
<li>The training data is reweighted to balance the bias introduced due to downsampling of head users data. The weight is computed within each batch. And a dedicated Beta calibration is introduced due to the CTR sensitive task (trained on an individual dataset)</li>
</ul>
<p>The <strong>feature probabilistic mapping</strong> is a new technique to me, plan to learn more details about this and see if it could be applied to the project that I’m working on.</p>
<p><a href="/assets/kdd-2025-long-tail-ctr.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-long-tail-ctr.png" alt="Improving Long-tail User CTR Prediction via Hierarchical Distribution Alignment" loading="lazy"></a></p>
<h4 id="scaling-transformers-for-discriminative-recommendation-via-generative-pretraining">
<span class="me-2">Scaling Transformers for Discriminative Recommendation via Generative Pretraining</span><a href="#scaling-transformers-for-discriminative-recommendation-via-generative-pretraining" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This works from Alibaba applied pretraining technique widely used in LLM training to CTR prediction model training to mitigate overfit problem.</p>
<ul>
<li>A foundation model is first pre-trained similar to how GPT is trained: user sequence item id is used as input tokens and use next token prediction as the loss (sampled softmax is used due to the large vocab size); other feature such as category features are converted to embeddings and sum into id embeddings. During NTP, these features are also being predicted as part of the loss.</li>
<li>The final ranking model is using transformer architecture, the best transfer strategy discovered is <strong>Full Transfer + Freeze Sparse</strong>, where all parameters from the pretrained model is transferred and all sparse parameters (e.g. id embeddings) are frozen and not updated during training.</li>
</ul>
<p><a href="/assets/kdd-2025-pretrain.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-pretrain.png" alt="Scaling Transformers for Discriminative Recommendation via Generative Pretraining" loading="lazy"></a></p>
<h3 id="semantic-id">
<span class="me-2">Semantic Id</span><a href="#semantic-id" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<p>Semantic id is becoming more and more popular. It offers infra benefits compared to the original dense embeddings. In my opinion, it is critical to generative recommender as it is a more efficient and compact tokens to represent items compared to plain text.</p>
<h4 id="real-time-indexing-for-large-scale-recommendation-by-streaming-vector-quantization-retriever">
<span class="me-2">Real-time Indexing for Large-scale Recommendation by Streaming Vector Quantization Retriever</span><a href="#real-time-indexing-for-large-scale-recommendation-by-streaming-vector-quantization-retriever" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This work from Bytedance discussed about the idea of adopting Vector Quantization (VQ) to retrieval index, which outperforms the traditional way of embedding index such as HNSW.</p>
<ul>
<li>During the training, still using the classic tow tower model to model user and item embeddings separately. Additionally, each item embedding would go through the VQ stage and top-1 neighbor is selected as cluster. Once item embedding is updated, the cluster embedding would not be updated immediately (via stop gradient op), and a moving averaging is used to compute cluster embedding based on the items assigned to this cluster.</li>
<li>During serving, the cluster is first ranked based on the target item, only item from the top cluster would be considered and move to the next stage.</li>
<li>The loss is compounded by two parts, one is computed over the cluster embedding and user embedding; and the other is the computed over the original item embedding and user embedding.</li>
<li>The benefit of this approach is that the index update time is essentially the same as the model update time; since the cluster assignment happened during the model training stage; also, compared to FAISS, the cluster embedding is optimized the downstream task.</li>
<li>For long tail items, a dedicated candidate stream (instead of the impression stream used for online training) is used to make sure that the long tail items is being assigned to the right clusters based on latest cluster embeddings learnt.</li>
</ul>
<p><a href="/assets/kdd-2025-vq-index.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-vq-index.png" alt="Real-time Indexing for Large-scale Recommendation by Streaming Vector Quantization Retriever" loading="lazy"></a></p>
<h4 id="generative-next-poi-recommendation-with-semantic-id">
<span class="me-2">Generative Next POI Recommendation with Semantic ID</span><a href="#generative-next-poi-recommendation-with-semantic-id" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>A work that leverages semantic id for POI recommendation. This could be a good example to learn how to integrate semantic id into recommendation model.</p>
<ul>
<li>The input used for semantic generation includes category, region code, timestamp bucket and user collaborative signal; it is a little surprise to me that no text or image information is used (probably due to data accessibility problem)</li>
<li>The algorithm used is still RQ-VAE, one diversity loss is introduced to make sure the codebook is in good utilization as well as too much conflicting semantic is collapsed into the same index</li>
<li>SID is hard prompted for generative recommender, which is a classic way to use SID for ranking models</li>
</ul>
<p><a href="/assets/kdd-2025-sid.png" class="popup img-link shimmer"><img src="/assets/kdd-2025-sid.png" alt="Generative Next POI Recommendation with Semantic ID" loading="lazy"></a></p>
<h3 id="recommendation-system-tradition-problem">
<span class="me-2">Recommendation System Tradition Problem</span><a href="#recommendation-system-tradition-problem" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="home-hierarchy-of-multi-gate-experts-for-multi-task-learning-at-kuaishou">
<span class="me-2">HoME: Hierarchy of Multi-Gate Experts for Multi-Task Learning at Kuaishou</span><a href="#home-hierarchy-of-multi-gate-experts-for-multi-task-learning-at-kuaishou" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This work from Kuaishou addressed several common problem encountered when apply MOE to multi-tasking learning scenarios.</p>
<ul>
<li>
<strong>Expert collapse</strong>, where only a few experts are being used. The solution proposed here is to add a normalization to the experts output.</li>
<li>
<strong>Expert degradation</strong>, where shared experts are only used by one or two tasks instead of benefitting all tasks. The solution proposed here is to category the tasks into categories and use the category level gate to aggregate the information and route.</li>
<li>
<strong>Expert Underfitting</strong>, where some task that are sparse in data would only use shared experts instead of leveraging task specific tasks as well. The proposed solution is to introduce a feature gate to transform the input features into multiple facets, which is similar to the multi-head projection used in transformer. One benefits of this approach is that it could prevent gradient from different tasks to collapse.</li>
</ul>
<h4 id="beyond-item-dissimilarities-diversifying-by-intent-in-recommender-systems">
<span class="me-2">Beyond Item Dissimilarities: Diversifying by Intent in Recommender Systems</span><a href="#beyond-item-dissimilarities-diversifying-by-intent-in-recommender-systems" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>One work from Google which provide a new approach for rerank diversification. Rerank usually is the last step in the multi-stage ranking system, where the ranked item went through another round of ranking based on certain business needs, such as avoid putting the similar items next to each other. In this work, user’s intention plays a key role to diversity the results.</p>
<p>One model is built to predict user’s intention. The model’s architecture and input is similar to the one widely used for ranking, but the challenging part is labeling as there is no explicit intention given by users. Some heuristic rules are used here to define user’s intention based on their behavior on homepage.</p>
<p>The next step is a relative complex algorithm using Bayes formula to adjust the item scores based on user intention. In an iterative process, the item with highest scores are selected, once an item is selected, the user’s intention distribution would also be updated in a counterfactual way (if intention <code class="language-plaintext highlighter-rouge">intent_a</code> is selected on position <code class="language-plaintext highlighter-rouge">m</code>, then <code class="language-plaintext highlighter-rouge">intent_a</code>’s probability in user intention distribution should be lowered on position <code class="language-plaintext highlighter-rouge">m+1</code>, because if user goes from <code class="language-plaintext highlighter-rouge">m</code> to <code class="language-plaintext highlighter-rouge">m+1</code>, then it means that user does not have <code class="language-plaintext highlighter-rouge">intent_a</code>).</p>
<h3 id="agentic">
<span class="me-2">Agentic</span><a href="#agentic" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="empowering-economic-simulation-for-massively-multiplayer-online-games-through-generative-agent-based-modeling">
<span class="me-2">Empowering Economic Simulation for Massively Multiplayer Online Games through Generative Agent-Based Modeling</span><a href="#empowering-economic-simulation-for-massively-multiplayer-online-games-through-generative-agent-based-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This is a work from NetEase, one of the largest web/mobile game company in China. In this work, they leverage agentic to understand the economy behaviors within the game (e.g. trading between players)</p>
<ul>
<li>They built a simulated environment for the agents to interact; the action space of the agent is also structured to limit the over complexity</li>
<li>Each agent is bootstrapped with certain user profile to mimic different style of players</li>
<li>Self-reflection through LLM and long/short term memory are also integrated with the agent</li>
</ul>
<p>This work is similar to the pervious sand-box style work “AI-towns”. If you are also interested in building something similar, highly recommend this paper as a referral.</p>
<h4 id="mockllm-a-multi-agent-behavior-collaboration-framework-for-online-job-seeking-and-recruiting">
<span class="me-2">MockLLM: A Multi-Agent Behavior Collaboration Framework for Online Job Seeking and Recruiting</span><a href="#mockllm-a-multi-agent-behavior-collaboration-framework-for-online-job-seeking-and-recruiting" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>A work from BOSS, which is a job hunting platform in China, regrading using LLM agentic to simulate the process of job matching. The agent is integrated with reflection mechanism for both interviewer and candidate; interviewer would evaluate the performance the candidate and see if he is a good fit; candidate would also evaluate the process and see if he feels the job is good match for him. The result generated from these simulations are used to guide the job matching on the platform.</p>
<blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote>
<p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p>
</div>
<div class="post-tail-wrapper text-muted">
<div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a>
</div>
<div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/machine-learning-design/" class="post-tag no-text-decoration">machine-learning-design</a> <a href="/tags/recommendation-system/" class="post-tag no-text-decoration">recommendation-system</a> <a href="/tags/user-sequence-modeling/" class="post-tag no-text-decoration">user-sequence-modeling</a> <a href="/tags/llm4rec/" class="post-tag no-text-decoration">llm4rec</a>
</div>
<div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 ">
<div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div>
<div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=KDD%202025%20Paper%20Summary%20-%20Coding%20Monkey&amp;url=https%3A%2F%2Fpyemma.github.io%2FKDD-2025-Paper-Summary%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=KDD%202025%20Paper%20Summary%20-%20Coding%20Monkey&amp;u=https%3A%2F%2Fpyemma.github.io%2FKDD-2025-Paper-Summary%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FKDD-2025-Paper-Summary%2F&amp;text=KDD%202025%20Paper%20Summary%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span>
</div>
</div>
</div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access">
<section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2>
<ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
<li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-3/">A Random Walk Down Recsys - Part 3</a>
</li>
<li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-2/">A Random Walk Down Recsys - Part 2</a>
</li>
<li class="text-truncate lh-lg"> <a href="/OpenOneRec-RL/">Learning VERL Part 1 - A Perspective from OpenOneRec</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Recommendation-Paper-2025-Review/">My 2025 Recommendation System Paper Summary</a>
</li>
<li class="text-truncate lh-lg"> <a href="/FSDP2-Code-Walk/">FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</a>
</li>
</ul></section><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a>
</div></section>
</div>
<section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2>
<nav id="toc"></nav></section></aside>
</div>
<div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
<aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3>
<nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Recsys-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1764230400" data-df="ll"> Nov 27, 2025 </time><h4 class="pt-0 my-2">Recsys 2025 Paper Summary</h4>
<div class="text-muted"><p>In this post, I would like to summary the paper from Recsys 2025 and share some of my learnings. We would cover several topics such as sequence modeling, cross domain learning as well as LLM integr...</p></div>
</div></a></article><article class="col"> <a href="/A-Random-Walk-Down-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1768723200" data-df="ll"> Jan 18, 2026 </time><h4 class="pt-0 my-2">A Random Walk Down Recsys - Part 1</h4>
<div class="text-muted"><p>This is a new series of blog beyond my conference paper reading blog, in which I would summarize the paper that I found interesting form Arxiv IR section and share my learnings. In this first blog...</p></div>
</div></a></article><article class="col"> <a href="/Recommendation-Paper-2025-Review/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1767340800" data-df="ll"> Jan 2, 2026 </time><h4 class="pt-0 my-2">My 2025 Recommendation System Paper Summary</h4>
<div class="text-muted"><p>In this post, I would like to share some insights from the paper I have read in year 2025 and summarize some trends over the year. The One The best work I enjoyed this year is the One-series from...</p></div>
</div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/Book-PyTorch-Training-Optimization/" class="btn btn-outline-primary" aria-label="Older"><p>PyTorch 性能与显存优化手册</p></a> <a href="/Recsys-2025-Paper-Summary/" class="btn btn-outline-primary" aria-label="Newer"><p>Recsys 2025 Paper Summary</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div>
<script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/KDD-2025-Paper-Summary/'; this.page.identifier = '/KDD-2025-Paper-Summary/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 "><p>© <time>2026</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p>
<p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer>
</div></div>
<div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content">
<div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a>
</div></section></div>
<div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
</div></div>
</div>
<aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside>
</div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false"><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close"></button>
</div>
<div class="toast-body text-center pt-0">
<p class="px-2 mb-3">A new version of content is available.</p>
<button type="button" class="btn btn-primary" aria-label="Update"> Update </button>
</div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&amp;register=true"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
