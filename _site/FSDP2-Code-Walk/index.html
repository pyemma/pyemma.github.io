<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="FSDP2 Under the Hood - A Deep Dive into PyTorch’s Fully Sharded Data Parallel Implementation" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="Fully Sharded Data Parallel (FSDP) is PyTorch’s approach to training large models that don’t fit in a single GPU’s memory. FSDP2 represents a significant redesign from FSDP1, with improved performance, better composability, and a cleaner architecture built on top of PyTorch’s DTensor abstraction. In this post, I’ll walk through the implementation details of FSDP2, exploring how it shards parameters, orchestrates communication, and overlaps computation with communication to achieve efficient distributed training. Please feel free to correct me if there are any misunderstanding." /><meta property="og:description" content="Fully Sharded Data Parallel (FSDP) is PyTorch’s approach to training large models that don’t fit in a single GPU’s memory. FSDP2 represents a significant redesign from FSDP1, with improved performance, better composability, and a cleaner architecture built on top of PyTorch’s DTensor abstraction. In this post, I’ll walk through the implementation details of FSDP2, exploring how it shards parameters, orchestrates communication, and overlaps computation with communication to achieve efficient distributed training. Please feel free to correct me if there are any misunderstanding." /><link rel="canonical" href="https://pyemma.github.io/FSDP2-Code-Walk/" /><meta property="og:url" content="https://pyemma.github.io/FSDP2-Code-Walk/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2026-01-03T00:00:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="FSDP2 Under the Hood - A Deep Dive into PyTorch’s Fully Sharded Data Parallel Implementation" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2026-01-18T16:11:30-08:00","datePublished":"2026-01-03T00:00:00-08:00","description":"Fully Sharded Data Parallel (FSDP) is PyTorch’s approach to training large models that don’t fit in a single GPU’s memory. FSDP2 represents a significant redesign from FSDP1, with improved performance, better composability, and a cleaner architecture built on top of PyTorch’s DTensor abstraction. In this post, I’ll walk through the implementation details of FSDP2, exploring how it shards parameters, orchestrates communication, and overlaps computation with communication to achieve efficient distributed training. Please feel free to correct me if there are any misunderstanding.","headline":"FSDP2 Under the Hood - A Deep Dive into PyTorch’s Fully Sharded Data Parallel Implementation","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/FSDP2-Code-Walk/"},"url":"https://pyemma.github.io/FSDP2-Code-Walk/"}</script><title>FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1767427200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 3, 2026 </time> </span> <span> Updated <time data-ts="1768781490" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 18, 2026 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2856 words" > <em>15 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>Fully Sharded Data Parallel (FSDP) is PyTorch’s approach to training large models that don’t fit in a single GPU’s memory. FSDP2 represents a significant redesign from FSDP1, with improved performance, better composability, and a cleaner architecture built on top of PyTorch’s DTensor abstraction. In this post, I’ll walk through the implementation details of FSDP2, exploring how it shards parameters, orchestrates communication, and overlaps computation with communication to achieve efficient distributed training. Please feel free to correct me if there are any misunderstanding.</p><p>FSDP2’s core philosophy is similar to <em>parameter-server</em> architecture, where the parameters are allocated <em>somewhere else</em> instead of the current node to reduce the memory overhead. The parameter is fetched on-demand when they are required for compute, and freed when they are no longer used.</p><blockquote><p>PS: I used cursor to help me navigate the entire pytorch codebase and explain certain syntax and business logic that I’m not familiar with. I spend around 6 hours in total to have a full E2E understand on how FSDP2 internally works, which is impossible without AI. I’m pretty impressed on this journey and it would reshape how in the future I would work and study.</p></blockquote><h2 id="high-level-architecture"><span class="me-2">High-Level Architecture</span><a href="#high-level-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>FSDP2 is built around several core components that work together to manage parameter sharding and communication:</p><h3 id="core-components"><span class="me-2">Core Components</span><a href="#core-components" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>FSDPState</strong> - The central state manager that tracks all FSDP-related metadata for a module. It maintains references to parameter groups, manages hook registration, and coordinates the overall FSDP execution flow.</p><p><strong>FSDPParamGroup</strong> - Groups multiple parameters together for efficient collective communication. This implements the “bucketing” strategy where parameters are flattened into a single tensor before communication, reducing the number of collective operations needed.</p><p><strong>FSDPParam</strong> - Wraps individual parameters and manages their sharding/unsharding lifecycle. Each <code class="language-plaintext highlighter-rouge">FSDPParam</code> encapsulates a <code class="language-plaintext highlighter-rouge">DTensor</code> that represents the sharded parameter, and handles the conversion between sharded and unsharded states.</p><p><strong>FSDPModule</strong> - A wrapper that transforms regular PyTorch modules into FSDP-aware modules by dynamically creating new module classes with the “FSDP” prefix.</p><h3 id="key-design-principles"><span class="me-2">Key Design Principles</span><a href="#key-design-principles" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ol><li><p><strong>Hook-based Execution</strong>: FSDP2 uses PyTorch’s forward/backward hooks to intercept module execution. Pre-forward hooks handle parameter unsharding, while post-forward hooks handle resharding to free memory.</p><li><p><strong>DTensor Integration</strong>: FSDP2 leverages PyTorch’s DTensor abstraction for sharding semantics. DTensors provide a clean way to represent distributed tensors with sharding specifications, and integrate seamlessly with autograd. However, one key thing here is that all parameters’ communication is bucketized together as <code class="language-plaintext highlighter-rouge">FSDPParamGroup</code> instead of using the individual API exposed via <code class="language-plaintext highlighter-rouge">DTensor</code>. This is a key optimization from FSDP2 because of the global view on the parameters.</p><li><p><strong>Stream-based Communication Overlap</strong>: FSDP2 uses separate CUDA streams for communication operations, allowing computation and communication to overlap. The <code class="language-plaintext highlighter-rouge">all_gather_stream</code> handles collective communication, while <code class="language-plaintext highlighter-rouge">all_gather_copy_in_stream</code> prepares data for the next layer’s communication.</p></ol><h2 id="initialization-flow"><span class="me-2">Initialization Flow</span><a href="#initialization-flow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The entry point to FSDP2 is the <code class="language-plaintext highlighter-rouge">fully_shard()</code> function. Let’s trace through what happens when you wrap a module with FSDP:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre>  <span class="n">modules</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">(</span><span class="n">module</span><span class="p">,)</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="nf">tuple</span><span class="p">(</span><span class="nf">_get_root_modules</span><span class="p">(</span><span class="n">module</span><span class="p">))</span>
  <span class="p">)</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">fully_shard</span><span class="p">.</span><span class="nf">state</span><span class="p">(</span><span class="n">modules</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
  <span class="n">state</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">modules</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">mp_policy</span><span class="p">,</span> <span class="n">auto_reshard_after_forward</span><span class="p">)</span>
  <span class="bp">...</span>
  <span class="k">if</span> <span class="n">params</span><span class="p">:</span>
      <span class="n">state</span><span class="p">.</span><span class="n">_fsdp_param_group</span> <span class="o">=</span> <span class="nc">FSDPParamGroup</span><span class="p">(</span>
          <span class="n">params</span><span class="p">,</span>
          <span class="n">modules</span><span class="p">,</span>
          <span class="n">mesh_info</span><span class="p">,</span>
          <span class="n">post_forward_mesh_info</span><span class="p">,</span>
          <span class="n">device</span><span class="p">,</span>
          <span class="n">shard_placement_fn</span><span class="p">,</span>
          <span class="n">mp_policy</span><span class="p">,</span>
          <span class="n">offload_policy</span><span class="p">,</span>
      <span class="p">)</span>
  <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">modules</span><span class="p">:</span>
      <span class="n">cls</span> <span class="o">=</span> <span class="n">module</span><span class="p">.</span><span class="n">__class__</span>
      <span class="n">new_cls</span> <span class="o">=</span> <span class="n">cls_to_fsdp_cls</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">cls</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">new_cls</span><span class="p">:</span>
          <span class="n">dct</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">__deepcopy__</span><span class="sh">"</span><span class="p">:</span> <span class="n">_unimplemented_deepcopy</span><span class="p">}</span>
          <span class="n">new_cls</span> <span class="o">=</span> <span class="nf">type</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">FSDP</span><span class="si">{</span><span class="n">cls</span><span class="p">.</span><span class="n">__name__</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="n">FSDPModule</span><span class="p">,</span> <span class="n">cls</span><span class="p">),</span> <span class="n">dct</span><span class="p">)</span>
          <span class="n">cls_to_fsdp_cls</span><span class="p">[</span><span class="n">cls</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_cls</span>
          <span class="n">module</span><span class="p">.</span><span class="n">__class__</span> <span class="o">=</span> <span class="n">new_cls</span>
</pre></table></code></div></div><p>The initialization process involves:</p><ol><li><p><strong>State Initialization</strong>: Creates an <code class="language-plaintext highlighter-rouge">FSDPState</code> object and registers pre/post forward/backward hooks on the module.</p><li><p><strong>Parameter Discovery</strong>: Finds all parameters that need to be sharded and groups them into an <code class="language-plaintext highlighter-rouge">FSDPParamGroup</code>.</p><li><p><strong>Module Transformation</strong>: Dynamically creates a new module class that inherits from both <code class="language-plaintext highlighter-rouge">FSDPModule</code> and the original module class, then replaces the module’s class. This allows FSDP to intercept module operations while preserving the original module’s functionality.</p><li><p><strong>DeviceMesh and ProcessGroup Setup</strong>: The <code class="language-plaintext highlighter-rouge">DeviceMesh</code> abstraction manages the process group for communication. The <code class="language-plaintext highlighter-rouge">FSDPParamGroup</code> retrieves its process group through the mesh info:</p></ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre>      <span class="nd">@property</span>
      <span class="k">def</span> <span class="nf">_all_gather_process_group</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dist</span><span class="p">.</span><span class="n">ProcessGroup</span><span class="p">:</span>
          <span class="n">mesh_info</span> <span class="o">=</span> <span class="p">(</span>
              <span class="nf">cast</span><span class="p">(</span><span class="n">FSDPMeshInfo</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">post_forward_mesh_info</span><span class="p">)</span>
              <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">is_sharded_post_forward</span>
              <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">mesh_info</span>
          <span class="p">)</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">mesh_info</span><span class="p">,</span> <span class="n">FSDPMeshInfo</span><span class="p">):</span>
              <span class="k">raise</span> <span class="nc">AssertionError</span><span class="p">(</span>
                  <span class="sa">f</span><span class="sh">"</span><span class="s">Expected mesh_info to be FSDPMeshInfo, got </span><span class="si">{</span><span class="nf">type</span><span class="p">(</span><span class="n">mesh_info</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span>
              <span class="p">)</span>
          <span class="k">return</span> <span class="n">mesh_info</span><span class="p">.</span><span class="n">shard_process_group</span>
</pre></table></code></div></div><h2 id="parameter-sharding"><span class="me-2">Parameter Sharding</span><a href="#parameter-sharding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><code class="language-plaintext highlighter-rouge">FSDPParam</code> is the key component within <code class="language-plaintext highlighter-rouge">FSDPParamGroup</code>. Each parameter is sharded via <code class="language-plaintext highlighter-rouge">FSDPParam._init_sharded_param()</code>. This is where DTensor integration happens:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>  <span class="c1"># shard the param data based on the world size and the dim to shard
</span>  <span class="n">chunks</span> <span class="o">=</span> <span class="nf">_chunk_with_empty</span><span class="p">(</span><span class="n">param_data</span><span class="p">,</span> <span class="n">shard_world_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">shard_dim</span><span class="p">)</span>
  <span class="n">sharded_param</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="n">shard_rank</span><span class="p">]</span>
  <span class="c1"># there is some additional padding logic here
</span>  <span class="bp">...</span>
  <span class="c1"># finally create the DTensor here and register it to the module
</span>  <span class="n">self</span><span class="p">.</span><span class="n">sharded_param</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">to_sharded_dtensor</span><span class="p">(</span><span class="n">sharded_param</span><span class="p">))</span>
  <span class="n">self</span><span class="p">.</span><span class="n">sharded_param</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="n">param</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="nf">_setattr_on_modules</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">sharded_param</span><span class="p">)</span>
  <span class="n">self</span><span class="p">.</span><span class="n">sharded_state</span> <span class="o">=</span> <span class="n">ShardedState</span><span class="p">.</span><span class="n">SHARDED</span>
</pre></table></code></div></div><p>During chunking, only shard 0 supports uneven sizes. For other dimensions, the size must be divisible by the world size. Shard 0 is always padded, and this padded size is used to pad other shards:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>  <span class="n">padded_sharded_size</span> <span class="o">=</span> <span class="n">chunks</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">size</span><span class="p">()</span>  <span class="c1"># 0th always padded
</span>  <span class="n">self</span><span class="p">.</span><span class="n">padded_sharded_param_size</span> <span class="o">=</span> <span class="n">padded_sharded_size</span>
  <span class="c1"># Pre-pad the sharded parameter to avoid padding before all-gather
</span>  <span class="n">padded_sharded_param</span> <span class="o">=</span> <span class="n">param_data</span><span class="p">.</span><span class="nf">new_zeros</span><span class="p">(</span><span class="n">padded_sharded_size</span><span class="p">)</span>
</pre></table></code></div></div><p>When the input parameter is already a <code class="language-plaintext highlighter-rouge">DTensor</code>, FSDP2 handles FSDP + Tensor Parallel hybrid parallelism. The device mesh needs to be configured accordingly, e.g., <code class="language-plaintext highlighter-rouge">[4, 2], [ShardDim(0), ShardDim(1)]</code> for a 2D mesh. (I will have another blog on TP using <code class="language-plaintext highlighter-rouge">DTensor</code>)</p><p>Note that <code class="language-plaintext highlighter-rouge">self._sharded_param_data</code> stores the flattened tensor (used for all-gather operations), while <code class="language-plaintext highlighter-rouge">self.sharded_param</code> preserves the parameter with original shape.</p><h2 id="forward-pass-execution"><span class="me-2">Forward Pass Execution</span><a href="#forward-pass-execution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The forward pass in FSDP2 is orchestrated through hooks that manage parameter unsharding, communication, and resharding. These hooks are defined within <code class="language-plaintext highlighter-rouge">FSDPState</code> and registered by function <code class="language-plaintext highlighter-rouge">_register_group_forward_hooks</code> and <code class="language-plaintext highlighter-rouge">_register_pre_backward_hook</code>.</p><h3 id="pre-forward-hook"><span class="me-2">Pre-Forward Hook</span><a href="#pre-forward-hook" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The <code class="language-plaintext highlighter-rouge">_pre_forward</code> hook is where the magic happens. It coordinates several critical operations:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_pre_forward</span><span class="p">(</span>
    <span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="p">...],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="p">...],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
    <span class="c1"># When composing with module-hook-based activation checkpointing, the
</span>    <span class="c1"># pre-backward hook is responsible for the unshard
</span>    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">==</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">PRE_BACKWARD</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">FORWARD</span>
    <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_root_pre_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_mp_policy</span><span class="p">.</span><span class="n">cast_forward_inputs</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">_mp_policy</span><span class="p">.</span><span class="n">param_dtype</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">profiler</span><span class="p">.</span><span class="nf">record_function</span><span class="p">(</span><span class="sh">"</span><span class="s">FSDP::cast_forward_inputs</span><span class="sh">"</span><span class="p">):</span>
            <span class="n">cast_fn</span> <span class="o">=</span> <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span>
                <span class="n">_cast_fp_tensor</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_mp_policy</span><span class="p">.</span><span class="n">param_dtype</span>
            <span class="p">)</span>
            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nf">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">args</span><span class="p">),</span>
                <span class="nf">_apply_to_tensors</span><span class="p">(</span><span class="n">cast_fn</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">),</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_fsdp_param_group</span><span class="p">:</span>
        <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_fsdp_param_group</span><span class="p">.</span><span class="nf">pre_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">fsdp_state</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_states_to_forward_prefetch</span><span class="p">:</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">target_param_group</span> <span class="p">:</span><span class="o">=</span> <span class="n">fsdp_state</span><span class="p">.</span><span class="n">_fsdp_param_group</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">FSDPParamGroup</span><span class="p">.</span><span class="nf">_prefetch_unshard</span><span class="p">(</span><span class="n">target_param_group</span><span class="p">,</span> <span class="sh">"</span><span class="s">forward</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
</pre></table></code></div></div><p>The hook performs several key tasks:</p><ol><li><strong>Stream Synchronization</strong>: Before starting forward pass, FSDP2 synchronizes communication streams with the optimizer stream to ensure parameters have been updated:</ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="c1"># Wait for optimizer before implicitly prefetched all-gathers
</span><span class="nf">if </span><span class="p">(</span><span class="n">event</span> <span class="p">:</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_state_ctx</span><span class="p">.</span><span class="n">post_optim_event</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_copy_in_stream</span><span class="p">.</span><span class="nf">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_stream</span><span class="p">.</span><span class="nf">wait_event</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_state_ctx</span><span class="p">.</span><span class="n">post_optim_event</span> <span class="o">=</span> <span class="bp">None</span>
          <span class="k">else</span><span class="p">:</span>
    <span class="n">current_stream</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_device_handle</span><span class="p">.</span><span class="nf">current_stream</span><span class="p">()</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_copy_in_stream</span><span class="p">.</span><span class="nf">wait_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_stream</span><span class="p">.</span><span class="nf">wait_stream</span><span class="p">(</span><span class="n">current_stream</span><span class="p">)</span>
</pre></table></code></div></div><p>This ensures that all-gather operations read the updated parameters after the optimizer step, avoiding stale data.</p><ol><li><p><strong>Mixed Precision Handling</strong>: If mixed precision is enabled, the hook casts input tensors to the appropriate dtype.</p><li><p><strong>Parameter Unsharding</strong>: The <code class="language-plaintext highlighter-rouge">FSDPParamGroup.pre_forward()</code> method handles unsharding (<code class="language-plaintext highlighter-rouge">self.unsard</code> and <code class="language-plaintext highlighter-rouge">self.wait_for_unshard</code>):</p></ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>      <span class="k">def</span> <span class="nf">pre_forward</span><span class="p">(</span>
          <span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">args</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="p">...],</span> <span class="n">kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span>
      <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="p">...],</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
          <span class="k">if</span> <span class="ow">not</span> <span class="nf">compiled_autograd_enabled</span><span class="p">():</span>
              <span class="n">logger</span><span class="p">.</span><span class="nf">debug</span><span class="p">(</span><span class="sh">"</span><span class="s">%s</span><span class="sh">"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="nf">_with_fqn</span><span class="p">(</span><span class="sh">"</span><span class="s">FSDP::pre_forward</span><span class="sh">"</span><span class="p">))</span>
          <span class="k">with</span> <span class="nf">record_function</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_with_fqn</span><span class="p">(</span><span class="sh">"</span><span class="s">FSDP::pre_forward</span><span class="sh">"</span><span class="p">)):</span>
              <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">FORWARD</span>
              <span class="n">self</span><span class="p">.</span><span class="nf">unshard</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">unshard_async_op</span><span class="p">)</span>
              <span class="n">self</span><span class="p">.</span><span class="nf">wait_for_unshard</span><span class="p">()</span>
              <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_register_post_backward_hook</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span>
              <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
</pre></table></code></div></div><ol><li><strong>Prefetching</strong>: The hook can prefetch parameters for the next layer to overlap communication with computation.</ol><h3 id="all-gather-communication"><span class="me-2">All-Gather Communication</span><a href="#all-gather-communication" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The <code class="language-plaintext highlighter-rouge">unshard()</code> method triggers the all-gather operation. The core communication logic is in <code class="language-plaintext highlighter-rouge">foreach_all_gather()</code>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
</pre><td class="rouge-code"><pre>  <span class="nd">@torch.no_grad</span><span class="p">()</span>
  <span class="k">def</span> <span class="nf">foreach_all_gather</span><span class="p">(</span>
      <span class="n">fsdp_params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">FSDPParam</span><span class="p">],</span>
      <span class="n">group</span><span class="p">:</span> <span class="n">dist</span><span class="p">.</span><span class="n">ProcessGroup</span><span class="p">,</span>
      <span class="n">async_op</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
      <span class="n">all_gather_copy_in_stream</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Stream</span><span class="p">,</span>
      <span class="n">all_gather_stream</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Stream</span><span class="p">,</span>
      <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">,</span>
      <span class="n">all_gather_comm</span><span class="p">:</span> <span class="n">AllGather</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AllGatherResult</span><span class="p">]:</span>
      <span class="n">world_size</span><span class="p">,</span> <span class="n">rank</span> <span class="o">=</span> <span class="n">group</span><span class="p">.</span><span class="nf">size</span><span class="p">(),</span> <span class="n">group</span><span class="p">.</span><span class="nf">rank</span><span class="p">()</span>
      <span class="n">device_handle</span> <span class="o">=</span> <span class="nf">_get_device_handle</span><span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="nb">type</span><span class="p">)</span>
      <span class="c1"># this is a context manager, and all kernel launch would be sent to `all_gather_copy_in_stream`
</span>      <span class="k">with</span> <span class="n">device_handle</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span><span class="n">all_gather_copy_in_stream</span><span class="p">):</span>
          <span class="c1"># this function create a flatten version of parameters, and split for separation
</span>          <span class="n">param_all_gather_inputs</span> <span class="o">=</span> <span class="nf">_get_param_all_gather_inputs</span><span class="p">(</span><span class="n">fsdp_params</span><span class="p">)</span>
          <span class="p">(</span>
              <span class="n">param_all_gather_input_dtypes</span><span class="p">,</span>
              <span class="n">param_all_gather_input_numels</span><span class="p">,</span>
              <span class="n">dtype</span><span class="p">,</span>
          <span class="p">)</span> <span class="o">=</span> <span class="nf">_get_all_gather_input_metadatas</span><span class="p">(</span><span class="n">param_all_gather_inputs</span><span class="p">)</span>
          <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="n">uint8</span><span class="p">:</span>
              <span class="n">all_gather_inputs</span> <span class="o">=</span> <span class="p">[</span>
                  <span class="n">t</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">uint8</span><span class="p">)</span> <span class="k">for</span> <span class="n">ts</span> <span class="ow">in</span> <span class="n">param_all_gather_inputs</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">ts</span>
              <span class="p">]</span>
          <span class="k">else</span><span class="p">:</span>
              <span class="n">all_gather_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">chain</span><span class="p">.</span><span class="nf">from_iterable</span><span class="p">(</span><span class="n">param_all_gather_inputs</span><span class="p">)]</span>
          <span class="n">inp_split_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">.</span><span class="nf">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">all_gather_inputs</span><span class="p">]</span>
          <span class="n">all_gather_input_numel</span> <span class="o">=</span> <span class="nf">sum</span><span class="p">(</span><span class="n">inp_split_sizes</span><span class="p">)</span>
          <span class="c1"># this is a interface that design for the task of communication primitives,
</span>          <span class="c1"># which defines how, where to allocate memory
</span>          <span class="n">all_gather_output</span> <span class="o">=</span> <span class="n">all_gather_comm</span><span class="p">.</span><span class="nf">allocate</span><span class="p">(</span>
              <span class="p">(</span><span class="n">all_gather_input_numel</span> <span class="o">*</span> <span class="n">world_size</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span>
          <span class="p">)</span>
          <span class="n">all_gather_input</span><span class="p">,</span> <span class="n">all_gather_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">fsdp</span><span class="p">.</span><span class="nf">all_gather_copy_in</span><span class="p">(</span>
              <span class="n">all_gather_inputs</span><span class="p">,</span>
              <span class="n">all_gather_output</span><span class="p">,</span>
              <span class="n">inp_split_sizes</span><span class="p">,</span>
              <span class="n">all_gather_input_numel</span><span class="p">,</span>
              <span class="n">rank</span><span class="p">,</span>
          <span class="p">)</span>
          <span class="k">del</span> <span class="n">param_all_gather_inputs</span>
      <span class="n">all_gather_stream</span><span class="p">.</span><span class="nf">wait_stream</span><span class="p">(</span><span class="n">all_gather_copy_in_stream</span><span class="p">)</span>
      <span class="k">with</span> <span class="n">device_handle</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span><span class="n">all_gather_stream</span><span class="p">):</span>
          <span class="c1"># this would invoke dist.all_gather_into_tensor 
</span>          <span class="n">all_gather_work</span> <span class="o">=</span> <span class="nf">all_gather_comm</span><span class="p">(</span>
              <span class="n">output_tensor</span><span class="o">=</span><span class="n">all_gather_output</span><span class="p">,</span>
              <span class="n">input_tensor</span><span class="o">=</span><span class="n">all_gather_input</span><span class="p">,</span>
              <span class="n">group</span><span class="o">=</span><span class="n">group</span><span class="p">,</span>
              <span class="n">async_op</span><span class="o">=</span><span class="n">async_op</span><span class="p">,</span>
          <span class="p">)</span>
          <span class="n">all_gather_event</span> <span class="o">=</span> <span class="n">all_gather_stream</span><span class="p">.</span><span class="nf">record_event</span><span class="p">()</span>
          <span class="k">return</span> <span class="nc">AllGatherResult</span><span class="p">(</span>
              <span class="n">all_gather_output</span><span class="p">,</span>
              <span class="n">all_gather_event</span><span class="p">,</span>
              <span class="n">all_gather_work</span><span class="p">,</span>
              <span class="n">param_all_gather_input_dtypes</span><span class="p">,</span>
              <span class="n">param_all_gather_input_numels</span><span class="p">,</span>
              <span class="n">inp_split_sizes</span><span class="p">,</span>
          <span class="p">)</span>
</pre></table></code></div></div><p>The process involves:</p><ol><li><p><strong>Flattening Parameters</strong>: All parameters in the group are flattened into a single tensor for efficient communication.</p><li><p><strong>Copy-In</strong>: The <code class="language-plaintext highlighter-rouge">all_gather_copy_in</code> operation copies each rank’s local shard into the appropriate position of the all-gather output buffer. This happens on the <code class="language-plaintext highlighter-rouge">all_gather_copy_in_stream</code>.</p><li><p><strong>Communication</strong>: The actual all-gather collective is executed on the <code class="language-plaintext highlighter-rouge">all_gather_stream</code>, which waits for the copy-in stream to complete.</p><li><p><strong>Copy-Out</strong>: After communication completes, <code class="language-plaintext highlighter-rouge">wait_for_unshard()</code> calls <code class="language-plaintext highlighter-rouge">foreach_all_gather_copy_out()</code> to distribute the gathered data back to individual parameters:</p></ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre><td class="rouge-code"><pre><span class="c1"># get all necessary information from the all_gather_result
</span><span class="p">(</span>
    <span class="n">all_gather_output</span><span class="p">,</span>
    <span class="n">all_gather_event</span><span class="p">,</span>
    <span class="n">all_gather_work</span><span class="p">,</span>
    <span class="n">param_all_gather_input_dtypes</span><span class="p">,</span>
    <span class="n">param_all_gather_input_numels</span><span class="p">,</span>
    <span class="n">all_gather_input_split_sizes</span><span class="p">,</span>
<span class="p">)</span> <span class="o">=</span> <span class="n">all_gather_result</span>

<span class="k">for</span> <span class="n">all_gather_input_numels</span><span class="p">,</span> <span class="n">all_gather_input_dtypes</span><span class="p">,</span> <span class="n">fsdp_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span>
    <span class="n">param_all_gather_input_numels</span><span class="p">,</span> <span class="n">param_all_gather_input_dtypes</span><span class="p">,</span> <span class="n">fsdp_params</span>
<span class="p">):</span>
    <span class="c1"># initialize the all gather outputs buffer in the FSDPParam
</span>    <span class="n">fsdp_param</span><span class="p">.</span><span class="nf">init_all_gather_outputs</span><span class="p">(</span>
        <span class="n">all_gather_input_numels</span><span class="p">,</span>
        <span class="n">all_gather_input_dtypes</span><span class="p">,</span>
        <span class="n">world_size</span><span class="p">,</span>
        <span class="n">device</span><span class="p">,</span>
        <span class="n">force_recreate</span><span class="o">=</span><span class="n">force_recreate</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">force_recreate</span><span class="p">:</span>
        <span class="n">fsdp_param</span><span class="p">.</span><span class="nf">alloc_all_gather_outputs</span><span class="p">()</span>
    <span class="c1"># param_all_gather_outputs point to the same reference as fsdp_param.all_gather_outputs
</span>    <span class="n">param_all_gather_outputs</span> <span class="o">=</span> <span class="n">fsdp_param</span><span class="p">.</span><span class="n">all_gather_outputs</span>
    <span class="n">split_with_sizes_out</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">param_all_gather_outputs</span><span class="p">)</span>

<span class="c1"># change the flatten all gather output buffer into (world_size, all_params_numel)
</span><span class="n">all_gather_output</span> <span class="o">=</span> <span class="n">all_gather_output</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># also change each parameters to (world_size, original_param_size)
</span><span class="n">out</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">split_with_sizes_out</span><span class="p">]</span>
<span class="c1"># split from the all_params_numel to the actual params size, which is stored in all_gather_input_split_sizes
# and copy to the out (to make sure the shape is aligned)
</span><span class="n">torch</span><span class="p">.</span><span class="n">ops</span><span class="p">.</span><span class="n">fsdp</span><span class="p">.</span><span class="nf">split_with_sizes_copy</span><span class="p">(</span>
    <span class="n">all_gather_output</span><span class="p">,</span> <span class="n">all_gather_input_split_sizes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span>
<span class="p">)</span>
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">init_unsharded_param()</code> method then creates the unsharded parameter from the all-gather output:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="n">unsharded_tensor</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">all_gather_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">unsharded_param</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">as_strided</span><span class="p">(</span>
    <span class="n">unsharded_tensor</span><span class="p">,</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_orig_size</span><span class="p">,</span>
    <span class="n">self</span><span class="p">.</span><span class="n">_contiguous_orig_stride</span><span class="p">,</span>
    <span class="n">storage_offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">_unsharded_param</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span>
    <span class="n">unsharded_param</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sharded_param</span><span class="p">.</span><span class="n">requires_grad</span>
<span class="p">)</span>
</pre></table></code></div></div><p>Note: The context manager <code class="language-plaintext highlighter-rouge">torch.autograd._unsafe_preserve_version_counter()</code> is used to tell autograd to ignore the inplace operation when copying data to the parameter location.</p><h3 id="post-forward-hook"><span class="me-2">Post-Forward Hook</span><a href="#post-forward-hook" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>After the forward pass completes, the <code class="language-plaintext highlighter-rouge">_post_forward</code> hook reshards parameters to free memory:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre>      <span class="k">def</span> <span class="nf">_post_forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
          <span class="c1"># When composing with module-hook-based activation checkpointing, the
</span>          <span class="c1"># post-backward hook is responsible for the reshard
</span>          <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">==</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">PRE_BACKWARD</span><span class="p">:</span>
              <span class="k">return</span> <span class="n">output</span>
          <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_fsdp_param_group</span><span class="p">:</span>
              <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_fsdp_param_group</span><span class="p">.</span><span class="nf">post_forward</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
          <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_register_pre_backward_hook</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
          <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">=</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">IDLE</span>
          <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_state_ctx</span><span class="p">.</span><span class="n">iter_forward_root</span> <span class="ow">is</span> <span class="n">self</span><span class="p">:</span>
              <span class="k">if</span> <span class="n">all_gather_state</span> <span class="p">:</span><span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_state</span><span class="p">:</span>
                  <span class="c1"># Free the last all-gather result if needed; refer to
</span>                  <span class="c1"># [Note: Overlapping all-gather copy-in and all-gather]
</span>                  <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_copy_in_stream</span><span class="p">.</span><span class="nf">wait_event</span><span class="p">(</span>
                      <span class="n">all_gather_state</span><span class="p">.</span><span class="n">event</span>
                  <span class="p">)</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_stream</span><span class="p">.</span><span class="nf">wait_event</span><span class="p">(</span><span class="n">all_gather_state</span><span class="p">.</span><span class="n">event</span><span class="p">)</span>
                  <span class="n">self</span><span class="p">.</span><span class="n">_comm_ctx</span><span class="p">.</span><span class="n">all_gather_state</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># free the all-gather result
</span>              <span class="n">self</span><span class="p">.</span><span class="n">_state_ctx</span><span class="p">.</span><span class="n">iter_forward_root</span> <span class="o">=</span> <span class="bp">None</span>
          <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_mp_policy</span><span class="p">.</span><span class="n">output_dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
              <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">profiler</span><span class="p">.</span><span class="nf">record_function</span><span class="p">(</span><span class="sh">"</span><span class="s">FSDP::cast_forward_outputs</span><span class="sh">"</span><span class="p">):</span>
                  <span class="n">output</span> <span class="o">=</span> <span class="nf">_apply_to_tensors</span><span class="p">(</span>
                      <span class="n">functools</span><span class="p">.</span><span class="nf">partial</span><span class="p">(</span><span class="n">_cast_fp_tensor</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">_mp_policy</span><span class="p">.</span><span class="n">output_dtype</span><span class="p">),</span>
                      <span class="n">output</span><span class="p">,</span>
                  <span class="p">)</span>
          <span class="k">return</span> <span class="n">output</span>
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">reshard()</code> method converts parameters back to sharded form:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">reshard</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_training_state</span> <span class="o">==</span> <span class="n">TrainingState</span><span class="p">.</span><span class="n">FORWARD</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">_reshard_after_forward</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_use_post_forward_mesh</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">_to_sharded_post_forward</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">_reshard_after_forward_event</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">device_handle</span><span class="p">.</span><span class="nc">Event</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_reshard_after_forward_event</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">_reshard_after_forward_event</span><span class="p">.</span><span class="nf">record</span><span class="p">()</span>
            <span class="k">return</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">_to_sharded</span><span class="p">()</span>
</pre></table></code></div></div><h2 id="backward-pass-execution"><span class="me-2">Backward Pass Execution</span><a href="#backward-pass-execution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The backward pass follows a similar pattern to the forward pass. The <code class="language-plaintext highlighter-rouge">_pre_backward</code> hook is registered on the output tensors and triggers when gradients start flowing backward. It performs parameter unsharding similar to the forward pass, but with one key difference: <strong>implicit prefetching</strong>.</p><p>In the forward pass, prefetching must be explicitly set up because we don’t know which layers will be executed next. However, in the backward pass, the forward computation graph is already known, so FSDP2 can automatically determine which parameters need to be prefetched and schedule the all-gather operations accordingly.</p><p>The backward pass also handles gradient communication through all-reduce operations, which are orchestrated through the same communication infrastructure.</p><h2 id="communication-primitives"><span class="me-2">Communication Primitives</span><a href="#communication-primitives" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Let’s trace the communication stack from FSDP2 down to the hardware:</p><h3 id="high-level-flow"><span class="me-2">High-Level Flow</span><a href="#high-level-flow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The communication path is:</p><div class="language-text highlighter-rouge"><div class="code-header"> <span data-label-text="Text"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>FSDPParamGroup.unshard() 
  → foreach_all_gather() 
  → AllGather.__call__() 
  → dist.all_gather_into_tensor() 
  → torch.distributed.distributed_c10d.all_gather_into_tensor() 
  → ProcessGroup._allgather_base() 
  → ProcessGroup C++ binding 
  → ProcessGroupNCCL (or other backend)
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">AllGather</code> object (defaulting to <code class="language-plaintext highlighter-rouge">DefaultAllGather</code>) provides an abstraction over the communication primitive, allowing different memory allocation strategies (e.g., RDMA-based allocation for NCCL).</p><h3 id="pytorch-distributed-stack"><span class="me-2">PyTorch Distributed Stack</span><a href="#pytorch-distributed-stack" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Python Layer</strong>: <code class="language-plaintext highlighter-rouge">torch.distributed.distributed_c10d</code> provides Python bindings for distributed operations.</p><p><strong>C++ ProcessGroup Abstraction</strong>: The <code class="language-plaintext highlighter-rouge">ProcessGroup</code> class in <code class="language-plaintext highlighter-rouge">torch/csrc/distributed/c10d/ProcessGroup.hpp</code> provides a virtual interface for communication backends. When you call <code class="language-plaintext highlighter-rouge">dist.broadcast(tensor, src)</code>, it goes through PyBind11 bindings to the C++ <code class="language-plaintext highlighter-rouge">ProcessGroup::broadcast()</code> method:</p><div class="language-cpp highlighter-rouge"><div class="code-header"> <span data-label-text="Cpp"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>      <span class="k">static</span> <span class="k">auto</span> <span class="n">op</span> <span class="o">=</span>
          <span class="n">c10</span><span class="o">::</span><span class="n">Dispatcher</span><span class="o">::</span><span class="n">singleton</span><span class="p">()</span>
              <span class="p">.</span><span class="n">findSchemaOrThrow</span><span class="p">(</span><span class="s">"c10d::broadcast_"</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
              <span class="p">.</span><span class="n">typed</span><span class="o">&lt;</span>
                  <span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">Work</span><span class="o">&gt;&gt;</span><span class="p">(</span>
                      <span class="n">at</span><span class="o">::</span><span class="n">TensorList</span><span class="p">,</span>
                      <span class="k">const</span> <span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;::</span><span class="n">c10d</span><span class="o">::</span><span class="n">ProcessGroup</span><span class="o">&gt;&amp;</span><span class="p">,</span>
                      <span class="kt">int64_t</span><span class="p">,</span>
                      <span class="kt">int64_t</span><span class="p">,</span>
                      <span class="kt">bool</span><span class="p">,</span>
                      <span class="kt">int64_t</span><span class="p">)</span><span class="o">&gt;</span><span class="p">();</span>
</pre></table></code></div></div><p>The operations are registered via macro functions in <code class="language-plaintext highlighter-rouge">Ops.cpp</code>:</p><div class="language-cpp highlighter-rouge"><div class="code-header"> <span data-label-text="Cpp"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre>  <span class="cp">#define IMPL_BROADCAST(DEV)                                               \
    std::tuple&lt;std::vector&lt;at::Tensor&gt;, c10::intrusive_ptr&lt;Work&gt;&gt;           \
        broadcast_##DEV(                                                    \
            at::TensorList tensors,                                         \
            const c10::intrusive_ptr&lt;ProcessGroup&gt;&amp; process_group,          \
            int64_t root_rank,                                              \
            int64_t root_tensor,                                            \
            bool asyncOp,                                                   \
            int64_t timeout) {                                              \
      auto tensor_vec = tensors.vec();                                      \
      auto work = process_group-&gt;getBackend(c10::DeviceType::DEV)           \
                      -&gt;broadcast(                                          \
                          tensor_vec,                                       \
                          BroadcastOptions{                                 \
                              root_rank,                                    \
                              root_tensor,                                  \
                              std::chrono::milliseconds(timeout),           \
                              asyncOp});                                    \
      return std::tuple&lt;std::vector&lt;at::Tensor&gt;, c10::intrusive_ptr&lt;Work&gt;&gt;( \
          std::move(tensor_vec), work);                                     \
    }
</span></pre></table></code></div></div><p><strong>NCCL Backend</strong>: <code class="language-plaintext highlighter-rouge">ProcessGroupNCCL</code> implements the actual GPU communication using NCCL. It uses a template function <code class="language-plaintext highlighter-rouge">collective()</code> as the foundation for all communication primitives:</p><div class="language-cpp highlighter-rouge"><div class="code-header"> <span data-label-text="Cpp"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre>  <span class="k">template</span> <span class="o">&lt;</span><span class="k">typename</span> <span class="nc">Fn</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">PreProcess</span><span class="p">,</span> <span class="k">typename</span> <span class="nc">PostProcess</span><span class="p">&gt;</span>
  <span class="n">c10</span><span class="o">::</span><span class="n">intrusive_ptr</span><span class="o">&lt;</span><span class="n">Work</span><span class="o">&gt;</span> <span class="n">ProcessGroupNCCL</span><span class="o">::</span><span class="n">collective</span><span class="p">(</span>
      <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span> <span class="n">inputs</span><span class="p">,</span>
      <span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;&amp;</span> <span class="n">outputs</span><span class="p">,</span>
      <span class="n">Fn</span> <span class="n">fn</span><span class="p">,</span>
      <span class="n">PreProcess</span> <span class="n">pre</span><span class="p">,</span>
      <span class="n">PostProcess</span> <span class="n">post</span><span class="p">,</span>
      <span class="n">OpType</span> <span class="n">opType</span><span class="p">,</span>
      <span class="kt">bool</span> <span class="n">asyncOp</span><span class="p">,</span>
      <span class="k">const</span> <span class="kt">char</span><span class="o">*</span> <span class="n">profilingTitle</span><span class="p">,</span>
      <span class="kt">bool</span> <span class="n">nanCheck</span><span class="p">)</span>
</pre></table></code></div></div><p>Here, <code class="language-plaintext highlighter-rouge">Fn</code>, <code class="language-plaintext highlighter-rouge">PreProcess</code>, and <code class="language-plaintext highlighter-rouge">PostProcess</code> are lambda functions. Different communication primitives provide different <code class="language-plaintext highlighter-rouge">Fn</code> implementations. For example, <code class="language-plaintext highlighter-rouge">allreduce_impl</code> passes <code class="language-plaintext highlighter-rouge">ncclAllReduce</code> as the <code class="language-plaintext highlighter-rouge">Fn</code>:</p><div class="language-cpp highlighter-rouge"><div class="code-header"> <span data-label-text="Cpp"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">input</span><span class="p">,</span>
    <span class="n">at</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&amp;</span> <span class="n">output</span><span class="p">,</span>
    <span class="n">ncclComm_t</span> <span class="n">comm</span><span class="p">,</span>
    <span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">CUDAStream</span><span class="o">&amp;</span> <span class="n">stream</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">ncclDataType</span> <span class="o">=</span> <span class="n">getNcclDataType</span><span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">scalar_type</span><span class="p">());</span>
  <span class="k">auto</span> <span class="n">ncclReduceOp</span> <span class="o">=</span>
      <span class="n">getNcclReduceOp</span><span class="p">(</span><span class="n">opts</span><span class="p">.</span><span class="n">reduceOp</span><span class="p">,</span> <span class="n">input</span><span class="p">,</span> <span class="n">ncclDataType</span><span class="p">,</span> <span class="n">comm</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">ncclAllReduce</span><span class="p">(</span>
      <span class="n">input</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">(),</span>
      <span class="n">output</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">(),</span>
      <span class="n">input</span><span class="p">.</span><span class="n">numel</span><span class="p">(),</span>
      <span class="n">ncclDataType</span><span class="p">,</span>
      <span class="n">ncclReduceOp</span><span class="p">,</span>
      <span class="n">comm</span><span class="p">,</span>
      <span class="n">stream</span><span class="p">.</span><span class="n">stream</span><span class="p">());</span>
<span class="p">},</span>
</pre></table></code></div></div><h3 id="stream-management"><span class="me-2">Stream Management</span><a href="#stream-management" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>FSDP2 uses separate CUDA streams for communication to enable overlap:</p><ul><li><strong><code class="language-plaintext highlighter-rouge">all_gather_stream</code></strong>: Handles the actual collective communication operations for the current layer.<li><strong><code class="language-plaintext highlighter-rouge">all_gather_copy_in_stream</code></strong>: Prepares data for the next layer’s communication (e.g., flattening buffers).</ul><p>When <code class="language-plaintext highlighter-rouge">asyncOp</code> is enabled, NCCL uses a dedicated stream instead of the current compute stream:</p><div class="language-cpp highlighter-rouge"><div class="code-header"> <span data-label-text="Cpp"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1">// in asyncOp=false [default] mode, we use currentStream as ncclStream</span>
<span class="c1">// otherwise, we use separate ncclStream and let it sync on currentStream</span>
<span class="k">auto</span> <span class="n">ncclStream</span> <span class="o">=</span> <span class="n">asyncOp</span> <span class="o">?</span> <span class="n">ncclStreams_</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
                          <span class="o">:</span> <span class="n">at</span><span class="o">::</span><span class="n">cuda</span><span class="o">::</span><span class="n">getCurrentCUDAStream</span><span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="n">index</span><span class="p">());</span>
<span class="k">if</span> <span class="p">(</span><span class="n">asyncOp</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// First let NCCL streams wait for input tensors allocation streams</span>
  <span class="n">syncStream</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">ncclEvents_</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">ncclStream</span><span class="p">);</span>
<span class="p">}</span>
</pre></table></code></div></div><p>This allows computation and communication to overlap, with proper synchronization to ensure data dependencies are respected.</p><h2 id="advanced-topics"><span class="me-2">Advanced Topics</span><a href="#advanced-topics" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="prefetching-strategies"><span class="me-2">Prefetching Strategies</span><a href="#prefetching-strategies" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>FSDP2 supports two types of prefetching:</p><p><strong>Explicit Prefetching (Forward Pass)</strong>: Since the forward computation graph isn’t known ahead of time, prefetching must be manually configured. The <code class="language-plaintext highlighter-rouge">_pre_forward</code> hook checks <code class="language-plaintext highlighter-rouge">self._states_to_forward_prefetch</code> (set via <code class="language-plaintext highlighter-rouge">FSDPModule.set_modules_to_forward_prefetch()</code>) and calls <code class="language-plaintext highlighter-rouge">FSDPParamGroup._prefetch_unshard()</code> for the next layer. Also the all-gather happens on different cuda stream, this makes it possible for CPU to schedule all-gather for next layer and compute for the current layer concurrently. This is a key to overlap compute and communication to hidden latency.</p><p><strong>Implicit Prefetching (Backward Pass)</strong>: After the forward pass completes, the computation graph is known. FSDP2 can automatically determine which parameters need to be prefetched and schedule all-gather operations on a separate stream while the current layer’s computation runs on the default stream, achieving overlap.</p><p>The effectiveness of implicit prefetching depends on whether the workload is CPU-bound or GPU-bound. For CPU-bound workloads, explicit prefetching setup may be necessary.</p><h3 id="memory-management"><span class="me-2">Memory Management</span><a href="#memory-management" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><strong>Parameter Bucketing</strong>: <code class="language-plaintext highlighter-rouge">FSDPParamGroup</code> groups multiple parameters together and flattens them into a single tensor before communication. This reduces the number of collective operations and improves efficiency, similar to DDP’s bucketing strategy.</p><h2 id="summary"><span class="me-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>FSDP2 represents a significant evolution in PyTorch’s distributed training capabilities. By building on DTensor and using a clean hook-based architecture, it provides:</p><ul><li><strong>Memory Efficiency</strong>: Parameters are sharded and only unsharded when needed<li><strong>Communication Efficiency</strong>: Parameter bucketing and stream-based overlap minimize communication overhead<li><strong>Composability</strong>: Clean integration with other PyTorch features like mixed precision and activation checkpointing<li><strong>Flexibility</strong>: Support for hybrid parallelism (FSDP + TP) and various memory allocation strategies</ul><blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote><p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p><h2 id="references"><span class="me-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li><a href="https://pytorch.org/docs/stable/fsdp.html">PyTorch FSDP Documentation</a><li><a href="https://pytorch.org/tutorials/prototype/dtensor_docs.html">PyTorch DTensor Documentation</a><li><a href="https://pytorch.org/docs/stable/distributed.html">PyTorch Distributed Communication (c10d)</a><li><a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html">NCCL Documentation</a><li>PyTorch Source Code:<ul><li><code class="language-plaintext highlighter-rouge">torch.distributed.fsdp._fully_shard</code><li><code class="language-plaintext highlighter-rouge">torch.distributed.fsdp._fully_shard._fsdp_state.py</code><li><code class="language-plaintext highlighter-rouge">torch.distributed.fsdp._fully_shard._fsdp_param_group.py</code><li><code class="language-plaintext highlighter-rouge">torch.distributed.fsdp._fsdp_collectives.py</code><li><code class="language-plaintext highlighter-rouge">torch/csrc/distributed/c10d/ProcessGroup.hpp</code></ul></ol></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/distributed-training/">Distributed Training</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/distributed-training/" class="post-tag no-text-decoration" >distributed-training</a> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >pytorch</a> <a href="/tags/fsdp/" class="post-tag no-text-decoration" >fsdp</a> <a href="/tags/model-parallelism/" class="post-tag no-text-decoration" >model-parallelism</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=FSDP2%20Under%20the%20Hood%20-%20A%20Deep%20Dive%20into%20PyTorch's%20Fully%20Sharded%20Data%20Parallel%20Implementation%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FFSDP2-Code-Walk%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=FSDP2%20Under%20the%20Hood%20-%20A%20Deep%20Dive%20into%20PyTorch's%20Fully%20Sharded%20Data%20Parallel%20Implementation%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FFSDP2-Code-Walk%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FFSDP2-Code-Walk%2F&text=FSDP2%20Under%20the%20Hood%20-%20A%20Deep%20Dive%20into%20PyTorch's%20Fully%20Sharded%20Data%20Parallel%20Implementation%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-3/">A Random Walk Down Recsys - Part 3</a><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-2/">A Random Walk Down Recsys - Part 2</a><li class="text-truncate lh-lg"> <a href="/OpenOneRec-RL/">Learning VERL Part 1 - A Perspective from OpenOneRec</a><li class="text-truncate lh-lg"> <a href="/Recommendation-Paper-2025-Review/">My 2025 Recommendation System Paper Summary</a><li class="text-truncate lh-lg"> <a href="/FSDP2-Code-Walk/">FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Book-PyTorch-Training-Optimization/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1752994800" data-df="ll" > Jul 20, 2025 </time><h4 class="pt-0 my-2">PyTorch 性能与显存优化手册</h4><div class="text-muted"><p>前一阵子在得道 APP 上读完了这本《PyTorch 性能与显存优化手册》，感觉是一本很不错的 PyTorch Training 入门读物，很适合刚刚接触这个领域的新手小白来读；同时整本书也提供了一个 PyTorch Training 的优化大纲，可以作为一个引子来扩展去学习更加底层的知识和技术。 这篇帖子主要是我读的时候标记下来的一些知识点，并没有包含书籍里面的全部内容。强烈推荐大家去阅...</p></div></div></a></article><article class="col"> <a href="/LLM-LLM-Training-101/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1725174000" data-df="ll" > Sep 1, 2024 </time><h4 class="pt-0 my-2">LLM Training 101</h4><div class="text-muted"><p>这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/Recommendation-Paper-2025-Review/" class="btn btn-outline-primary" aria-label="Older" ><p>My 2025 Recommendation System Paper Summary</p></a> <a href="/A-Random-Walk-Down-Recsys/" class="btn btn-outline-primary" aria-label="Newer" ><p>A Random Walk Down Recsys - Part 1</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/FSDP2-Code-Walk/'; this.page.identifier = '/FSDP2-Code-Walk/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
