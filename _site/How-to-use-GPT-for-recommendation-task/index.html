<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="How to use LLM for recommendation task" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey." /><meta property="og:description" content="Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey." /><link rel="canonical" href="https://pyemma.github.io/How-to-use-GPT-for-recommendation-task/" /><meta property="og:url" content="https://pyemma.github.io/How-to-use-GPT-for-recommendation-task/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-12-12T00:00:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="How to use LLM for recommendation task" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2024-10-05T16:53:30-07:00","datePublished":"2023-12-12T00:00:00-08:00","description":"Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey.","headline":"How to use LLM for recommendation task","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/How-to-use-GPT-for-recommendation-task/"},"url":"https://pyemma.github.io/How-to-use-GPT-for-recommendation-task/"}</script><title>How to use LLM for recommendation task | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>How to use LLM for recommendation task</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>How to use LLM for recommendation task</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1702368000" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Dec 12, 2023 </time> </span> <span> Updated <time data-ts="1728172410" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Oct 5, 2024 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="1309 words" > <em>7 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey.</p><p>Some key take away:</p><ul><li>LLM internally has encapsulated lots of knowledge about the world and it could leverage these knowledge to do some general recommendation (such as Movie)<li>In context learning is a powerful technique to inject various information into promote to provide more context for LLM, such as user profile and user past interaction history<li>Use training data that specifically constructed for recommendation task to fine tune LLM could further improve the performance of LLM<li>We could directly use LLM to output candidate, or use LLM output as additional signal to inject into existing recommendation models</ul><blockquote><p>PS: due to the rapid change of this area, the paper I read might have been outdated. Please feel free to leave comments on the latest work/idea in this domain. Also I’m reading the latest paper from arxiv and will potentially have a new series of post on summarizing the latest work in LLM and ML area, stay tuned! PPS: I would primarily summarize my understanding without to much technical terms and mathematic formula; the main goal is to grasp the high level idea of the paper</p></blockquote><h2 id="context"><span class="me-2">Context</span><a href="#context" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In classical recommendation system, we usually adopt a 2-stage architecture. In first stage, we adopt heuristic rule, or leverage some simple model to quickly identify some promising candidates from the entire eligible population (<em>actually, there is indexing step before here as well, but for simplicity, let’s skip that</em>). This first stage is called <strong>candidate retrieval</strong>, which we usually optimize for <strong>recall</strong>. In the second stage, we would rank the candidates we retrieved in the first stage, via more signals and more powerful model. This stage is usually called <strong>rerank</strong>, which optimize for <strong>precision</strong>.</p><h2 id="pairwise-ranking-via-llm"><span class="me-2">Pairwise Ranking via LLM</span><a href="#pairwise-ranking-via-llm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In paper <a href="https://arxiv.org/pdf/2306.17563.pdf">“Large Language Model Are Effective Text Rankers With Pairwise Ranking Prompting”</a>, the author proposed a new format of prompt that let LLM to rank a pair of candidates given a query, which outperforms the point-wise and list-wise format. The format of the prompt is as follow:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="sa">f</span><span class="sh">"""</span><span class="s">
Given a query </span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="s">, which of the following two passage is more relevant to the query?

Passage A: </span><span class="si">{</span><span class="n">description</span> <span class="n">of</span> <span class="n">A</span><span class="si">}</span><span class="s">

Passage B: </span><span class="si">{</span><span class="n">description</span> <span class="n">of</span> <span class="n">B</span><span class="si">}</span><span class="s">

Output Passage A or Passage B
</span><span class="sh">"""</span>
</pre></table></code></div></div><p>For each pair of candidates, we use the above prompt to let LLM output the choice, and compute the final scores as</p>\[s_{i} = 1 * \sum_{j \neq i} I_{d_{i} &gt; d_{j}} + 0.5 * \sum_{j \neq i} I_{d_{i} = d_{j}}\]<p>and rank the document accordingly.</p><h2 id="enrich-the-information-for-llm-to-recommend"><span class="me-2">Enrich the information for LLM to recommend</span><a href="#enrich-the-information-for-llm-to-recommend" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Personalized recommendation is critical to improve the conversion rate. Use profiling, user past’s item interaction history bring valuable signal for recommendation. In this section, we will take a look some idea on how to inject such information into prompt to let LLM “learn” the flavor of user and provide better personalized result.</p><p>In <a href="https://arxiv.org/pdf/2304.10149.pdf">“Is ChatGPT a Good Recommender? A Preliminary Study”</a>, the authors proposed different type of prompt of different type of tasks. These prompt could be decomposed as <strong>task descriptor</strong>, <strong>user-specific injection</strong>, <strong>formatting restrictions</strong>. <strong>User-specific injection</strong> is the part where we add user’s past item interaction info. The format for <em>sequential recommendation</em> is as follow (content in bracket is comment)</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="sa">f</span><span class="sh">"""</span><span class="s">
Requirement: you must choose 10 items for recommendation and sort them in order of priority, 
from hightest to lowest. [task descriptor]

Output format: a python list. Do not explain the reason for include any other words. [formatting restrictions]

Given user</span><span class="sh">'</span><span class="s">s interaction history in chronological order: </span><span class="si">{</span><span class="p">[</span><span class="n">i_1</span><span class="p">,</span> <span class="n">i_2</span><span class="p">,</span> <span class="n">i_3</span><span class="p">,</span> <span class="p">...,</span> <span class="n">i_n</span><span class="p">]</span><span class="si">}</span><span class="s">, 
the next interaction item is </span><span class="si">{</span><span class="n">i_n</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">. [In context learning]
Now, if the interaction history is updated to </span><span class="si">{</span><span class="p">[</span><span class="n">j_1</span><span class="p">,</span> <span class="n">j_2</span><span class="p">,</span> <span class="n">j_3</span><span class="p">,</span> <span class="p">...,</span> <span class="n">j_n</span><span class="p">]</span><span class="si">}</span><span class="s"> and the user is likely to interact again, 
recommend the next item. [user-specific injection]
</span><span class="sh">"""</span>
</pre></table></code></div></div><p>In this prompt, a common technique, which is called <em>in context learning</em>, or <em>few shot prompting</em> , is used. By showing LLM some examples to follow in the prompt, we could change the underlying distribution of LLM model and bias it to generate the output <em>conditionally</em> on the examples we have given. This stanford <a href="https://ai.stanford.edu/blog/understanding-incontext/">blog</a> is a great source to learn more on how <em>in context learning</em> works. In short words, the additional example we provided helps LLM to better <em>locate</em> concept internally, and thus more aligned. A Bayesian inference view on that is as follow, which is pretty easy to understand</p>\[p(output|prompt) = \int_{concept}p(output|concept, prompt)p(concept|prompt)d(concept)\]<p>In <a href="https://arxiv.org/pdf/2305.07622.pdf">“PALR: Personalization Aware LLMs for Recommendation”</a>, author adopted similar approach to integrate users’ past interaction into prompt. One novel idea in this paper is to leverage LLM to generate user profile, which leverages the summarization capability of LLM. The prompt is as follow (use MovieLens-1M as example)</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="sa">f</span><span class="sh">"""</span><span class="s">
Input: Your task is to use two keywords to summarize user</span><span class="sh">'</span><span class="s">s preference based on history interactions.
The output is an itemized list based on importance. The output template is:
</span><span class="si">{</span><span class="n">KEYWORD_1</span><span class="si">:</span> <span class="sh">"</span><span class="s">HISTORY_MOVE_1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">HISTORY_MOVE_2</span><span class="sh">"</span><span class="p">;</span> <span class="n">KEYWORD_2</span><span class="si">:</span> <span class="sh">"</span><span class="s">HISTORY_MOVE_2</span><span class="sh">"</span><span class="si">}</span><span class="s">
The history movies and their keywords
</span><span class="sh">"</span><span class="s">MOVIE_1</span><span class="sh">"</span><span class="s">: KEYWORD_1, KEYWORD_2
</span><span class="sh">"</span><span class="s">MOVIE_2</span><span class="sh">"</span><span class="s">: KEYWORD_1, KEYWORD_3
</span><span class="sh">"</span><span class="s">MOVIE_3</span><span class="sh">"</span><span class="s">: KEYWORD_4
</span><span class="sh">"</span><span class="s">MOVIE_4</span><span class="sh">"</span><span class="s">: KEYWORD_1, KEYWORD_3, KEYWORD_4
</span><span class="sh">"""</span>
</pre></table></code></div></div><p>Then the user profile is also input into the prompt to let LLM recommend items from the candidate set.</p><p><em>In context learning</em> is a technique that I widely used during my project. It is much cheaper compared to fine-tune LLM, and the performance is also pretty good as long as you have high quality data. From my experience, <em>formatting control</em> is pretty challenge and sometimes could not be 100% solved by explicit instructions or few shot. Sometimes, we need to have some dedicated business code to do some postprocessing on LLM output to parse the part we interested most out.</p><h2 id="go-beyond-in-context-learning-fine-tune-llm-for-recommendation-task"><span class="me-2">Go beyond In-Context Learning: Fine-tune LLM for recommendation task</span><a href="#go-beyond-in-context-learning-fine-tune-llm-for-recommendation-task" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>In context learning is a powerful technique, however, due to the fact that LLM is trained on NLP task instead of recommendation task, its performance is still sometime limited. Using some training data that is specifically constructed for recommendation to fine-tune LLM could help LLM to <em>learn</em> more for recommendation task.</p><p>In <a href="https://arxiv.org/pdf/2305.00447.pdf">TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation</a>, the author proposed a 2-stage fine-tuning framework. In first stage, they leverage <a href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca Tuning</a> to improve LLM’s generalization ability, and then in 2nd stage, they use recommendation training data to do <em>rec tuning</em>. The format of the training data is as follow</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="sa">f</span><span class="sh">"""</span><span class="s">
Task instruction: Given the user</span><span class="sh">'</span><span class="s">s historical interactions, please determine whether the user
will enjoy the target new movie by answering </span><span class="sh">"</span><span class="s">Yes</span><span class="sh">"</span><span class="s"> or </span><span class="sh">"</span><span class="s">No</span><span class="sh">"</span><span class="s">.
Task input:
    - User</span><span class="sh">'</span><span class="s">s liked items: GodFather.
    - User</span><span class="sh">'</span><span class="s">s disliked items: Star Wars.
    - Target new movie: Iron Man.
Task output: No
</span><span class="sh">"""</span>
</pre></table></code></div></div><p>A high level flow is as follow <a href="/assets/tallrec.png" class="popup img-link shimmer"><img src="/assets/tallrec.png" alt="TALLRec" loading="lazy"></a></p><h2 id="work-with-existing-recommendation-models"><span class="me-2">Work with existing Recommendation models</span><a href="#work-with-existing-recommendation-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Besides directly let LLM to output the recommendation from the candidates, we could also use LLM together with existing recommendation models. Use the output of one model as input to another model has been a widely adopted practice in the ranking world, e.g. using the GBDT leave as feature in NN. You could think of that we leverage model to do some compression and preprocessing on the signals, which is similar to traditional feature engineering.</p><p>In <a href="https://arxiv.org/pdf/2307.15780.pdf">LLM-Rec: Personalized Recommendation via Prompting Large Language Models</a>, the author used different prompt to generate various text description from the original content, and then embedding them as additional signals and feed into MLP for ranking together with the original descriptions. Below is a high level architecture of their model</p><p><a href="/assets/llm-rec.png" class="popup img-link shimmer"><img src="/assets/llm-rec.png" alt="LLM-Rec" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a>, <a href="/categories/recommendation-system/">Recommendation System</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=How%20to%20use%20LLM%20for%20recommendation%20task%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FHow-to-use-GPT-for-recommendation-task%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=How%20to%20use%20LLM%20for%20recommendation%20task%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FHow-to-use-GPT-for-recommendation-task%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FHow-to-use-GPT-for-recommendation-task%2F&text=How%20to%20use%20LLM%20for%20recommendation%20task%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-2/">A Random Walk Down Recsys - Part 2</a><li class="text-truncate lh-lg"> <a href="/OpenOneRec-RL/">Learning VERL Part 1 - A Perspective from OpenOneRec</a><li class="text-truncate lh-lg"> <a href="/Recommendation-Paper-2025-Review/">My 2025 Recommendation System Paper Summary</a><li class="text-truncate lh-lg"> <a href="/FSDP2-Code-Walk/">FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</a><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys/">A Random Walk Down Recsys - Part 1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/LLM-LLM-Training-101/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1725174000" data-df="ll" > Sep 1, 2024 </time><h4 class="pt-0 my-2">LLM Training 101</h4><div class="text-muted"><p>这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家...</p></div></div></a></article><article class="col"> <a href="/Recommendation-Paper-2025-Review/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1767340800" data-df="ll" > Jan 2, 2026 </time><h4 class="pt-0 my-2">My 2025 Recommendation System Paper Summary</h4><div class="text-muted"><p>In this post, I would like to share some insights from the paper I have read in year 2025 and summarize some trends over the year. The One The best work I enjoyed this year is the One-series from...</p></div></div></a></article><article class="col"> <a href="/Recsys-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1764230400" data-df="ll" > Nov 27, 2025 </time><h4 class="pt-0 my-2">Recsys 2025 Paper Summary</h4><div class="text-muted"><p>In this post, I would like to summary the paper from Recsys 2025 and share some of my learnings. We would cover several topics such as sequence modeling, cross domain learning as well as LLM integr...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/How-to-Design-Webhook/" class="btn btn-outline-primary" aria-label="Older" ><p>How to Design Webhook</p></a> <a href="/How-to-design-auction-system/" class="btn btn-outline-primary" aria-label="Newer" ><p>How to Design Auction System</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/How-to-use-GPT-for-recommendation-task/'; this.page.identifier = '/How-to-use-GPT-for-recommendation-task/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
