<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="A Random Walk Down Recsys - Part 3" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="Welcome back to the third installment of A Random Walk Down Recsys. This time, all five papers revolve around a single theme: Semantic IDs (SIDs) — how to generate them, how to improve their quality, and how to leverage them effectively in generative recommender (GR) models. The papers span a wide range of ideas: compressing long user sequences through SID hierarchies, injecting reasoning capabilities into SID-based re-ranking, producing better embeddings for SID quantization, handling the unique challenges of live-streaming content, and even bypassing the traditional two-stage SID pipeline entirely with end-to-end MLLM generation." /><meta property="og:description" content="Welcome back to the third installment of A Random Walk Down Recsys. This time, all five papers revolve around a single theme: Semantic IDs (SIDs) — how to generate them, how to improve their quality, and how to leverage them effectively in generative recommender (GR) models. The papers span a wide range of ideas: compressing long user sequences through SID hierarchies, injecting reasoning capabilities into SID-based re-ranking, producing better embeddings for SID quantization, handling the unique challenges of live-streaming content, and even bypassing the traditional two-stage SID pipeline entirely with end-to-end MLLM generation." /><link rel="canonical" href="https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-3/" /><meta property="og:url" content="https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-3/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2026-02-22T00:00:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="A Random Walk Down Recsys - Part 3" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2026-02-22T00:00:00-08:00","datePublished":"2026-02-22T00:00:00-08:00","description":"Welcome back to the third installment of A Random Walk Down Recsys. This time, all five papers revolve around a single theme: Semantic IDs (SIDs) — how to generate them, how to improve their quality, and how to leverage them effectively in generative recommender (GR) models. The papers span a wide range of ideas: compressing long user sequences through SID hierarchies, injecting reasoning capabilities into SID-based re-ranking, producing better embeddings for SID quantization, handling the unique challenges of live-streaming content, and even bypassing the traditional two-stage SID pipeline entirely with end-to-end MLLM generation.","headline":"A Random Walk Down Recsys - Part 3","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-3/"},"url":"https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-3/"}</script><title>A Random Walk Down Recsys - Part 3 | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>A Random Walk Down Recsys - Part 3</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>A Random Walk Down Recsys - Part 3</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1771747200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 22, 2026 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2486 words" > <em>13 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>Welcome back to the third installment of <em>A Random Walk Down Recsys</em>. This time, all five papers revolve around a single theme: <strong>Semantic IDs (SIDs)</strong> — how to generate them, how to improve their quality, and how to leverage them effectively in generative recommender (GR) models. The papers span a wide range of ideas: compressing long user sequences through SID hierarchies, injecting reasoning capabilities into SID-based re-ranking, producing better embeddings for SID quantization, handling the unique challenges of live-streaming content, and even bypassing the traditional two-stage SID pipeline entirely with end-to-end MLLM generation.</p><p>The five papers covered are: <strong>GLASS</strong> (Kuaishou), <strong>Generative Reasoning Re-ranker</strong> (Meta), <strong>QARM V2</strong> (Kuaishou), <strong>OneLive</strong> (Kuaishou), and <strong>End-to-End Semantic ID Generation</strong> (Tencent).</p><h2 id="glass-long-sequence-modeling-via-sid-tier-and-semantic-search"><span class="me-2">GLASS: Long-Sequence Modeling via SID-Tier and Semantic Search</span><a href="#glass-long-sequence-modeling-via-sid-tier-and-semantic-search" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This Kuaishou paper centers on a key insight: the <strong>hierarchical structure of semantic IDs</strong> is an under-exploited resource, particularly the first-level SID token $\text{SID}_1$ and its corresponding codebook $\text{codebook}_1$. GLASS leverages this hierarchy in two ways — compressing long-term user sequences into a compact representation, and guiding the SID decoding process through semantic search.</p><p><a href="/assets/glass.png" class="popup img-link shimmer"><img src="/assets/glass.png" alt="GLASS" loading="lazy"></a></p><h3 id="sid-tier-compressing-long-sequences-via-codebook-heatmaps"><span class="me-2">SID-Tier: Compressing Long Sequences via Codebook Heatmaps</span><a href="#sid-tier-compressing-long-sequences-via-codebook-heatmaps" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Each SID token is mapped to an embedding through a standard embedding lookup. The core innovation is using $\text{codebook}_1$ to compress the information contained in long user behavior sequences. The mechanism works as follows:</p><ol><li>Compute the <strong>cosine similarity</strong> between every item in the long sequence and every vector in $\text{codebook}_1$.<li><strong>Bucket</strong> and count the similarity scores to form a heatmap of size $K_0 \times N$, where $K_0$ is the codebook size and $N$ is the number of buckets.<li>Pass the heatmap through an <strong>MLP</strong> to compress it into a single token — the <strong>SID-Tier token</strong>.</ol><p>This token is appended to the short-term user sequence before being fed into the encoder, effectively injecting a compact summary of long-term interests without the computational cost of attending to the full long sequence.</p><h3 id="semantic-search-decoding"><span class="me-2">Semantic Search Decoding</span><a href="#semantic-search-decoding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>During decoding, GLASS introduces a dual-source cross-attention mechanism:</p><ul><li>The <strong>short-term sequence</strong> serves as one set of keys and values.<li>Based on the generated $\text{SID}_1$, the model <strong>searches</strong> the user’s long-term history for items whose first SID token matches $\text{SID}_1$, forming a second set of keys and values.<li>The decoded SID token acts as the query, performing <strong>cross-attention</strong> against both sources.<li>A <strong>gating function</strong> then computes a weighted combination of the short-term and long-term attention outputs.</ul><p>The paper frames this retrieval mechanism as RAG-like, though it is more reminiscent of Search-based Interest Models (SIM) in spirit.</p><h3 id="handling-sparse-retrieval"><span class="me-2">Handling Sparse Retrieval</span><a href="#handling-sparse-retrieval" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>When the long sequence is short or the codebook is large, the SID-based retrieval may return too few items. Two solutions are proposed:</p><ul><li><strong>Approximate matching</strong>: Include items with similar (but not identical) first-level SIDs.<li><strong>Smaller $\text{codebook}_1$</strong>: Use a codebook with fewer entries to increase the number of items per bucket. Interestingly, this is the opposite direction from what PLUM advocates.</ul><h3 id="ablation-insights"><span class="me-2">Ablation Insights</span><a href="#ablation-insights" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The ablation study reveals that GLASS achieves larger improvements on the Taobao dataset than on KuaiRec. The authors attribute this to the fact that Taobao embeddings were generated through <strong>supervised contrastive learning</strong>, which produces higher-quality representations — a reminder that SID quality is fundamentally bounded by the quality of the input embeddings.</p><h2 id="generative-reasoning-re-ranker"><span class="me-2">Generative Reasoning Re-ranker</span><a href="#generative-reasoning-re-ranker" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This Meta paper explores using generative models with semantic IDs for <strong>re-ranking</strong>, incorporating chain-of-thought reasoning into the generation process. The work has been evaluated on Amazon datasets but does not report production deployment results.</p><h3 id="semantic-id-training"><span class="me-2">Semantic ID Training</span><a href="#semantic-id-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The paper does not specify the exact input embeddings, though they appear to be text-based item description embeddings. The focus is instead on improving <strong>codebook utilization</strong> through several techniques, ranked by contribution:</p><ol><li><strong>EMA update</strong> (highest impact): Exponential moving average updates to the codebook vectors, which effectively prevents codebook collapse — a well-known failure mode where most entries go unused.<li><strong>Random last-M codebook</strong> (second highest): During inference, the last $M$ codebook layers use random assignment. This introduces diversity in the tail layers where exact assignment matters less.<li><strong>Dead code reset</strong>: If a codebook entry goes unused for several batches, it is reset to the input vector that is farthest from any existing entry.<li><strong>K-Means initialization</strong>: Standard practice for initializing codebook entries.<li><strong>Diversity loss</strong>: A regularization term encouraging uniform codebook usage, though its impact is marginal.</ol><p>Additionally, a <strong>contrastive loss</strong> is introduced during SID training, which yields a substantial improvement in downstream re-ranking performance.</p><h3 id="pre-training"><span class="me-2">Pre-Training</span><a href="#pre-training" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The item alignment stage interleaves textual descriptions with SID tokens and trains via <strong>next-token prediction (NTP)</strong> — the same approach as Continual Pre-Training (CPT) in PLUM. This aligns the SID representations with the LLM’s latent space.</p><h3 id="post-training-reasoning-trace-generation"><span class="me-2">Post-Training: Reasoning Trace Generation</span><a href="#post-training-reasoning-trace-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>A major contribution is the construction of <strong>reasoning traces</strong> for training. A teacher model is prompted to generate structured reasoning data through two sampling strategies:</p><ul><li><strong>Target sampling</strong>: The ground-truth item is provided, and the teacher generates a reasoning path toward it.<li><strong>Rejection sampling</strong>: No ground truth is given; the teacher performs multiple inference rounds until it produces a correct recommendation.</ul><p>The reasoning traces follow a structured multi-step format:</p><ul><li>System role and task definition<li><strong>Collaborative context</strong>: The user’s purchase history<li><strong>Domain knowledge priming</strong>: E-commerce common sense<li><strong>Critical guidelines</strong>: Output constraints such as requiring SID citations<li><strong>Structured reasoning steps</strong>: Guided steps such as identifying product type, analyzing user preferences, etc.</ul><h3 id="reinforcement-learning-with-dapo"><span class="me-2">Reinforcement Learning with DAPO</span><a href="#reinforcement-learning-with-dapo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The RL stage uses <strong>DAPO</strong> (Dynamic Advantage Policy Optimization), which is similar to GRPO but addresses entropy collapse and rollout length bias by using <strong>separate upper and lower clip thresholds</strong>. The training combines two losses:</p><ul><li>A <strong>reasoning trace loss</strong> for the quality of the generated reasoning chain.<li>A <strong>ranking loss</strong> for the correctness of the final recommendation.</ul><p>The reward signal comes from the <strong>prompted distance</strong> in the re-ranker, supplemented by a <strong>format reward</strong> to ensure structural compliance of the output.</p><h2 id="qarm-v2-multi-modal-recommendation-with-improved-embeddings-and-sid-quantization"><span class="me-2">QARM V2: Multi-Modal Recommendation with Improved Embeddings and SID Quantization</span><a href="#qarm-v2-multi-modal-recommendation-with-improved-embeddings-and-sid-quantization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This Kuaishou paper tackles two interconnected problems in generative recommendation: (1) how to make LLMs better embedding generators for recommendation tasks, and (2) how to reduce SID collisions through improved quantization strategies.</p><p><a href="/assets/qarm_v2.png" class="popup img-link shimmer"><img src="/assets/qarm_v2.png" alt="QARM V2" loading="lazy"></a></p><h3 id="making-llms-better-embedding-generators"><span class="me-2">Making LLMs Better Embedding Generators</span><a href="#making-llms-better-embedding-generators" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>LLM embeddings are not inherently optimized for search, advertising, and recommendation tasks. QARM V2 proposes a pipeline to address this.</p><h4 id="data-denoising-via-llm-reasoning"><span class="me-2">Data Denoising via LLM Reasoning</span><a href="#data-denoising-via-llm-reasoning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Item pairs derived from I2I (Swing) and U2I (Two-Tower) models are inherently noisy. The paper uses LLM reasoning to filter these pairs:</p><ul><li><strong>I2I pairs</strong>: ~10% are filtered out as noise.<li><strong>U2I pairs</strong>: ~70% are filtered out — a striking difference that highlights how much noisier user-item collaborative signals can be.</ul><p>The surviving pairs are then used to generate <strong>QA pairs</strong> through reasoning, which serve as auxiliary training data.</p><h4 id="hybrid-training-with-item-embed-qa-format"><span class="me-2">Hybrid Training with Item-Embed-QA Format</span><a href="#hybrid-training-with-item-embed-qa-format" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The embedding generation adopts a structured input format: <code class="language-plaintext highlighter-rouge">&lt;item segment&gt;&lt;embed token&gt;&lt;qa segment&gt;</code>. The attention flow is designed so that:</p><ul><li>The <code class="language-plaintext highlighter-rouge">&lt;embed token&gt;</code> attends to the <code class="language-plaintext highlighter-rouge">&lt;item segment&gt;</code> to absorb item information.<li>The <code class="language-plaintext highlighter-rouge">&lt;qa segment&gt;</code> attends to the <code class="language-plaintext highlighter-rouge">&lt;embed token&gt;</code> to leverage the condensed representation.</ul><p>The <code class="language-plaintext highlighter-rouge">&lt;embed token&gt;</code> outputs are pooled and trained with <strong>in-batch contrastive loss</strong>, while the QA segment is trained with standard <strong>NTP loss</strong>. This hybrid approach preserves both contrastive training and language modeling objectives, outperforming pure contrastive learning alone.</p><h3 id="sid-quantization-rq-kmeans--fsq"><span class="me-2">SID Quantization: RQ-KMeans + FSQ</span><a href="#sid-quantization-rq-kmeans--fsq" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>A key observation is that the first two layers of RQ-KMeans already capture <strong>categorical information</strong> well. The last layer, therefore, should focus on maximizing item-level diversity to minimize SID collisions. QARM V2 adopts a hybrid quantization strategy:</p><ul><li><strong>2 layers of RQ-KMeans</strong> for the coarse hierarchical structure.<li><strong>Finite Scalar Quantization (FSQ)</strong> for the final layer to maximize diversity.</ul><p>Training is performed on <strong>10M samples</strong> in a single batch rather than online mini-batch updates, which helps produce more stable codebooks and reduces collisions.</p><h3 id="usage-in-the-recommendation-pipeline"><span class="me-2">Usage in the Recommendation Pipeline</span><a href="#usage-in-the-recommendation-pipeline" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The embeddings and SIDs serve different roles in a cascaded retrieval architecture:</p><ul><li><strong>Embeddings</strong> power the <strong>GSU</strong> (General Search Unit / coarse retrieval) stage.<li><strong>SIDs</strong> are used as individual tokens in the <strong>ESU</strong> (Exact Search Unit / fine-grained ranking) stage.</ul><h3 id="practical-observations"><span class="me-2">Practical Observations</span><a href="#practical-observations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Even with RQ-KMeans + FSQ, the collision rate remains at <strong>32%</strong> — a sobering number that reinforces the difficulty of achieving low collisions in large-scale item catalogs. On the serving side, QARM V2 uses a <strong>graph engine</strong> for online SID serving, which could be a useful reference for production deployments.</p><h2 id="onelive-generative-framework-for-live-streaming-recommendation"><span class="me-2">OneLive: Generative Framework for Live-Streaming Recommendation</span><a href="#onelive-generative-framework-for-live-streaming-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This Kuaishou paper extends the generative recommendation paradigm to <strong>live-streaming</strong> scenarios. Live streaming poses unique challenges: content within a single stream changes rapidly, user behavior shifts accordingly, and the strong timeliness of live content demands real-time semantic understanding.</p><p><a href="/assets/onelive.png" class="popup img-link shimmer"><img src="/assets/onelive.png" alt="OneLive" loading="lazy"></a></p><h3 id="dynamic-embedding-generation"><span class="me-2">Dynamic Embedding Generation</span><a href="#dynamic-embedding-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Traditional static embeddings fail for live-streaming content because the semantic meaning of a stream evolves continuously. OneLive addresses this with a <strong>sliding-window MLLM approach</strong>:</p><ol><li>A lightweight MLLM (distilled from a larger teacher LLM) processes live-stream content every <strong>30 seconds</strong> through a sliding window.<li>The generated embeddings are fed into a <strong>dual-tower model</strong> (User Tower and Author Tower).<li>The towers are fine-tuned with <strong>contrastive learning</strong> to align user and author representations in a shared space.</ol><p>SID generation itself uses standard <strong>RQ-KMeans</strong> — the innovation here is entirely in the dynamic embedding pipeline that feeds into it.</p><h3 id="pre-training-architecture"><span class="me-2">Pre-Training Architecture</span><a href="#pre-training-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The backbone follows OneRec V2’s <strong>context processor</strong> design: a decoder-only architecture where the user sequence is processed through a context processor to generate keys and values.</p><p><strong>Temporal Injection</strong>: Time information is injected into every token in the sequence. During decoding, the BOS token also carries temporal information, grounding the generation process in the current time context.</p><p><strong>Multi-Token Prediction</strong>: Inspired by DeepSeek V3, OneLive introduces <strong>multi-token prediction</strong> — something I had been considering as well, and it is encouraging to see it applied in practice. The approach works as follows:</p><ul><li>Two lightweight <strong>auxiliary decoders</strong> (single-layer each) are added alongside the main decoder.<li>During training, all three decoders predict their respective SID tokens ($q_0$, $q_1$, $q_2$) in parallel.<li>Each auxiliary decoder takes the previous decoder’s hidden state and the current preceding SID embedding as input.<li>During inference, the main decoder generates $q_0$, then the auxiliary decoders generate $q_1$ and $q_2$ sequentially.</ul><p>While the inference is still sequential in nature, the auxiliary decoders are far simpler than the main decoder, yielding meaningful latency improvements.</p><h3 id="post-training-with-rl"><span class="me-2">Post-Training with RL</span><a href="#post-training-with-rl" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>A <strong>ranking model</strong> serves as the reward model for RL training. The paper compares DPO and GRPO, finding that <strong>GRPO consistently outperforms DPO</strong>.</p><p>An interesting engineering note: RL is integrated into the <strong>offline stream training</strong> pipeline. This raises practical questions about implementation — for instance, how to combine VERL with FSDP-based model backends for efficient distributed RL training.</p><h2 id="end-to-end-semantic-id-generation-for-generative-advertisement-recommendation"><span class="me-2">End-to-End Semantic ID Generation for Generative Advertisement Recommendation</span><a href="#end-to-end-semantic-id-generation-for-generative-advertisement-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This Tencent paper challenges the conventional two-stage SID pipeline (first generate embeddings, then quantize via RQ-VAE). Instead, it proposes <strong>fine-tuning an MLLM to directly generate SID tokens</strong> from multi-modal inputs in a single end-to-end framework.</p><p><a href="/assets/tencent-e2e-sid.png" class="popup img-link shimmer"><img src="/assets/tencent-e2e-sid.png" alt="End-to-End Semantic ID Generation" loading="lazy"></a></p><h3 id="training-framework"><span class="me-2">Training Framework</span><a href="#training-framework" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The input to the MLLM is organized as follows:</p><ol><li><strong>Multi-modal tokens</strong>: Ad image, ad title, and product attributes (e.g., product category) are tokenized and concatenated.<li><strong>Task description prompt</strong>: A prefix prompt describing the SID generation task.<li><strong>Learnable SID tokens</strong>: Placeholder tokens that the model learns to fill with appropriate SID values.<li><strong><code class="language-plaintext highlighter-rouge">&lt;EMB&gt;</code> token</strong>: A special token whose hidden state is used for embedding extraction.</ol><h3 id="multi-task-training-objectives"><span class="me-2">Multi-Task Training Objectives</span><a href="#multi-task-training-objectives" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The model is trained with several complementary losses:</p><ul><li><strong>SID projection + contrastive loss</strong>: The MLLM’s output at SID token positions is passed through a <strong>SID head</strong> that projects onto the SID vocabulary (with appropriate partitioning across layers). A contrastive loss aligns SID embeddings of items within the same ad category (positive set).<li><strong>Embedding contrastive loss</strong>: The <code class="language-plaintext highlighter-rouge">&lt;EMB&gt;</code> token’s representation is trained via standard contrastive learning.<li><strong>Ad summary reconstruction</strong>: A frozen large model generates ground-truth summaries from ad attributes. A <strong>reconstruction head</strong> takes the hidden states of both SID and <code class="language-plaintext highlighter-rouge">&lt;EMB&gt;</code> tokens as input and uses them as conditioning for an LLM to reconstruct the summary via NTP. This auxiliary task is conceptually similar to the QA reasoning component in QARM V2 — both use generative reconstruction to ensure that the learned representations retain rich semantic information.</ul><h3 id="why-end-to-end-matters"><span class="me-2">Why End-to-End Matters</span><a href="#why-end-to-end-matters" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>By unifying embedding generation and SID quantization into a single MLLM forward pass, this approach avoids the information loss inherent in the two-stage pipeline, where the quantization step is disconnected from the upstream embedding objective. The MLLM can jointly optimize for both representation quality and quantization fidelity.</p><h2 id="comparison-of-semantic-id-approaches"><span class="me-2">Comparison of Semantic ID Approaches</span><a href="#comparison-of-semantic-id-approaches" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>All five papers tackle different facets of the semantic ID problem. The table below summarizes their approaches across key dimensions:</p><div class="table-wrapper"><table><thead><tr><th>Dimension<th>GLASS<th>Reasoning Re-ranker<th>QARM V2<th>OneLive<th>E2E SID (Tencent)<tbody><tr><td><strong>SID Generation</strong><td>Standard (not the focus)<td>RQ-KMeans with codebook utilization tricks<td>2-layer RQ-KMeans + FSQ<td>RQ-KMeans on dynamic embeddings<td>End-to-end MLLM generation<tr><td><strong>Input Embedding</strong><td>External (quality-dependent)<td>Text-based<td>LLM with hybrid CL + NTP training<td>Sliding-window MLLM + dual-tower CL<td>Multi-modal MLLM (image + text + attributes)<tr><td><strong>Codebook Utilization</strong><td>Smaller codebook as fallback<td>EMA, dead code reset, random last-M, K-Means init<td>FSQ for last layer, 10M-sample batch training<td>Not discussed<td>Learned end-to-end (no explicit codebook tricks)<tr><td><strong>Collision Handling</strong><td>Approximate SID matching, smaller codebook<td>Random last-M codebook<td>FSQ final layer (still 32% collision)<td>Not discussed<td>Implicit via joint optimization<tr><td><strong>How SID Is Used</strong><td>SID hierarchy for long-sequence compression + decoding search<td>Token-level NTP with reasoning traces<td>Individual tokens in ESU stage<td>Tokens with multi-token prediction<td>Tokens generated directly by MLLM<tr><td><strong>Contrastive Loss</strong><td>Not discussed<td>Yes (large impact on re-ranking)<td>In-batch CL for embeddings<td>Dual-tower CL for embeddings<td>SID-level CL (same-category positives) + embedding CL<tr><td><strong>RL Method</strong><td>N/A<td>DAPO<td>N/A<td>GRPO &gt; DPO<td>N/A<tr><td><strong>Unique Contribution</strong><td>Exploiting SID hierarchy for long-sequence modeling<td>Structured reasoning traces for re-ranking<td>Hybrid CL+NTP embedding training, LLM-based data denoising<td>Dynamic embeddings for live content, multi-token prediction<td>Eliminating the two-stage pipeline entirely</table></div><h3 id="key-observations"><span class="me-2">Key Observations</span><a href="#key-observations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ol><li><p><strong>The two-stage pipeline is not sacred.</strong> Tencent’s end-to-end approach demonstrates that the conventional embedding-then-quantize pipeline can be replaced with a single MLLM forward pass. Whether this scales to the billion-item catalogs of the other papers remains to be seen.</p><li><p><strong>Codebook utilization remains a practical challenge.</strong> Meta’s systematic study shows that EMA updates and random last-M assignment are the most effective techniques. QARM V2’s 32% collision rate even with FSQ underscores that this problem is far from solved at scale.</p><li><p><strong>Embedding quality is the foundation.</strong> GLASS’s ablation (supervised contrastive learning embeddings vs. standard ones) and QARM V2’s hybrid training both confirm that better input embeddings translate directly to better SIDs and better downstream performance.</p><li><p><strong>SID hierarchies are under-exploited.</strong> GLASS uniquely leverages the hierarchical structure of SIDs for both sequence compression and retrieval-guided decoding. Most other works treat SID tokens as flat sequences — there may be significant untapped potential in hierarchy-aware approaches.</p><li><p><strong>Contrastive loss is consistently valuable.</strong> Every paper that introduces contrastive learning (Meta for SID training, QARM V2 for embeddings, OneLive for dual-tower alignment, Tencent for SID + embedding) reports meaningful improvements. This suggests that contrastive objectives are complementary to the standard NTP training regime.</p></ol><blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote><p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/generative-recommender/" class="post-tag no-text-decoration" >generative-recommender</a> <a href="/tags/llm4rec/" class="post-tag no-text-decoration" >llm4rec</a> <a href="/tags/semantic-id/" class="post-tag no-text-decoration" >semantic-id</a> <a href="/tags/user-sequence-modeling/" class="post-tag no-text-decoration" >user-sequence-modeling</a> <a href="/tags/contrastive-learning/" class="post-tag no-text-decoration" >contrastive-learning</a> <a href="/tags/reinforcement-learning/" class="post-tag no-text-decoration" >reinforcement-learning</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=A%20Random%20Walk%20Down%20Recsys%20-%20Part%203%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FA-Random-Walk-Down-Recsys-Part-3%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=A%20Random%20Walk%20Down%20Recsys%20-%20Part%203%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FA-Random-Walk-Down-Recsys-Part-3%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FA-Random-Walk-Down-Recsys-Part-3%2F&text=A%20Random%20Walk%20Down%20Recsys%20-%20Part%203%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-3/">A Random Walk Down Recsys - Part 3</a><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys-Part-2/">A Random Walk Down Recsys - Part 2</a><li class="text-truncate lh-lg"> <a href="/OpenOneRec-RL/">Learning VERL Part 1 - A Perspective from OpenOneRec</a><li class="text-truncate lh-lg"> <a href="/Recommendation-Paper-2025-Review/">My 2025 Recommendation System Paper Summary</a><li class="text-truncate lh-lg"> <a href="/FSDP2-Code-Walk/">FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/A-Random-Walk-Down-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1768723200" data-df="ll" > Jan 18, 2026 </time><h4 class="pt-0 my-2">A Random Walk Down Recsys - Part 1</h4><div class="text-muted"><p>This is a new series of blog beyond my conference paper reading blog, in which I would summarize the paper that I found interesting form Arxiv IR section and share my learnings. In this first blog...</p></div></div></a></article><article class="col"> <a href="/A-Random-Walk-Down-Recsys-Part-2/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1770278400" data-df="ll" > Feb 5, 2026 </time><h4 class="pt-0 my-2">A Random Walk Down Recsys - Part 2</h4><div class="text-muted"><p>Welcome back to the second installment of A Random Walk Down Recsys. In this post, I continue surveying interesting papers from the Arxiv IR section, covering five recent works: HyFormer, Token-lev...</p></div></div></a></article><article class="col"> <a href="/Recsys-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1764230400" data-df="ll" > Nov 27, 2025 </time><h4 class="pt-0 my-2">Recsys 2025 Paper Summary</h4><div class="text-muted"><p>In this post, I would like to summary the paper from Recsys 2025 and share some of my learnings. We would cover several topics such as sequence modeling, cross domain learning as well as LLM integr...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/A-Random-Walk-Down-Recsys-Part-2/" class="btn btn-outline-primary" aria-label="Older" ><p>A Random Walk Down Recsys - Part 2</p></a><div class="btn btn-outline-primary disabled" aria-label="Newer"><p>-</p></div></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-3/'; this.page.identifier = '/A-Random-Walk-Down-Recsys-Part-3/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/generative-recommender/">generative-recommender</a> <a class="post-tag btn btn-outline-primary" href="/tags/semantic-id/">semantic-id</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
