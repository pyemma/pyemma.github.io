<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://pyemma.github.io/</id><title>Coding Monkey</title><subtitle>I am a coding monkey, and I am proud of it. I have done lots of work in machine learning area, especially recommendation system and AutoML. This blog summarize my journey to become an expert monkey in distributed system and LLM.</subtitle> <updated>2026-02-05T22:23:44-08:00</updated> <author> <name>Coding Monkey</name> <uri>https://pyemma.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://pyemma.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://pyemma.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator> <rights> © 2026 Coding Monkey </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>A Random Walk Down Recsys - Part 2</title><link href="https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-2/" rel="alternate" type="text/html" title="A Random Walk Down Recsys - Part 2" /><published>2026-02-05T00:00:00-08:00</published> <updated>2026-02-05T00:00:00-08:00</updated> <id>https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-2/</id> <content src="https://pyemma.github.io/A-Random-Walk-Down-Recsys-Part-2/" /> <author> <name>pyemma</name> </author> <category term="Machine Learning" /> <summary>Welcome back to the second installment of A Random Walk Down Recsys. In this post, I continue surveying interesting papers from the Arxiv IR section, covering five recent works: HyFormer, Token-level Collaborative Alignment, OneMall, a Sparse Attention approach for long-term user behaviors, and Farewell to Item IDs. HyFormer: Hybrid Cross-Attention for Sequential and Non-Sequential Features ...</summary> </entry> <entry><title>Learning VERL Part 1 - A Perspective from OpenOneRec</title><link href="https://pyemma.github.io/OpenOneRec-RL/" rel="alternate" type="text/html" title="Learning VERL Part 1 - A Perspective from OpenOneRec" /><published>2026-01-31T00:00:00-08:00</published> <updated>2026-01-31T00:00:00-08:00</updated> <id>https://pyemma.github.io/OpenOneRec-RL/</id> <content src="https://pyemma.github.io/OpenOneRec-RL/" /> <author> <name>pyemma</name> </author> <category term="Machine Learning" /> <summary>This post documents my journey learning VERL (Volcano Engine Reinforcement Learning), a scalable and efficient reinforcement learning framework, through the lens of OpenOneRec’s implementation. OpenOneRec uses VERL to implement PPO-based training for recommendation systems with a two-stage generation approach (Chain-of-Thought reasoning followed by item ID generation). VERL provides a sophisti...</summary> </entry> <entry><title>A Random Walk Down Recsys - Part 1</title><link href="https://pyemma.github.io/A-Random-Walk-Down-Recsys/" rel="alternate" type="text/html" title="A Random Walk Down Recsys - Part 1" /><published>2026-01-18T00:00:00-08:00</published> <updated>2026-01-18T00:00:00-08:00</updated> <id>https://pyemma.github.io/A-Random-Walk-Down-Recsys/</id> <content src="https://pyemma.github.io/A-Random-Walk-Down-Recsys/" /> <author> <name>pyemma</name> </author> <category term="Machine Learning" /> <summary>This is a new series of blog beyond my conference paper reading blog, in which I would summarize the paper that I found interesting form Arxiv IR section and share my learnings. In this first blog, I would like to summarize key insights from four papers that represent the current state-of-the-art in generative recommendation: OpenOneRec, OxygenRec, Meta’s Efficient Sequential Recommendation, a...</summary> </entry> <entry><title>FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</title><link href="https://pyemma.github.io/FSDP2-Code-Walk/" rel="alternate" type="text/html" title="FSDP2 Under the Hood - A Deep Dive into PyTorch&amp;apos;s Fully Sharded Data Parallel Implementation" /><published>2026-01-03T00:00:00-08:00</published> <updated>2026-01-18T16:11:30-08:00</updated> <id>https://pyemma.github.io/FSDP2-Code-Walk/</id> <content src="https://pyemma.github.io/FSDP2-Code-Walk/" /> <author> <name>pyemma</name> </author> <category term="Distributed Training" /> <summary>Fully Sharded Data Parallel (FSDP) is PyTorch’s approach to training large models that don’t fit in a single GPU’s memory. FSDP2 represents a significant redesign from FSDP1, with improved performance, better composability, and a cleaner architecture built on top of PyTorch’s DTensor abstraction. In this post, I’ll walk through the implementation details of FSDP2, exploring how it shards parame...</summary> </entry> <entry><title>My 2025 Recommendation System Paper Summary</title><link href="https://pyemma.github.io/Recommendation-Paper-2025-Review/" rel="alternate" type="text/html" title="My 2025 Recommendation System Paper Summary" /><published>2026-01-02T00:00:00-08:00</published> <updated>2026-01-18T16:11:30-08:00</updated> <id>https://pyemma.github.io/Recommendation-Paper-2025-Review/</id> <content src="https://pyemma.github.io/Recommendation-Paper-2025-Review/" /> <author> <name>pyemma</name> </author> <category term="Machine Learning" /> <summary>In this post, I would like to share some insights from the paper I have read in year 2025 and summarize some trends over the year. The One The best work I enjoyed this year is the One-series from Kuaishou, such as OneRec, OneSearch, OneRec v2, etc. From high level, the One-series use a single generative recommendation model (which is usually use encoder-decoder or some variation as the backbo...</summary> </entry> </feed>
