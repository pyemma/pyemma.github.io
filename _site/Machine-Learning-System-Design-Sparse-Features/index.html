<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Trend of Sparse Features in Recommendation System" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as reviewing some new ideas about how sparse features are being used in modern recommendation system, especially in the realm of LLM world." /><meta property="og:description" content="In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as reviewing some new ideas about how sparse features are being used in modern recommendation system, especially in the realm of LLM world." /><link rel="canonical" href="https://pyemma.github.io/Machine-Learning-System-Design-Sparse-Features/" /><meta property="og:url" content="https://pyemma.github.io/Machine-Learning-System-Design-Sparse-Features/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-11-03T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Trend of Sparse Features in Recommendation System" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2025-04-25T22:59:06-07:00","datePublished":"2024-11-03T00:00:00-07:00","description":"In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as reviewing some new ideas about how sparse features are being used in modern recommendation system, especially in the realm of LLM world.","headline":"Trend of Sparse Features in Recommendation System","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/Machine-Learning-System-Design-Sparse-Features/"},"url":"https://pyemma.github.io/Machine-Learning-System-Design-Sparse-Features/"}</script><title>Trend of Sparse Features in Recommendation System | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a>
</h1>
<p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta &amp; LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav">
<li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a>
</li>
<li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a>
</li>
<li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a>
</li>
<li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a>
</li>
<li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a>
</li>
</ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer"> <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href%20=%20'mailto:'%20+%20['pyemma1991','gmail.com'].join('@')" aria-label="email"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss"> <i class="fas fa-rss"></i> </a>
</div></aside><div id="main-wrapper" class="d-flex justify-content-center">
<div class="container d-flex flex-column px-xxl-5">
<header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100">
<nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Trend of Sparse Features in Recommendation System</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div>
<button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
</div></header><div class="row flex-grow-1">
<main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Trend of Sparse Features in Recommendation System</h1>
<div class="post-meta text-muted"> <span> Posted <time data-ts="1730617200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom"> Nov 3, 2024 </time> </span> <span> Updated <time data-ts="1745647146" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom"> Apr 25, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2738 words"> <em>15 min</em> read</span>
</div>
</div>
<div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" alt="Views" loading="lazy"></a> </span>
</div>
</div></header><div class="content">
<p>In <a href="/Features-in-Recommendation-System/">my pervious post</a>, I have briefly mentioned about <em>sparse features</em> and how they could be used in recommendation system. In this post, let’s have a deeper look into <em>sparse features</em>, as well as reviewing some new ideas about how <em>sparse features</em> are being used in modern recommendation system, especially in the realm of LLM world.</p>
<h2 id="what-is-sparse-features">
<span class="me-2">What is sparse features</span><a href="#what-is-sparse-features" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h2>
<p>Sparse features are usually composed as <strong>some entity ids</strong> that a user has <em>taken certain action</em> during <em>a period of time</em>. For example, the facebook post ids that a user has viewed in the last 24 hours or the amazon product ids that a user has purchased in last week. These ids would be feed into model as input to generate predictions. Usually, we would go through a component called <code class="language-plaintext highlighter-rouge">Embedding Lookup Table</code> to convert these raw meaningless ids into some dense vectors so that they could be numerically computed with other features or layer weights. The <code class="language-plaintext highlighter-rouge">Embedding Lookup Table</code> would be updated along with other model parameters during training stage. A simple illustration is as follow</p>
<p><a href="/assets/embedding-lookup.png" class="popup img-link shimmer"><img src="/assets/embedding-lookup.png" alt="Embedding Lookup" loading="lazy"></a></p>
<p>If you come from XGBoost world, you might be as surprised as I was when seeing how ids are being used as feature into a model. XGBoost could not directly use id as input features. To leverage these ids, we usually need to manually crafting a <em>dense version</em> of these ids, for example, aggregating the number of posts that the user has viewed in the last 24 hours. If we need to have some more <em>fine granularity view</em>, a common practice in industry is to add additional <strong>breakdowns</strong>, e.g. breakdown on the category of posts.</p>
<p>However, one obvious limitation to this feature engineering methodology is that it lose <em>memorization</em> of user interactions and lose subtle differences among entities. For example, only knowing a user has viewed 3 posts related to sports is much less predictive than knowing the user has viewed 3 posts from NBA official account; and even though the post is related to basketball, users reaction to NBA and to CBA could be pretty different. Representing entities in their raw ids, converting them to dense vectors (a.k.a embeddings) and training with the target dataset (e.g. user clicks) could help <em>memorize</em> user interactions and learn a better representation of these entities in the target domain.</p>
<p>Recently I have watched a talk given by <a href="https://www.youtube.com/watch?v=kYWUEV_e2ss">Hyung Won Chung in MIT</a> and one opinion from him impress me a lot regarding why GPT4 or other large language model suddenly demonstrate such powerful performance:</p>
<blockquote><p>We should enforce less structure in learning; instead, we should leverage the more and more easily accessible computing power to learn from unstructured data</p></blockquote>
<p>I think the adoption of sparse features in recommendation system also matches to this, where we deprecate the structure part (feature engineering) and move towards less structure (raw ids) and more computing (more capacity of the model).</p>
<h2 id="recent-works-on-sparse-features">
<span class="me-2">Recent works on sparse features</span><a href="#recent-works-on-sparse-features" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h2>
<p>As we have a brief review on what is sparse feature, let’s move to some recent works on sparse feature, or embedding, and see what kind of problem they are trying to resolve. I hope that this could help you borden your ideas and benefit you either for the preparation of machine learning design interview or for ML projects that you are working on.</p>
<h3 id="shareability-of-embeddings">
<span class="me-2">Shareability of embeddings</span><a href="#shareability-of-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<p>Making the embeddings learnt from one domain to be reusable in another domain has been a pretty popular research topic in industry due to a practical reason: memorization of history knowledge without full retraining. For example, reuse the same sparse features’ embedding from CTR model to CVR model; or reuse the embeddings from last month’s model to new release candidate to carry-over the <em>past knowledge</em>. However, naively extracting the embeddings from source model and integrating them into target model does not work well. A technique called <em>transfer learning</em> is usually used to mitigate this type of <em>incompatibility</em> issue.</p>
<p>In this <a href="https://arxiv.org/pdf/2408.16238">paper from Meituan</a> (a Chinese company similar to DoorDash and has pretty strong technique in cross-domain recommendation), the author proposed a new way to transfer learning from source domain to target domain. They leveraged the idea of <em>pre-trained model</em> dominating LLM world and the <em>hierarchy philosophy</em> for information compression:</p>
<ul>
<li>First, a model called <em>tiny pre-trained model (TPM)</em> is trained. <em>TPM</em> is trained with simple model architecture, less number of features but large volume of data (past 6 months). Each month’s snapshot of embedding is extracted and stored separately</li>
<li>Second, a model called <em>complete pre-trained model (CPM)</em> is trained. <em>CPM</em> is trained with the exact same architecture and features against the target domain model on past 1 month of source domain data. The embedding from <em>TPM</em> is adopted for <em>CPM</em> model training (bootstrapping) via an attention pooling. The training of <em>CPM</em> help mitigate the issue of <em>full-qualified-name mismatch</em> issue during model parameter initialization with the cost of flexibility</li>
<li>Finally, the <em>target domain model</em> is trained. The model is initialized with the parameters from <em>CPM</em> (for both embeddings and dense parameters, batch norm layer’s is dropped due to different traffic) and then do incremental training on the past several days data on target domain</li>
<li>
<em>TPM</em> helps memorize long term history, and is refreshed monthly; <em>CPM</em> helps to fuse long term and short term memory, and is refreshed weekly; <em>target domain model</em> adapt to latest online traffic, and is refreshed daily</li>
</ul>
<p><a href="/assets/meituan-cross-domain-embedding.png" class="popup img-link shimmer"><img src="/assets/meituan-cross-domain-embedding.png" alt="Meituan Cross Domain Embedding" loading="lazy"></a></p>
<p>The idea of this method is not very complex. In my pervious company, we have been trying to use the embeddings from production model to bootstrap the performance of new model candidates so that it could catch up more quickly. This method requires less cost compared to <em>knowledge distillation</em> and <em>data augmentation</em>, but putting more pressure on MLOps as well as making the model development cycle much more complex.</p>
<h3 id="generalizability-of-embeddings">
<span class="me-2">Generalizability of embeddings</span><a href="#generalizability-of-embeddings" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<p>As mentioned above, embeddings in recommendation system are usually learnt from the training dataset, which is composed of user’s behavior information against the candidates. Actually, we could assume that the embedding learnt is trying to capture the <em>collaborative signal</em> from users’ behavior. It is similar to the non-model based approach such as <strong>Collaborative Filtering</strong>, which is also trying to compute a vector representation for each user/item for the matrix filling task. This actually puts some limitation on the <em>generalizability</em> of embeddings. For example, the learnt embedding of a pizza restaurant in Sunnyvale might be quite different from a pizza restaurant in Beijing because of the difference in the population or culture (which is observed via data we collected), even though their branding name includes <em>pizza text</em> which is an obvious information for humans to understand that these two restaurant should be the same (I was asked a similar question during my ML interview with Airbnb <img class="emoji" title=":joy_cat:" alt=":joy_cat:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f639.png" height="20" width="20">). In the realm of LLM, we have more powerful tool to process such text/image information and extract their semantic information. How we could better integrate such semantic information into traditional id embeddings has been a popular topic recently.</p>
<p>In this paper <a href="https://arxiv.org/pdf/2408.11523">LARR from Meituan</a> (yes, besides cross-domain problem, they also have cross-scene POI problem), the author proposed one approach to align the semantic embedding generated from LLM with the collaborative id embedding learnt from recommendation system, to improve the performance under realtime scene recommendation scenario. The main idea is to first generate the semantic embedding via text constructed via heuristic rules and then leverage contrastive learning to align the embeddings:</p>
<ul>
<li>First, a LLM is fine-tuned with the rich textual data available from the entities, e.g. the name and description of the restaurant. During the generation of the input into LLM, different textual feature is separated with different special token to help LLM tell apart them (a pretty common practice). In their setup, the input text is the name and location of the restaurant, and they try to predict the description and menu of the restaurant. This helps LLM to learn the connection between location and associated dishes</li>
<li>In the second stage, the fine-tuned LLM would be used to generate embedding for different entities and further trained via contrastive learning. The embedding of last token from the input sequence would be projected via a MLP layer, and then scored through a similarity function (e.g. cosin) with another embedding. They preformed user profile &lt;-&gt; user profile, POI &lt;-&gt; POI and user profile &lt;-&gt; POI contrastive learning. The final loss of this training stage is a linear combination of these 3 contrastive learning loss, which are all similar to</li>
</ul>\[L = - \frac{1}{N} \sum_{i=1}^{N} \frac{\exp(s(u, u_{+}))}{\exp(s(u, u_{+})) + \sum \exp(s(u, u_{-}))}\]<ul><li>In the last stage, the parameters of LLM are freezed and used to generate semantic embedding from realtime scene text features. However, these embeddings lack collaborative signal and thus require some additional processing. Due to the inference limitation, only 10 realtime scene text feature is used, each of them would be processed through LLM, and then one additional bi-directional encoder is used to aggregate these 10 embeddings, with the information summarized into a special token <code class="language-plaintext highlighter-rouge">&lt;agg&gt;</code>. Finally a MLP is used to project the aggregation into the same dimension of collaborative id embeddings, and then one additional contrastive learning is adopted (so many CL <img class="emoji" title=":laughing:" alt=":laughing:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f606.png" height="20" width="20">).</li></ul>
<p><a href="/assets/meituan-larr.png" class="popup img-link shimmer"><img src="/assets/meituan-larr.png" alt="LARR demostration" loading="lazy"></a></p>
<p>Google recently also published <a href="https://arxiv.org/pdf/2409.11699">one paper</a> on how to better leverage the text information of ids to boost the traditional collaborative id embeddings. In this work, they adopted masked language modeling and CLIP style contrastive learning to make the long tail ids benefit from the popular ids. For example, <em>Pizza Hut</em> and <em>Coding Monkey Pizza</em> are both pizza restaurant, but <em>Coding Monkey Pizza</em> would not receive much exposures (a.k.a impressions) similar to <em>Pizza Hut</em>, and thus we could not learn a good embedding for <em>Coding Monkey Pizza</em> (lack of training samples). However, leverage the text data, such as the description of the restaurant, both <em>Pizza Hut</em> and <em>Coding Monkey Pizza</em> would share similar semantic embeddings, and this would help connect them and help <em>Coding Monkey Pizza</em> to <strong>steal</strong> some information from <em>Pizza Hut</em>. There high level approach is as follow:</p>
<ul>
<li>One part of the training objective comes from the masked language modeling type loss. For a sequence of item ids (e.g. the product id user has purchased), some of them would be randomly masked, and then the ids would go through the embedding lookup table, and then go through the transformer to predict the masked id. The embedding is a combination of the collaborative id embedding and LLM processed text embeddings</li>
<li>Another part of the training objective is the alignment between the collaborative id embedding and its corresponding semantic embedding, via contrastive learning</li>
<li>One additional thing besides the <code class="language-plaintext highlighter-rouge">&lt;id, text&gt;</code> pair is the <em>critique string</em>, which is also some text information, but separately encoded via LLM. This information is not masked during MLM and the reason for that is to encourage model to learn to predict target id via semantic information instead of the memorizing ids</li>
</ul>
<p><a href="/assets/google-flare.png" class="popup img-link shimmer"><img src="/assets/google-flare.png" alt="Google FLARE" loading="lazy"></a></p>
<p>The last example trying to align semantic embedding from LLM and id embedding comes from <a href="https://dl.acm.org/doi/pdf/10.1145/3640457.3688106">Huawei</a>. In this work, they proposed a framework to align this 2 types of embedding, which is suitable for most of the model architecture. Similar to the work from Google, they also adopted MLM for training, but their method steps more towards the CLIP style modeling:</p>
<ul>
<li>For any traditional sparse features, they are going to convert it to a text narrative (different from the Meituan’s work where the text is already available), similar to a combination of <code class="language-plaintext highlighter-rouge">feature name: narrative of feature value</code> pair. Then all sparse features’ text narrative would be concat together. During the training, some random feature is going to be masked. And then, given the LLM embedding on the text narrative, and the masked id embedding, predict the masked feature value (called MTM); or given the id embedding and masked text narrative, predict the masked text tokens (MLM)</li>
<li>In MLM, the id embedding is concat to all text embeddings for prediction; in MTM, cross-attention, pre-feature MLP and InfoNCE is used for prediction</li>
<li>Besides the alignment within instance (feature level), instance level contrastive learning is also used</li>
<li>After aligning both semantic embedding and id embedding, they are fine-tuned with downstream task via a tow-tower architecture</li>
</ul>
<p><a href="/assets/huawei-flip.png" class="popup img-link shimmer"><img src="/assets/huawei-flip.png" alt="Huawei FLIP" loading="lazy"></a></p>
<h3 id="semantic-id">
<span class="me-2">Cardinality of embeddings</span><a href="#semantic-id" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<p>One hidden story I haven’t talk about is how actually a raw id get converted to a dense vector through the <code class="language-plaintext highlighter-rouge">embedding lookup table</code>. In general, the raw id would be converted to an index within the <code class="language-plaintext highlighter-rouge">embedding lookup table</code> and retrieve the corresponding vector. If the total number of raw ids is not that large, we could have a 1-to-1 mapping between ids and index (in another world, the column size of the <code class="language-plaintext highlighter-rouge">embedding lookup table</code> is the same as the number of ids). However, if we have much much more number of ids, it is impossible to have a that large table. In this scenario, we would apply what is called <em>hashing trick</em>: apply a hash function on the id and mod the total column number. This means that a single vector actually represent multiple different ids which might be totally irrelevant with each other: they might be contradict with each other; or the vector is overwhelm by popular ids. In my pervious company, I have asked about this issue and proposed if we could infuse certain category information into id or id hashing function to alleviate this collision issue, but didn’t work it out due to “ruthless prioritization” <img class="emoji" title=":pensive:" alt=":pensive:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f614.png" height="20" width="20">.</p>
<p>Google recently proposed a new solution similar to categorization, which is called <a href="https://dl.acm.org/doi/pdf/10.1145/3640457.3688190">semantic id</a>. The high level idea is to learn a implicit hierarchy structure to index ids, so that the ids sharing similar semantic information would be grouped closer:</p>
<ul>
<li>The solution is composed by 2 parts: the first part learns a model to encode ids; the second part freeze the model learnt and encode ids for downstream model training</li>
<li>In the first part, they adopted RQ-VAE to encode the content embeddings; the codex id within each layer of RQ is concat together to compose the semantic id. Via this approach, the ids that share similar content (because of similar content embedding) would share a similar prefix within the semantic id, but still would preserver some slightly difference in the tail part of the id. The training part follows the VAE, where the codex id’s corresponding embedding is retrieved and summed together and try to reconstruct to the input content embedding</li>
<li>In the second part, based on the semantic id, we would learn another set of embedding based on them. The semantic id is a sequence of codex id, and we could use different strategy to convert them to dense vectors, we could use ngram to create different combinations, or we could use Sentence Piece Model to dynamically group them, and then go through the <code class="language-plaintext highlighter-rouge">embedding lookup table</code> (here we could have a 1-to-1 mapping similar to LLM, instead of using <em>hashing trick</em> again)</li>
</ul>
<p><a href="/assets/rq-vae.png" class="popup img-link shimmer"><img src="/assets/rq-vae.png" alt="RQ-VAE" loading="lazy"></a></p>
<h2 id="my-bet-on-the-future-trend">
<span class="me-2">My bet on the future trend</span><a href="#my-bet-on-the-future-trend" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h2>
<p>After reviewing these work, here is some of my bet on the future of how sparse features or embeddings going to involve in recommendation system:</p>
<ol>
<li>Using LLM’s output as additional input into traditional DRS model would still be the main-stream. More exploration would be done here, such as fine-tune LLM for specific downstream task to generate better semantic embeddings, adopt multi-modal LLM to ingest richer information into RS, leverage more semantic embeddings to resolve the cold-start problem</li>
<li>There would be more work studying how to better combine embedding generated from LLM and traditional id embedding to improve the generalization capability of recommendation system</li>
<li>In the future, the embedding might take more and more responsibility for memory and the dense layers take more on computing and reasoning; better structure of embedding would help encode more implicit information which could be unveiled during computing</li>
<li>Due to the cost of inference of LLM, semantic embedding would probably still be precomputed, and this would put some new challenge on the feature serving and management infrastructure</li>
</ol>
<blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote>
<p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p>
<h2 id="acknowledgement">
<span class="me-2">Acknowledgement</span><a href="#acknowledgement" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h2>
<p>Thanks Yunzhong and Yitong for the great discussion on these papers.</p>
<h2 id="reference">
<span class="me-2">Reference</span><a href="#reference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h2>
<ul>
<li><a href="https://arxiv.org/pdf/2408.16238">Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction</a></li>
<li><a href="https://arxiv.org/pdf/2408.11523">LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding</a></li>
<li><a href="https://arxiv.org/pdf/2409.11699">FLARE: Fusing Language Models and Collaborative Architectures for Recommender Enhancement</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3640457.3688106">FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3640457.3688190">Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</a></li>
</ul>
</div>
<div class="post-tail-wrapper text-muted">
<div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a>
</div>
<div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/machine-learning-design/" class="post-tag no-text-decoration">machine learning design</a> <a href="/tags/sparse-features/" class="post-tag no-text-decoration">sparse features</a> <a href="/tags/embeddings/" class="post-tag no-text-decoration">embeddings</a>
</div>
<div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 ">
<div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div>
<div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Trend%20of%20Sparse%20Features%20in%20Recommendation%20System%20-%20Coding%20Monkey&amp;url=https%3A%2F%2Fpyemma.github.io%2FMachine-Learning-System-Design-Sparse-Features%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Trend%20of%20Sparse%20Features%20in%20Recommendation%20System%20-%20Coding%20Monkey&amp;u=https%3A%2F%2Fpyemma.github.io%2FMachine-Learning-System-Design-Sparse-Features%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FMachine-Learning-System-Design-Sparse-Features%2F&amp;text=Trend%20of%20Sparse%20Features%20in%20Recommendation%20System%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span>
</div>
</div>
</div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access">
<section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2>
<ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
<li class="text-truncate lh-lg"> <a href="/Book-PyTorch-Training-Optimization/">PyTorch 性能与显存优化手册</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Long-User-Sequence-Modeling-In-Recsys/">Recommendation System - Long User Sequence Modeling</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Machine-Learning-System-Design-Sparse-Features/">Trend of Sparse Features in Recommendation System</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Recsys-Paper-Review-2025-Q1/">Recsys Paper Summary 2025 Q1</a>
</li>
<li class="text-truncate lh-lg"> <a href="/How-to-design-slack/">How to Design Slack</a>
</li>
</ul></section><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a>
</div></section>
</div>
<section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2>
<nav id="toc"></nav></section></aside>
</div>
<div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
<aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3>
<nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Recsys-2024-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1735977600" data-df="ll"> Jan 4, 2025 </time><h4 class="pt-0 my-2">Recsys 2024 Paper Summary</h4>
<div class="text-muted"><p>In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research...</p></div>
</div></a></article><article class="col"> <a href="/Long-User-Sequence-Modeling-In-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1751094000" data-df="ll"> Jun 28, 2025 </time><h4 class="pt-0 my-2">Recommendation System - Long User Sequence Modeling</h4>
<div class="text-muted"><p>User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a ...</p></div>
</div></a></article><article class="col"> <a href="/Recsys-Paper-Review-2025-Q1/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1745046000" data-df="ll"> Apr 19, 2025 </time><h4 class="pt-0 my-2">Recsys Paper Summary 2025 Q1</h4>
<div class="text-muted"><p>In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here i...</p></div>
</div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/How-to-design-online-chess-game/" class="btn btn-outline-primary" aria-label="Older"><p>How to Design Online Chess Game</p></a> <a href="/Recsys-2024-Paper-Summary/" class="btn btn-outline-primary" aria-label="Newer"><p>Recsys 2024 Paper Summary</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div>
<script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/Machine-Learning-System-Design-Sparse-Features/'; this.page.identifier = '/Machine-Learning-System-Design-Sparse-Features/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 "><p>© <time>2025</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p>
<p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer>
</div></div>
<div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content">
<div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a>
</div></section></div>
<div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
</div></div>
</div>
<aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside>
</div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false"><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close"></button>
</div>
<div class="toast-body text-center pt-0">
<p class="px-2 mb-3">A new version of content is available.</p>
<button type="button" class="btn btn-primary" aria-label="Update"> Update </button>
</div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&amp;register=true"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
