<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Recsys Paper Summary 2025 Q1" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here is the full list of papers in this summary, which are all available on Arxiv:" /><meta property="og:description" content="In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here is the full list of papers in this summary, which are all available on Arxiv:" /><link rel="canonical" href="https://pyemma.github.io/Recsys-Paper-Review-2025-Q1/" /><meta property="og:url" content="https://pyemma.github.io/Recsys-Paper-Review-2025-Q1/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-04-19T00:00:00-07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Recsys Paper Summary 2025 Q1" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2025-04-19T00:00:00-07:00","datePublished":"2025-04-19T00:00:00-07:00","description":"In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here is the full list of papers in this summary, which are all available on Arxiv:","headline":"Recsys Paper Summary 2025 Q1","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/Recsys-Paper-Review-2025-Q1/"},"url":"https://pyemma.github.io/Recsys-Paper-Review-2025-Q1/"}</script><title>Recsys Paper Summary 2025 Q1 | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a>
</h1>
<p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta &amp; LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav">
<li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a>
</li>
<li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a>
</li>
<li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a>
</li>
<li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a>
</li>
<li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a>
</li>
</ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer"> <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href%20=%20'mailto:'%20+%20['pyemma1991','gmail.com'].join('@')" aria-label="email"> <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss"> <i class="fas fa-rss"></i> </a>
</div></aside><div id="main-wrapper" class="d-flex justify-content-center">
<div class="container d-flex flex-column px-xxl-5">
<header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100">
<nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Recsys Paper Summary 2025 Q1</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div>
<button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button>
</div></header><div class="row flex-grow-1">
<main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Recsys Paper Summary 2025 Q1</h1>
<div class="post-meta text-muted"> <span> Posted <time data-ts="1745046000" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom"> Apr 19, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="2619 words"> <em>14 min</em> read</span>
</div>
</div>
<div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&amp;label=&amp;icon=github&amp;color=%23198754&amp;message=&amp;style=flat&amp;tz=UTC" alt="Views" loading="lazy"></a> </span>
</div>
</div></header><div class="content">
<p>In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here is the full list of papers in this summary, which are all available on <a href="https://arxiv.org/">Arxiv</a>:</p>
<ul>
<li><a href="https://arxiv.org/pdf/2501.13344">Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation</a></li>
<li><a href="https://github.com/Applied-Machine-Learning-Lab/NoteLLM">NoteLLM-2: Multimodal Large Representation Models for Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2503.04162">Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2503.12183">Bridging Textual-Collaborative Gap through Semantic Codes for Sequential Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2501.17670">Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation</a></li>
<li><a href="https://arxiv.org/pdf/2502.05561">Diffusion Model for Interest Refinement in Multi-Interest Recommendation</a></li>
<li><a href="https://arxiv.org/abs/2502.17494">External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation</a></li>
<li><a href="https://arxiv.org/abs/2504.05217">LLM-Alignment Live-Streaming Recommendation</a></li>
</ul>
<p>Recently, there is a trend that tries to integrate diffusion process into user sequential behavior modeling which is a new idea to me. Besides that, I’m also seeing an increased trends on utilizing semantic ids (if you are not familiar with this concept, please refer <a href="/Machine-Learning-System-Design-Sparse-Features/#semantic-id">this section</a> from my pervious post) to improve the training or serving efficiency. How to better utilize both the semantic embeddings (from LLM or VLM) and id embeddings (from collaborative signals) continuous to be a hot area. And distillation has been drawing eyeballs not only in small LLM, but also in industry to help reduce the inference pressure incurred from larger models.</p>
<h3 id="diffusion">
<span class="me-2">Diffusion</span><a href="#diffusion" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<p>Probably everyone today is already familiar with stable diffusion. It is a popular technique used for image generation in recent years and powers website such as <a href="https://www.midjourney.com/explore?tab=top_month">Midjourney</a>. Due to there strong abilities to model data distribution and generate high-quality items, diffusion models have also been adopted for sequential recommendation.</p>
<h4 id="distinguished-quantized-guidance-for-diffusion-based-sequence-recommendation">
<span class="me-2">Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation</span><a href="#distinguished-quantized-guidance-for-diffusion-based-sequence-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>In this work from Kuaishou, a famous Chinese company in short video/live recommendation track, the author proposed some augmentation to how diffusion model is being applied to sequential modeling. Traditionally, noise is added to the next item and user’s interaction sequence is used to progressively denoise it. But user’s interaction sequence usually contains some noisy items due to stochastic user behaviors, or the sequence could be pretty sparse (a.k.a short) to provide meaning information to the denoise process, which would hind this denoise process. One solution proposed in this work is to use <em>vector-quantization</em> to encode and augment the original user sequence.</p>
<p>Given a user behavior sequence $s = [x_{1}, x_{2}, \dots, x_{L-1}] \in R^{(L-1) \times D}$, which we have already converted the item id to the item embedding; we would match against to a semantic codebook which is defined as $C = \{ c_m \}$, and $c_m \in R^{(L-1) \times D}$. To find $m^*$ that best matches the to the original input, <em>sampling from predicted vector distribution</em> approach is used. In this approach, $s$ is feed into a MLP to generate logits of size $M$ (corresponding to the dimension of cookbook) and then <em>Gumbel-Softmax technique</em> (pretty similar to softmax with temperature, to resolve the in-differential problem of argmin) is used to find $m^*$. Then the quantized sequence is defined as $s_{q} = c_{m}$ and combined with origin input sequence $s$ with a controllable rate. The codebook is also trained via <em>expectation-maximization</em> approach, which is a commonly used optimization algorithm for such <em>clustering-alike</em> process. During the training process, we would update $c_i$ using the average of all $s$ that is assigned with $i$-th codebook. The picture below highlights the overall process of this <em>vector-quantization</em>. Through this process, we reduce the noise in the original sequence by dragging it towards a more <em>common</em> representation across the entire user space (similar to a cluster centric, all user that assigned to the same codebook belongs to certain pattern); as well as using this <em>common</em> representation to augment for the sequence that is sparse.</p>
<p><a href="/assets/semantic-vector-quantization.png" class="popup img-link shimmer"><img src="/assets/semantic-vector-quantization.png" alt="Semantic Vector Quantization" width="400" height="300" loading="lazy"></a></p>
<p>Another technique proposed in this work is to add one additional contrastive loss in the original reconstruction loss so that we could enforce the diffusion process to be less bias to popular items in the data and yield more personalized interests for each user.</p>
<p>The original paper contains lots of mathematic, which might be a little bit intimidate to read if you are not familiar with the original stable diffusion work. I would recommend this <a href="https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166">blog</a> to learn the basics.</p>
<h4 id="diffusion-model-for-interest-refinement-in-multi-interest-recommendation">
<span class="me-2">Diffusion Model for Interest Refinement in Multi-Interest Recommendation</span><a href="#diffusion-model-for-interest-refinement-in-multi-interest-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This work is from Xiaohongshu, which is a famous Chinese company similar to Pinterest. In this work, the diffusion process to improve the multi-interest embeddings extracted from user sequence, which is called <strong>Diffusion Multi-Interest Model</strong>, to make each individual interest embedding is more <em>clear</em> and contains less noise polluted from users’ multi-interest convoluted interest.</p>
<p>The work starts with apply self-attention on the user history sequence $H \in R^{T \times d}$ by using learnable parameters to compute the attention scores $A \in R^{K \times T} $ as</p>\[A = softmax(W_2 tanh(W_1H^T))\]<p>where $K$ stands for the number of interests. Then the interest vector could be obtained as $V = AH$. Once interest vector is obtained, the next step is to leverage the diffusion step to denoise it. The logic here is a little bit complicated and has lots of mathematics, but the overall flow could be described as follow:</p>
<ol>
<li>For a given item, find the interest vector that is most close to it (out of total of $K$) as $v_0$</li>
<li>Compute $v_t$ based on $v_0$ and sampled step $t$, this is the forward step</li>
<li>Reconstruct $\hat{v_0}$ via the denoising module. This module uses the cross-attention with the original user history sequence embedding and an item-pruning strategy</li>
<li>The reconstructed $\hat{v_0}$ and the original interest vector is combined together as the final user representation</li>
<li>This final user representation is used for loss computation against the item during training, or used as query vector during inference</li>
</ol>
<p>I still have some questions regarding how a transformer architecture is used in the reverse process. I will share more details later once I found more resources.</p>
<p><a href="/assets/diffusion-multi-interest.png" class="popup img-link shimmer"><img src="/assets/diffusion-multi-interest.png" alt="Multi-Interest Diffusion" width="600" height="400" loading="lazy"></a></p>
<h3 id="multi-modal">
<span class="me-2">Multi Modal</span><a href="#multi-modal" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="notellm-2-multimodal-large-representation-models-for-recommendation">
<span class="me-2">NoteLLM-2: Multimodal Large Representation Models for Recommendation</span><a href="#notellm-2-multimodal-large-representation-models-for-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>For multi-modal scenarios, such as the post in Xiaohongshu platform which contains both image and text, a traditional approach to model the information from the post is to have encoder to encode the image and text data separately and then use mechanism such as cross attention or weighted fusion to learn. However, this might not be the best option due to the isolation of encoding process and sometimes the vision information might not learnt well in the post hidden state.</p>
<p>This paper from Xiaohongshu proposed a new <strong>prompt</strong> based approach to better learn the vision information and established a new type of multi-modal large language model. First, a special format of prompt is used, which is as follow</p>
<pre><code class="language-raw">Note content: {'image': &lt;IMG&gt;}, Compress this note into one word: "&lt;IMG_EMB&gt;".
Note content: {'title': t_i, 'topic': tp_i, 'content': ct_i}, Compress this note into one word:
</code></pre>
<p>where the <code class="language-plaintext highlighter-rouge">&lt;IMG&gt;</code> token is a special token, which is going to be replaced with the output of a vision encoder after the tokenization step, and <code class="language-plaintext highlighter-rouge">&lt;IMG_EMB&gt;</code> is another special token, which is used to extracted the <em>LLM processed vision embeddings</em>. The embedding of the last token of the prompt is also extracted which is used at the <em>note multi-modal embedding</em>. This step lets LLM leverage its internal knowledge to aggregate the vision and text information. Secondly, to make sure the vision information is preserved, a late fusion approach is adopted to combine the <em>original vision embedding</em> and <em>LLM processed vision embedding</em>, where a gate mechanism is used to fuse them in a learnt way. Finally, contrastive learning is used on both the <em>fused vision embedding</em> and <em>note multi-modal embedding</em> to learn from the data.</p>
<p>The model is deployed offline to process the embedding for the new posts published and store the embeddings into an embedding table. This embedding table is used to extract embedding queries based on user history sequence as well as for building ANN index.</p>
<p><a href="/assets/rednote-multi-modal.png" class="popup img-link shimmer"><img src="/assets/rednote-multi-modal.png" alt="Xiaohongshu Multi Modal" width="400" height="300" loading="lazy"></a></p>
<h3 id="llm-embedding--id-embedding">
<span class="me-2">LLM Embedding &amp; ID Embedding</span><a href="#llm-embedding--id-embedding" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="full-stack-optimized-large-language-models-for-lifelong-sequential-behavior-comprehension-in-recommendation">
<span class="me-2">Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation</span><a href="#full-stack-optimized-large-language-models-for-lifelong-sequential-behavior-comprehension-in-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>One novel idea in this paper is how they combined the LLM embedding and id embedding from traditional recsys model:</p>
<ol>
<li>Train a traditional recsys model to learn the collaborative signal from the dataset and get the id embedding, for item $i$ we have $c_i$</li>
<li>Use LLM to encode the text associated with the item into LLM embedding, which is $E_i = [v_1, v_2, v_3, \dots, v_L]$</li>
<li>Train a projector (e.g. MLP) to project the id embedding into LLM embedding space and append it to the last via soft-prompt $\hat{E_i} = [v_1, v_2, v_3, \dots, v_L, MLP(c_i)]$, the soft-prompt here is just use a special token in the prompt and replace it with the id embedding</li>
</ol>
<p>In this approach, we could merge the collaborative signal and the semantic signal together and let’s LLM to learn from the augmented input.</p>
<h4 id="semantic-retrieval-augmented-contrastive-learning-for-sequential-recommendation">
<span class="me-2">Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation</span><a href="#semantic-retrieval-augmented-contrastive-learning-for-sequential-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>In this paper from Tecent, the researchers proposed a new way to <strong>find</strong> positive samples for contrastive learning, especially for the case of recommendation system scenarios where data sparsity is common.</p>
<p>Traditionally, clustering approach is used for cross-sequence CL and masking is used for intra-sequence CL. However, both methods would be affected by the data sparsity issue. In this paper, semantic embedding obtained from LLM plays a critical role to ensemble positive samples, which depends on the natural text information and is not affected by the sparsity issue.</p>
<p>The idea is relative straight forward. First, user’s sequence is hard prompted into text and then use merchant LLM to summarize; the summarized text is feed into LLM and convert to a semantic embedding. Then the similar users could be identified via this semantic embedding and for each user, we could generate a candidate set. However, this set is generated purely via the semantic information and the actual collaborative signal is missing. Thus additional processing is required so that we could weight each candidate in the set correctly based on the training data we have. The author used a simple attention mechanism and softmax to compute the weight. And the final user positive sample embedding is represented as</p>\[h_{u}^{+} = \sum_{u^\prime \in N_{u}} p_{u, u^\prime}h_{u^\prime}\]<p>Similar approach is used on item side as well, but the positive samples are directly sampled from the candidate set instead of using a learnable approach to merge them together.</p>
<h4 id="bridging-textual-collaborative-gap-through-semantic-codes-for-sequential-recommendation">
<span class="me-2">Bridging Textual-Collaborative Gap through Semantic Codes for Sequential Recommendation</span><a href="#bridging-textual-collaborative-gap-through-semantic-codes-for-sequential-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This paper from Renmin University proposed a new approach to fuse the text embedding and id embedding, to achieve a better trade off between the semantic information and collaborative information. In this method, the semantic code, which is generated via <em>product quantization</em> or <em>residual quantization</em> is used to merge with the text embedding:</p>
<ul>
<li>For each item, convert each attribute’s raw text into text embedding via certain encoder model, $Z^t = [z_1^t, z_2^t, \dots, z_m^t]$</li>
<li>Based on the attribution embedding, use PQ/RQ to quantize and obtain the semantic embedding $Z^c = [z_1^c, z_2^c, \dots, z_n^c]$</li>
<li>For semantic embedding, first apply multi-head attention, and then use attribution embedding as KV to perform cross-attention, which eventually generate $H \in R^{n \times d}$, use a pooling module to convert the hidden state into a single embedding vector, also combine the pooling of semantic embedding to enforce the learning of collaborative signal in those embeddings</li>
</ul>
<p>I think this is the most interesting part: for the semantic coding part, similar item would be enforced to share similar codex and thus the collaborative signal would be shared among these items, achieving shareability; the self-attention and cross-attention provide a more sufficient compute capability for model to learn across different attribute of the item and enforcing the model to learn semantic embedding well instead of solely relay on text embeddings.</p>
<p>For the learning part, the work adopted idea from masked language model, that some semantic code and item in the sequence would be masked and ask the model learn to reconstruct.</p>
<h3 id="distillation">
<span class="me-2">Distillation</span><a href="#distillation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="external-large-foundation-model-how-to-efficiently-serve-trillions-of-parameters-for-online-ads-recommendation">
<span class="me-2">External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation</span><a href="#external-large-foundation-model-how-to-efficiently-serve-trillions-of-parameters-for-online-ads-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>Distillation is a technique used to transfer knowledge from large model into smaller model to achieve a trade off between accuracy and efficiency. For example, in LLM area it is a hot topic to transfer the knowledge in lager language model into smaller language model which is targeted to run on device. In short, the model with larger capacity could learn and memory more patterns from the data which model with small capacity is hard to learn well; thus the output from the larger model (which we also usually called teacher model) could be used as a type of soft label which is easier for the smaller model (which we also called student model) to learn.</p>
<p>In recommendation system, there is also such trend. This paper from Meta pushed this direction to the extreme: they trained a foundation model (FM) billions of parameters, compute intensive architectures and 15x ~ 100x volume of training data and covers all verticals/domains compared to the vertical model (VM) that is serving online. In the paper, they shared the system architecture of their online distillation system, which is called Data Augmentation Service (DAS):</p>
<ul>
<li>The training data is generated in streaming approach, which is for online training</li>
<li>Once the streaming training data is generated, one additional model inference call is sent to offline FM inference to obtain the soft label; once label is get, it is joined with the streaming data again as teacher supervision (my guess here is that they are reusing the same online joining framework)</li>
<li>The data is stored in shared dataset, which could be filtered by VM based on their traffic/sector</li>
<li>To make sure the FM could obtain the latest production traffic, it needs to be regular refreshed with the shared dataset; new snapshot would be published regularly and DAS is responsible for identifying and loading the latest snapshot for offline inference</li>
</ul>
<p><a href="/assets/das.png" class="popup img-link shimmer"><img src="/assets/das.png" alt="Data Augmentation Service" width="600" height="400" loading="lazy"></a></p>
<h3 id="recsys-modeling">
<span class="me-2">Recsys Modeling</span><a href="#recsys-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h3>
<h4 id="llm-alignment-live-streaming-recommendation">
<span class="me-2">LLM-Alignment Live-Streaming Recommendation</span><a href="#llm-alignment-live-streaming-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a>
</h4>
<p>This paper from Kuaishou introduces some challenges in live streaming recommendation, which is some problem that I haven’t encountered. For example, the live streaming is realtime, which means that pre-generating embeddings is not an option and the embeddings has to be generated on the fly when it is alive; also audience might join live at different timestamp and see different screens of the live, which makes it harder to modeling users’ behavior. The author introduced some technique they have been successfully deployed online, which worth to learn about, especially if you are also working in live recommendation:</p>
<ul>
<li>A 7B VLM model is used to generate embedding for live streaming every 30s; the 7B model is fine tuned use the data that is annotated by a powerful 100B in offline</li>
<li>A gating mechanism (similar to the NoteLLM-2 in the above section) is used to fuse the semantic embedding generated from VLM and the author id embedding (for collaborative signal)</li>
<li>To make the online serving efficient, the fused embedding is further quantized and only the codex id to save the online serving storage. The codebook here is generated via a hierarchy K-means approach where a group of author embedding is first clustered into 512 categories, and then the residual part is clustered into 256 categories, and so on. A total of 3 layers of codebook is used. The codex id is used as feature into the deep cross network, which is also used for attention computation.</li>
</ul>
<p>I’m pretty enjoy reading this paper as there not much fancy and complex mathematic involved. Everything is relative straight forward to explain and understand. Highly recommend for a read (and I will also try these tricks in my side project <img class="emoji" title=":smiley_cat:" alt=":smiley_cat:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f63a.png" height="20" width="20">)</p>
<p><a href="/assets/kuaishou-live-streaming.png" class="popup img-link shimmer"><img src="/assets/kuaishou-live-streaming.png" alt="Kuaishou Live Streaming Recsys" width="800" height="800" loading="lazy"></a></p>
<blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote>
<p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p>
</div>
<div class="post-tail-wrapper text-muted">
<div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a>
</div>
<div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/machine-learning-design/" class="post-tag no-text-decoration">machine learning design</a> <a href="/tags/recommendation-system/" class="post-tag no-text-decoration">recommendation-system</a>
</div>
<div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 ">
<div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div>
<div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Recsys%20Paper%20Summary%202025%20Q1%20-%20Coding%20Monkey&amp;url=https%3A%2F%2Fpyemma.github.io%2FRecsys-Paper-Review-2025-Q1%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Recsys%20Paper%20Summary%202025%20Q1%20-%20Coding%20Monkey&amp;u=https%3A%2F%2Fpyemma.github.io%2FRecsys-Paper-Review-2025-Q1%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FRecsys-Paper-Review-2025-Q1%2F&amp;text=Recsys%20Paper%20Summary%202025%20Q1%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span>
</div>
</div>
</div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access">
<section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2>
<ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2">
<li class="text-truncate lh-lg"> <a href="/Recsys-2025-Paper-Summary/">Recsys 2025 Paper Summary</a>
</li>
<li class="text-truncate lh-lg"> <a href="/KDD-2025-Paper-Summary/">KDD 2025 Paper Summary</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Book-PyTorch-Training-Optimization/">PyTorch 性能与显存优化手册</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Long-User-Sequence-Modeling-In-Recsys/">Recommendation System - Long User Sequence Modeling</a>
</li>
<li class="text-truncate lh-lg"> <a href="/Machine-Learning-System-Design-Sparse-Features/">Trend of Sparse Features in Recommendation System</a>
</li>
</ul></section><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine-learning-design</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a>
</div></section>
</div>
<section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2>
<nav id="toc"></nav></section></aside>
</div>
<div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4">
<aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3>
<nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Long-User-Sequence-Modeling-In-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1751094000" data-df="ll"> Jun 28, 2025 </time><h4 class="pt-0 my-2">Recommendation System - Long User Sequence Modeling</h4>
<div class="text-muted"><p>User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a ...</p></div>
</div></a></article><article class="col"> <a href="/Recsys-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1764230400" data-df="ll"> Nov 27, 2025 </time><h4 class="pt-0 my-2">Recsys 2025 Paper Summary</h4>
<div class="text-muted"><p>In this post, I would like to summary the paper from Recsys 2025 and share some of my learnings. We would cover several topics such as sequence modeling, cross domain learning as well as LLM integr...</p></div>
</div></a></article><article class="col"> <a href="/KDD-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1760684400" data-df="ll"> Oct 17, 2025 </time><h4 class="pt-0 my-2">KDD 2025 Paper Summary</h4>
<div class="text-muted"><p>It has been a while since KDD 2025, finally I have had sometime to finish reading all papers that I interested in and summarize some of my learnings in this post <img class="emoji" title=":sweat_smile:" alt=":sweat_smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f605.png" height="20" width="20">. My primary focus is ...</p></div>
</div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/How-to-design-slack/" class="btn btn-outline-primary" aria-label="Older"><p>How to Design Slack</p></a> <a href="/Long-User-Sequence-Modeling-In-Recsys/" class="btn btn-outline-primary" aria-label="Newer"><p>Recommendation System - Long User Sequence Modeling</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div>
<script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/Recsys-Paper-Review-2025-Q1/'; this.page.identifier = '/Recsys-Paper-Review-2025-Q1/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 "><p>© <time>2025</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p>
<p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer>
</div></div>
<div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content">
<div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2>
<div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine-learning-design</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a>
</div></section></div>
<div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
</div></div>
</div>
<aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside>
</div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false"><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close"></button>
</div>
<div class="toast-body text-center pt-0">
<p class="px-2 mb-3">A new version of content is available.</p>
<button type="button" class="btn btn-primary" aria-label="Update"> Update </button>
</div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&amp;register=true"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
