<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Recsys 2024 Paper Summary" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research &amp; study interest and is highly objective. I highly encourage you to review the accepted papers as well and I’m happy to discuss if there is some great paper I missed out." /><meta property="og:description" content="In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research &amp; study interest and is highly objective. I highly encourage you to review the accepted papers as well and I’m happy to discuss if there is some great paper I missed out." /><link rel="canonical" href="https://pyemma.github.io/Recsys-2024-Paper-Summary/" /><meta property="og:url" content="https://pyemma.github.io/Recsys-2024-Paper-Summary/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-01-04T00:00:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Recsys 2024 Paper Summary" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2025-01-04T00:00:00-08:00","datePublished":"2025-01-04T00:00:00-08:00","description":"In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research &amp; study interest and is highly objective. I highly encourage you to review the accepted papers as well and I’m happy to discuss if there is some great paper I missed out.","headline":"Recsys 2024 Paper Summary","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/Recsys-2024-Paper-Summary/"},"url":"https://pyemma.github.io/Recsys-2024-Paper-Summary/"}</script><title>Recsys 2024 Paper Summary | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Recsys 2024 Paper Summary</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Recsys 2024 Paper Summary</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1735977600" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 4, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="3213 words" > <em>17 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research &amp; study interest and is highly objective. I highly encourage you to review the accepted papers as well and I’m happy to discuss if there is some great paper I missed out.</p><p>There are around 20 papers I have selected and proofread, with a mixing of long/short format paper. For short paper, there is not much details due to the limitation on the number of pages. We would just go over there high level idea and compare it against with some other papers.</p><p>Here is the list of all papers (PS: the papers are from ACM which might have limited accessibility, sorry):</p><ul><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688111">Embedding Optimization for Training Large-scale Deep Learning Recommendation Systems with EMBark</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688136">AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688185">EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations</a><li><a href="https://dl.acm.org/doi/pdf/10.1145/3640457.3688121">CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688098">A Multi-modal Modeling Framework for Cold-start Short-video Recommendation</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688190">Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688106">FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688037">Toward 100TB Recommendation Models with Embedding Offloading</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688030">Short-form Video Needs Long-term Interests: An Industrial Solution for Serving Large User Sequence Models</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688123">Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688129">Scaling Law of Large Sequential Recommendation Models</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688161">LLMs for User Interest Exploration in Large-scale Recommendation Systems</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688033">Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation System</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688060">AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688040">Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688055">Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688107">The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation</a><li><a href="https://dl.acm.org/doi/10.1145/3640457.3688041">Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale Recommenders</a></ul><p>The paper <em>FLIP</em> and <em>Better Generalization with Semantic IDs</em> has been discussed before and thus would be skipped in this post. Please refer to <a href="/Machine-Learning-System-Design-Sparse-Features/">my pervious post</a> for details. I would categorized these papers based on their high level domain and group them in individual sections.</p><h3 id="llm"><span class="me-2">LLM</span><a href="#llm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Large language model is definitely the most popular topic in recent years. In Recsys 2024, there are several papers related to how LLM could be integrated into the recommendation system.</p><h4 id="embsum-leveraging-the-summarization-capabilities-of-large-language-models-for-content-based-recommendations"><span class="me-2">EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations</span><a href="#embsum-leveraging-the-summarization-capabilities-of-large-language-models-for-content-based-recommendations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This work comes from Meta and one novel idea is to leverage LLM to generate a <em>summary of user interest</em> explicitly before generating the user representation. Some key ideas in the work:</p><ul><li>The user history (interaction with items in the past) is chunked by session to get ride of context length limitation (besides chunking, actually perform certain compression to the original prompt probably would also work, such as how HLLM optimize to support super long user history). Each session is encoded via a T5 encoder model, and the <code class="language-plaintext highlighter-rouge">&lt;SOS&gt;</code> token, which is the special token stands for <code class="language-plaintext highlighter-rouge">start of sentence</code> is extracted as a intermediate representation of this user session (classical approach in BERT)<li>Leverage commercial model to generate a summarization of user engagement (they adopted Mixtral-8x22B-Instruct, curious why not Llama 3.1). Then all encoded intermediate representation of user sessions are concatenated together and input into T5 decoder and then have the model to predict the user interest summary (in a NTP fashion)<li>Then the last token from T5 encoder, a.k.a EOS, is concatenated together with the encoded session representation. They go through multiple individual attention block to learn more fine-granularity user representations (similar to multi head attention). And these final representation is going to be used for CTR prediction<li>Candidate is processed similarly as user history, the only difference is how the input prompt is constructed<li>The CTR prediction is computed through a inner product between the final user and item representation, together with a attention mechanism to compute wight to aggregated all matching scores</ul><p><a href="/assets/embsum.png" class="popup img-link shimmer"><img src="/assets/embsum.png" alt="EmbSum" loading="lazy"></a></p><h4 id="calrec-contrastive-alignment-of-generative-llms-for-sequential-recommendation"><span class="me-2">CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation</span><a href="#calrec-contrastive-alignment-of-generative-llms-for-sequential-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This work from Google proposed a new framework to fine-tune LLM for sequential recommendation and they adopted a <em>generative recommendation</em> strategy, which means directly leverage LLM as the main model for recommendation. Some key insights of this work is:</p><ul><li>They defined the main task as <strong>Next Item Generation Objective</strong>, which is try to predict the next item’s text description based on the pervious item in user’s history, which is pretty similar to the NTP task<li>The main task only focus on the token level information, and might miss the big picture on the user/item level. They adopted 2 contrastive learning to overcome this and forge the collaboration signal into LLM: one is to add a contrastive loss between the <em>item embedding from LLM</em> and <em>user history representation from LLM exclude the item</em>, as well as the <em>item embedding conditioned on user history</em><li>One more interesting part is how the model is inferred online. Since the LLM is generating text output, they adopted one additional BM25 matching to retrieve the actual item from the candidate set that is closet to the LLM output<li>One finding from the work is that LLM could quickly memorize the item description it has seen during fine tuning and thus we could even the BM25 matching stage; but this would cause huge issue for item cold start<li>The work does not beat one of their baseline which adopt item id + text (actually this is very common case in current recommendation system)</ul><p><a href="/assets/calrec.png" class="popup img-link shimmer"><img src="/assets/calrec.png" alt="CALRec" loading="lazy"></a></p><p>After reviewing the work, one question I have for this work is how does the model get online trained so that it could learn new item descriptions or new user/item interactions from the production traffic. In the traditional recommendation system, we could enable online training to update the embedding table continuously; while updating LLM parameters would be more challenging even with PEFT technology.</p><h4 id="a-multi-modal-modeling-framework-for-cold-start-short-video-recommendation"><span class="me-2">A Multi-modal Modeling Framework for Cold-start Short-video Recommendation</span><a href="#a-multi-modal-modeling-framework-for-cold-start-short-video-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This work from Kuaishou is to use the multi-modal model to help resolve the item cold start problem within recommendation system. Their approach is similar to the <a href="https://arxiv.org/pdf/2409.11699">FLARE paper</a> we have introduced before from Google in <a href="/Machine-Learning-System-Design-Sparse-Features/">my pervious post</a>, which is also combine the content embedding to help boost the new short-video created. Some key differences are as follow:</p><ul><li>A dedicated modal encoder is designed to encode different type of input, such as text, visual and acoustic. Since the embedding is generated using pre-trained model and these embeddings are freezed, they also introduced a trainable cluster id embedding, which is obtained via K-means algorithm, to be fused with the original modal embedding<li>In the user tower, user embedding interacts with the item embeddings from user behavior sequence via MHA (user embedding as Q and item embeddings as KV); besides, user embedding is also going to interact with the item’s multi-modal embedding from the model encoder, and all these output would go through a <code class="language-plaintext highlighter-rouge">multi-modal interest intensity learning</code> network (this is sightly different strategy how id embedding and content embedding is fused compared with FLARE) to aggregate embedding from different modal with a learnable weight (maybe some user are more text focus, while some enjoy visual more)<li>The item tower is similar with one unique gate component. This gate component is used to control how id embedding and multi-modal embedding is merged together. In the beginning while item is still in cold start stage, we could use more multi-modal embedding; while as the item get more exposure and the id embedding carry better collaboration signal, we could increase its effect by turning up the gate parameter</ul><p><a href="/assets/kuaishou-cold-start.png" class="popup img-link shimmer"><img src="/assets/kuaishou-cold-start.png" alt="Kuaishou Cold Start" loading="lazy"></a></p><p>Using LLM definitely provide a good direction to resolve the item cold start problem in recommendation system, as text, image these semantic information is generic and does not require collaboration signal to learn (like we are recommending items based on some common sense from the world). However, user side cold start problem is still there, how could you get the flavor of new user as soon as possible and target them with the right item is pretty critical for their retention on your platform.</p><h4 id="bridging-search-and-recommendation-in-generative-retrieval-does-one-task-help-the-other"><span class="me-2">Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?</span><a href="#bridging-search-and-recommendation-in-generative-retrieval-does-one-task-help-the-other" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is a research paper from Spotify where the researchers would like to verify the hypothesis that, under generative retrieval scenario (not traditional DL based model), would having dedicated model to train on specific task better or having a single model that trained on the joint task is better.</p><p>In this paper, they only discussed the recommendation and search task, which might due to these 2 scenarios are the most popular one on their platform. Their conclusion is that joint task is better than separated task one. And their explanation is as follow:</p><ul><li>The joint task would make the overall training data more smooth, which perform a type of normalization. This is relative easy to understand as usually more data points could make the overall distribution less skewed (central limit theory)<li>The joint task would make the intermediate representation learned more regulated, as the representation needs to be perform well on both task, which might help avoid some local minimal</ul><p>This conclusion might be case by case and need to verified via the actual data of your problem. In my pervious company, we have identified contradict scenarios that the joint trained model would lift the metrics for one task while hinge the other task.</p><h4 id="llms-for-user-interest-exploration-in-large-scale-recommendation-systems"><span class="me-2">LLMs for User Interest Exploration in Large-scale Recommendation Systems</span><a href="#llms-for-user-interest-exploration-in-large-scale-recommendation-systems" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is a short paper from Google where they explored the potential of LLM to inference user interest. First they build a interest cluster and assign item with cluster ids. Then based on user’s past behavior, inject these cluster id as prompt and let LLM to predict what could be some novel cluster id that user would be interested in. Once get these novel interest cluster id, traditional sequential recommendation is leveraged to generate the recommendation.</p><p>In order to control the output of LLM and let it understand the term of interest cluster as while as the collaboration signal from their domain, they get some high quality data online and adopted fine tuning on the LLM to improve its quality.</p><h3 id="sequential-modeling"><span class="me-2">Sequential Modeling</span><a href="#sequential-modeling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>I have a personal view on the evolution of deep learning model used in recommendation system (highly subjective):</p><ul><li>in the ancient age, it was a three kingdom period, where collaborative filtering, GBDT and logistic regression share the world<li>next comes the deep neural network, which quickly take over the leading position in recommendation realm due to its superior performance<li>the following trend is the integration of sparse features into model, which dramatically increase the capacity of model<li>after that, we have different types of feature interaction techniques blossom to improve the expressivity of model</ul><p>If there is no advance of LLM, it would be sequential modeling’s world.</p><h4 id="scaling-law-of-large-sequential-recommendation-models"><span class="me-2">Scaling Law of Large Sequential Recommendation Models</span><a href="#scaling-law-of-large-sequential-recommendation-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>Scaling Law has been in LLM area for a while, and there are many more different style of <em>scaling law</em> coming out. This paper verified that <em>scaling law</em> also exists in the sequential recommendation models, regarding the data volume, model capacity and training epoch.</p><p>They also verified that larger model also performs better on the downstream task. However, I have some questions regarding the conclusion here. The better performance might comes from the memorization of the model instead of generalizability.</p><h4 id="the-elephant-in-the-room-rethinking-the-usage-of-pre-trained-language-model-in-sequential-recommendation"><span class="me-2">The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation</span><a href="#the-elephant-in-the-room-rethinking-the-usage-of-pre-trained-language-model-in-sequential-recommendation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is another research paper study how LLM should be adopted in sequential recommendation. Based on their study, they purposed a framework to balance the quality and cost of adopting LLM: <em>fine tune LLM with sequential data and use the embedding output as the initialization parameters for the ID embedding in traditional DL models</em>.</p><p>One thing they observed during their study is that, the embedding from deep layers’ head behaves similar to the ID embedding in traditional SR models. Also, they found that they could get similar evaluation result with a LLM only fine tuned a few layers using sequential data, which means there are lots of redundancy in LLM parameters.</p><h3 id="training--inference"><span class="me-2">Training &amp; Inference</span><a href="#training--inference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><h4 id="embedding-optimization-for-training-large-scale-deep-learning-recommendation-systems-with-embark"><span class="me-2">Embedding Optimization for Training Large-scale Deep Learning Recommendation Systems with EMBark</span><a href="#embedding-optimization-for-training-large-scale-deep-learning-recommendation-systems-with-embark" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This work is from NVIDIA about how to optimize the large embedding tables within the traditional deep learning recommendation. I’m still learning this area and just share this paper here for awareness. Their work is incorporated in <a href="https://github.com/NVIDIA-Merlin/HugeCTR">HugeCTR repo</a>.</p><h4 id="toward-100tb-recommendation-models-with-embedding-offloading"><span class="me-2">Toward 100TB Recommendation Models with Embedding Offloading</span><a href="#toward-100tb-recommendation-models-with-embedding-offloading" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is a short paper from Meta which introduce CPU offloading during large model training. With offloading, we could train model with large embedding tables that exceed the available GPU memory. One key optimization here is to overlap the data transfer between CPU and GPU with computation to minimize the latency impact. Thus they adopted pipelining and pre-fetching to avoid GPU waiting for data to compute. They also optimize the sharding algorithm to balance embedding offloading across all ranks (they used pin-backing algorithm to achieve this).</p><h4 id="short-form-video-needs-long-term-interests-an-industrial-solution-for-serving-large-user-sequence-models"><span class="me-2">Short-form Video Needs Long-term Interests: An Industrial Solution for Serving Large User Sequence Models</span><a href="#short-form-video-needs-long-term-interests-an-industrial-solution-for-serving-large-user-sequence-models" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This work from Google introduced how to integrate user sequential model into the main ranking model in a resource efficient way. The key idea is to adopt pre-computing idea: moving the user sequential model inference out of the critical path. They designed a <em>User Behavior Service</em> to pre-compute the embedding and export them to offline storage. During inference, this pre-computed user embedding is used. However, the user sequential model and the main model is co-trained.</p><h4 id="enhancing-performance-and-scalability-of-large-scale-recommendation-systems-with-jagged-flash-attention"><span class="me-2">Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention</span><a href="#enhancing-performance-and-scalability-of-large-scale-recommendation-systems-with-jagged-flash-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>A quick introduction to Jagged Tensor and Jagged Flash Attention. The work is implemented via <a href="https://github.com/triton-lang/triton">Triton</a> which is a very popular library for authoring CUDA kernels.</p><h3 id="llm-agents"><span class="me-2">LLM Agents</span><a href="#llm-agents" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>I have the fortune to work on some LLM code agent as part of my project in the new company and have the chance to holistically review the work in this area. From my current experience, developing coding agent is not as simple as writing some good prompt and make a call to LLM. There are lots of things need to be considered to make the agent generate high quality result consistently.</p><h4 id="ai-assisted-coding-with-cody-lessons-from-context-retrieval-and-evaluation-for-code-recommendations"><span class="me-2">AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations</span><a href="#ai-assisted-coding-with-cody-lessons-from-context-retrieval-and-evaluation-for-code-recommendations" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is the only one paper I have found related to LLM coding agent in Recsys. The overall architecture is a RAG system. They designed a context engine to find the best context to be integrated into prompt. This context engine would retrieve from multiple sources to improve recall and then go through a ranking model to find the best candidate.</p><p>The author also introduced some high level challenge they have faced with when developing their coding agent. One is regarding the data privacy. Lots of context data only exists on users’ devices and could not be logged to server. Another one is the data sparsity regarding high quality labeled data. They have to have experienced engineering to label them manually, which is pretty inefficient (I also encountered similar situation in my project).</p><h3 id="recommendation-system"><span class="me-2">Recommendation System</span><a href="#recommendation-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>There are still lots of study and research fall into the old school style.</p><h4 id="aie-auction-information-enhanced-framework-for-ctr-prediction-in-online-advertising"><span class="me-2">AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising</span><a href="#aie-auction-information-enhanced-framework-for-ctr-prediction-in-online-advertising" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is the first paper I have read that talks about <em>auction bias</em> which exists in the Ads delivery system. In Ads delivery system, the final rank usually is computed as $eCPM = pCTR * bid$, which meas that if a very low quality ads (no one like it, pCTR is low) always places a high bid (I have nothing but money), then our system would rank these ads high, leading to the system would only collect negative feedback (no one is clicking on our recommendation). This is pretty dangerous as our model need positive feedback signal to learn well. If the low quality ads slowly overwhelm the system, then it would be hard to improve our model to provide accurate pCTR estimation.</p><p>The paper mentioned 2 techniques to resolve the issue. One is to use the auction information as additional input during model training to help model debias. This is pretty similar to how we resolve the position bias. Note that since these information is only available after the model inference, they falls into the definition of <em>privileged features</em> (please refer to <a href="https://arxiv.org/pdf/1907.05171">this paper</a> for details). Another one is to introduce a auxiliary task to predict the price of the ads and compute a weight for the positive samples in the main CTR prediction task (similar to IPS).</p><h4 id="co-optimize-content-generation-and-consumption-in-a-large-scale-video-recommendation-system"><span class="me-2">Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation System</span><a href="#co-optimize-content-generation-and-consumption-in-a-large-scale-video-recommendation-system" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This paper from Google discussed about the problem of how to incentive users to generate more content, which means could we recommend some videos to user that could motivate them to become a video uploader. This is one kind of down funnel problem, similar to ads click -&gt; website conversion.</p><p>They primarily adopted the multi-task modeling framework. Some main technique used in their work is as follow:</p><ul><li>to overcome the sparsity of label, they adopted proxy label which is known to be highly positive correlated with content generation<li>conditional loss is used to reduce the noisy, such as the logout user session<li>MMoE is adopted in the main architecture of the model, where the gate is controllable; resnet is also adopted for representation learning</ul><h4 id="bridging-the-gap-unpacking-the-hidden-challenges-in-knowledge-distillation-for-online-ranking-systems"><span class="me-2">Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems</span><a href="#bridging-the-gap-unpacking-the-hidden-challenges-in-knowledge-distillation-for-online-ranking-systems" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This is a pretty good paper to learn about the industrial practice of adopting knowledge distillation in recommendation system. The paper mainly focused on three problems:</p><ol><li>how to handle the distribution shift between teacher and student (the teacher might has learnt some bias during offline training, and this would also be learnt by student)<li>how to efficiently manage the configuration of teacher model (MLOps)<li>how to reduce the maintenance cost of KD infra (MLOps)</ol><p>Regrading the first problem, they adopted a auxiliary task for knowledge distillation instead of having the student model directly trained on teacher’s label. For the second problem, they do it a hard way which is similar to a grid search to find the best configuration in one shot. For the last problem, they use a single teacher model to distill several student model, where the output from the teacher model is written into a columnar database.</p><h4 id="self-auxiliary-distillation-for-sample-efficient-learning-in-google-scale-recommenders"><span class="me-2">Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale Recommenders</span><a href="#self-auxiliary-distillation-for-sample-efficient-learning-in-google-scale-recommenders" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>This paper introduce a new way of doing knowledge distillation, which could help avoid the mis-calibration issue caused by teacher’s soft label. The main idea is to add a auxiliary head which works as a student. The main head is used for searching to to generate teacher soft labels, and this label is considered together with the ground truth label via a label selector (e.g. curriculum learning) and then decided the label that the student head should learn.</p><p><a href="/assets/self-auxiliary-distill.png" class="popup img-link shimmer"><img src="/assets/self-auxiliary-distill.png" alt="Self Auxiliary Distillation" width="600" height="400" loading="lazy"></a></p><p>One thing that has impressed me a lot is that they used this new distillation for the signal loss scenario and achieved pretty good result.</p><blockquote class="prompt-tip"><p>If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee</p></blockquote><p><a href="/assets/qr%20code.png" class="popup img-link shimmer"><img src="/assets/qr%20code.png" alt="Thank You" width="300" height="300" loading="lazy"></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/machine-learning-design/" class="post-tag no-text-decoration" >machine learning design</a> <a href="/tags/sparse-features/" class="post-tag no-text-decoration" >sparse features</a> <a href="/tags/embeddings/" class="post-tag no-text-decoration" >embeddings</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Recsys%202024%20Paper%20Summary%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FRecsys-2024-Paper-Summary%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Recsys%202024%20Paper%20Summary%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FRecsys-2024-Paper-Summary%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FRecsys-2024-Paper-Summary%2F&text=Recsys%202024%20Paper%20Summary%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/KDD-2025-Paper-Summary/">KDD 2025 Paper Summary</a><li class="text-truncate lh-lg"> <a href="/Book-PyTorch-Training-Optimization/">PyTorch 性能与显存优化手册</a><li class="text-truncate lh-lg"> <a href="/Long-User-Sequence-Modeling-In-Recsys/">Recommendation System - Long User Sequence Modeling</a><li class="text-truncate lh-lg"> <a href="/Machine-Learning-System-Design-Sparse-Features/">Trend of Sparse Features in Recommendation System</a><li class="text-truncate lh-lg"> <a href="/Recsys-Paper-Review-2025-Q1/">Recsys Paper Summary 2025 Q1</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/Machine-Learning-System-Design-Sparse-Features/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1730617200" data-df="ll" > Nov 3, 2024 </time><h4 class="pt-0 my-2">Trend of Sparse Features in Recommendation System</h4><div class="text-muted"><p>In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as revi...</p></div></div></a></article><article class="col"> <a href="/Long-User-Sequence-Modeling-In-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1751094000" data-df="ll" > Jun 28, 2025 </time><h4 class="pt-0 my-2">Recommendation System - Long User Sequence Modeling</h4><div class="text-muted"><p>User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a ...</p></div></div></a></article><article class="col"> <a href="/Recsys-Paper-Review-2025-Q1/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1745046000" data-df="ll" > Apr 19, 2025 </time><h4 class="pt-0 my-2">Recsys Paper Summary 2025 Q1</h4><div class="text-muted"><p>In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here i...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/Machine-Learning-System-Design-Sparse-Features/" class="btn btn-outline-primary" aria-label="Older" ><p>Trend of Sparse Features in Recommendation System</p></a> <a href="/How-to-design-slack/" class="btn btn-outline-primary" aria-label="Newer" ><p>How to Design Slack</p></a></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/Recsys-2024-Paper-Summary/'; this.page.identifier = '/Recsys-2024-Paper-Summary/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/message-queue/">message queue</a> <a class="post-tag btn btn-outline-primary" href="/tags/realtime-system/">realtime system</a> <a class="post-tag btn btn-outline-primary" href="/tags/sparse-features/">sparse features</a> <a class="post-tag btn btn-outline-primary" href="/tags/stateful-service/">stateful service</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
