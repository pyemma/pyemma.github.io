[ { "title": "Recommendation System - Long User Sequence Modeling", "url": "/Long-User-Sequence-Modeling-In-Recsys/", "categories": "Machine Learning", "tags": "machine learning design, recommendation-system, user-sequence-modeling", "date": "2025-06-28 00:00:00 -0700", "snippet": "User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a ...", "content": "User sequence modeling has been a hot topic recently in recommendation system thanks to the advancement of transformer architecture and more powerful hardware. In this blog, I would like to have a simple review on the evolution of user sequence modeling work, especially long user sequence modeling. Hope this blog could inspire broader exploration ideas for the future.Here is a quick outlets on the papers that we are going to discuss about today TWIN: TWo-stage Interest Network for Lifelong User Behavior Modeling in CTR Prediction at Kuaishou TWIN V2: Scaling Ultra-Long User Behavior Sequence Modeling for Enhanced CTR Prediction at Kuaishou TransAct: Transformer-based Realtime User Action Model for Recommendation at Pinterest TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders DV365: Extremely Long User History Modeling at InstagramNow, let’s begin our journey with the ancient world.Ancient WorldUser sequence essentially is a chronologic sequence of collaborative signals collected from certain domain, e.g. the feed posts that a user has viewed, clicked and commented or the ads that a user has clicked. In the past, these signals are usually processed in a reduction format, which means that we would drop the chronologic information and use a simple pooling strategy to fuse the information together, similar to the example below.There are some issues within this approach: The lost of chronologic information leads to the loss of granularity on user’s short term and long term interest. There is some solution such as introducing recency wight to put more emphasis on user’s recent behavior, but still might lead to suboptimal representation of user The simple pooling strategy leads us not able to learn too much useful signals from user’s sequence behavior. For example, after seeing a video of NBA, user might visit sports shop for basketball. Such relationship is implicitly encoded within user’s behavior but could not be learnt once they are aggregated together.There are some work to overcome the reduction style of user sequence handling in the past. For example, in DIN the attention mechanism (more formally, it is target attention) is introduced to selective find the items most relevant to the current ranking candidates from user’s behavior history. This helps the model to use the most effective portion of information from user’s past behavior, instead of letting those signals being averaged out.Modern WorldTransformer architecture has revolutionized the NLP world. Due to its superiority in terms of modeling relationship among tokens and inference efficiency compared to RNN, researchers also start to apply this technique into user sequence modeling. One of the most representative work is Pinterest’s TransAct model.Before introducing TransAct, there are some common properties I have summarized from the papers I have read. I plan to use this as an architecture to explain the works that is going to be introduced in this post: Token representation: which refers to how the sequence is represented. For example, is each token a composition of several different embeddings, or there are different types of token (similar to multi modality) in the sequence Sequence length: the time span of the user past behavior to use. For example, use past 2 years history, or just recent history Attention type: multi head attention or cross attention Dimension reduction: mainly adopted for long sequence scenarios to reduce the computation time complexity. For example, use search in the sequence or directly compress the sequence to shorter lengthThe idea in TransAct is relative straight-forward: applying transformer encoder to user’s recent (in their paper, they called it as real-time) sequence and forwarding the output from transformer block to downstream of the model (TransAct is being a module within the entire model, other component adopted is DCNv2). Token representation: The token contains the positive actions from user’s past behaviors, e.g. pins clicked or shared. Each token is a composition of 3 parts, the action embedding, the interacted item embedding and the candidate embedding. The action embedding is learned during the training and the interacted item embedding and candidate embedding is from PinFormer, which could be viewed as static. These 3 embeddings are concatenated to form the final token embeddings (they compared using sum of embeddings and production the concat version). Sequence length: only the latest 100 user actions is used due to the training &amp; serving cost at the time of the work; padding is used if there is no 100 actions from the user (e.g. cold-start users). Since the sequence is pretty short and to avoid model over-fitting on the last user actions, they introduced a random mask based on the request timestamp (masked attention). Attention type: self attention is adopted as only the encoder of transformer is used; note that the cross part of the candidate item with items user historically interacted with is implicitly handled because of the token representation. Dimension reduction: no dimension reduction is adopted due to the relative short length used.The last K output of the transformer blocker + max polling of all token’s hidden representation is used as the input to the remaining part of the model; it is flattened and concatenated with other vectors such as embedding lookup output of sparse features.Towards Longer SequenceDue to $O(N^2)$ time complexity of the transformer blocker, it is pretty challenging to really scale to longer sequence. To make it work in practice, different solutions from infra or from modeling has been proposed.TWIN is one of the pioneer to scale user sequence from $10^2$ to $10^4$ so that we could model user’s lifelong behaviors. Token representation: The feature associated with each item is converted to categorical and go through the embedding lookup to convert to the dense vector format. Sequence length: There are 2 stages in TWIN framework. The first stage is a general search stage where the input sequence length is $10^4$, and the second stage is exact search stage where the input sequence is $10^2$, which is the top items selected from the general search stage. This is similar to the retrieval-ranking mutli-stage arch in traditional recommendation system. Attention type: Multi-head target attention is adopted. This is still similar to DIN’s cross attention, but use different projection to transform the QKV into multiple heads so that each head could learn different aspect of the hidden representation. Dimension reduction: The technique adopted is still search style, which is top-k attention scores. In the general search stage, the candidate item is used as query to perform multi-head cross attention with $10^4$ user history interactions. And the top $10^2$ scores are selected and sent to exact search stage.To make the computation in the general search more efficient, a feature decomposition is adopted to enable precompute &amp; cache For each token representation, it is decomposed as a item specific section and a item-user cross section; each section is associated with a projection matrix $W$; the item specific part is still used as normal attention computation, and the item-user cross part is modeled as a bias term to be added to the attention scores After offline training, the project matrix of the item specific could be used to precompute and cached into the offline inference service. This cache is updated with the latest embeddings of items and latest project matrix synchronized from the training system to minimize the staleness of the result. Once there is a request comes in, the offline inference server could return the precomputed result for user’s history sequence and the remaining computation is done on the fly. This removes the major computation bottle neck in TWIN which is the projection of $10^4$ user sequence.LONGER is another long user sequence work from Bytedance and it used a different approach to reduce the sequence length. Also the overall architecture is similar to HSTU. Token representation: Different type of features going through the same shared embedding layer, and then with the addition of position embedding. To reduce the token length, a token merge strategy is used here to merge adjacency tokens into a single one. InnerTrans block is used for this merging so that local information is still preserved after token merge. Sequence length: Over $10^3$ length of user history items. The construction of the sequence not only include the user history interactions, but also include the candidate item features and user profile features, which is used for construction as global tokens to interact with all all user history behaviors. Attention type: In the first layer, causal cross attention is used and regular causal self attention is used for the remaining layers. In causal cross attention, the global token is used as the query, along with several items retrieved from user’s behavior sequence (they find using the most recent k items yield best performance). Dimension reduction: As mentioned, the primary reduction strategy is through compression. Token merge is one layer of compression, cross attention on selective query tokens are another layer of compression to reduce the sequence length.Recently Pinterest also upgrade their TransAct to TransAct V2 to scale user sequence from $10^2$ to $10^4$. Token representation: Static embedding is still leveraged as input (from PinSage) and candidate’s embedding is still append to each user interacted items’ embedding. However, the action embedding is not concatenated but added. Besides action embedding, surface embedding and timestamp embedding (as position) is also introduced, and also added with the item embedding. Sequence length: 3 sequences are introduced. Lifelong sequence length is 90th percentile of user’s past 2 years history, which is at $10^4$ scale. Realtime sequence which contains user’s latest interaction sequence scales at $10^2$ level. Impression sequence which contains user’s negative interaction (no action from users) scales at $10^2$. Attention type: Similar to TransAct. Dimension reduction: Nearest neighbor search against the candidate item is used to reduce the sequence length for all 3 sequences. After NN the sub-sequences are concatenated together to go through the transformer encoder.Another modeling improvement in TransAct v2 is to adopt contrastive learning to enhance the representation learning. For the hidden representation for timestamp $t$ (not that due to the causal attention ), the $t+1$ item from the realtime sequence is used as the positive samples and the 2 representation are pushed closer; while random negatives are sampled from the impression sequence to be pushed away.Sequence Length ++Although $10^4$ is already a pretty long sequence, researchers do not stop their effort to scale to even longer sequence.TWIN v2 is one upgrade of the TWIN algorithm and scale the sequence from $10^4$ to $10^5$. The majority of the components do not change except for the part of dimension reduction. Token representation: Cluster embedding which is computed through the hierarchy K-means algorithm. Dimension reduction: Besides the general search and exact search unit similar in TWIN, one addition clustering based sequence reduction process is adopted to mitigate the scaling challenge. The clustering algorithm is done in 2 stage: In the first stage, item in users’s sequence is grouped via a heuristic approach. In the work, they group the items based on the percentage of the consumption of the video by the user. Within each group, a K-means algorithm is used to cluster the item into several clusters. The newly formed cluster would go through another round of K-means if the number of item within the cluster is higher than certain threshold; once the number of item drops below the threshold, this cluster is finalized and would be moved out from the process and append to global result. The item embedding used for the K-means is from the recommendation model, which means it is using collaborative signals. Eventually, user’s original sequence would be converted to a sequence of clusters. And the mean pooling of all items in the cluster is used as the representation of the cluster (which is going to be the new token representation). The last work is from Instagram, which is called DV365. The sequence length is also scaled to $10^5$, where the longest one is at 70000 and average is 40000. This work is used as a foundation model to generate high performant user profiling embeddings which is used as input to other downstream models. This is a relative disaggregated view compared to the work we have mentioned above. Token representation: Carefully manual crafted &amp; selected features are used as the token representation. The features are bucketized (categorized) and then converted to dense representation via the embedding lookup. Sequence length: $10^5$ scale of length. User sequence are constructed in 2 different format, one is explicit sequence which contains users’ action such as click; and the other one is implicit sequence which contains users’ implicit reaction such as video duration &amp; dwell time. Attention type: Funnel transformer is adopted, which pools in token dimension to reduce the sequence length in later stage of model. Linear compression is also adopted to compress the original sequence input and combined with the final output from funnel transformer. Dimension reduction: No other reduction technique is used such as item selection or clustering.Is this end of eraThe landscape of user sequence modeling has been fundamentally changed by transformer architecture and more powerful hardwares. Besides this traditional view of the user sequence modeling where it is treated as a feature processor or feature generator, there is also another disruptive stream in the recommendation area, which is Generative Recommendation. In GR, the input sequence is already changed from impression level to member level, and consume the member sequence directly as the input to predict the next item that user is likely to interact with. This is an interesting and active area, stay tune for my upcoming post!Is GR going to be the killer for the traditional user sequence modeling work? Yes and no, and actually these 2 domain synergy pretty well with each other: Both needs to handle the scale of the user sequence. Right now in GR the raw sequence is still the primary choice, but we could see that lots of dimension reduction techniques used in user sequence modeling could be applied to GR as well. Item representation is shared. How to synergy collaborative embedding, semantic embedding, and even multi-modal in the sequence representation would still be the key. Inference challenging is the same. Lots of infra optimization work needs to be done to make it for online. Also how to enable incremental training and online training is also a challenging task. If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeAcknowledgmentThis blog is inspired from a group discussion with several Daolao: Yunzhong, Daqi, Zeyu, Lili, Michael, Qiaqia. Appreciate their generous sharing idea and insights." }, { "title": "Recsys Paper Summary 2025 Q1", "url": "/Recsys-Paper-Review-2025-Q1/", "categories": "Machine Learning", "tags": "machine learning design, recommendation-system", "date": "2025-04-19 00:00:00 -0700", "snippet": "In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here i...", "content": "In this post, I would like to provide a simple summary on the papers I have read in the first quarter of 2025 and discuss some of my thoughts on recent trend regarding recommendation system. Here is the full list of papers in this summary, which are all available on Arxiv: Full-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation NoteLLM-2: Multimodal Large Representation Models for Recommendation Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation Bridging Textual-Collaborative Gap through Semantic Codes for Sequential Recommendation Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation Diffusion Model for Interest Refinement in Multi-Interest Recommendation External Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads Recommendation LLM-Alignment Live-Streaming RecommendationRecently, there is a trend that tries to integrate diffusion process into user sequential behavior modeling which is a new idea to me. Besides that, I’m also seeing an increased trends on utilizing semantic ids (if you are not familiar with this concept, please refer this section from my pervious post) to improve the training or serving efficiency. How to better utilize both the semantic embeddings (from LLM or VLM) and id embeddings (from collaborative signals) continuous to be a hot area. And distillation has been drawing eyeballs not only in small LLM, but also in industry to help reduce the inference pressure incurred from larger models.DiffusionProbably everyone today is already familiar with stable diffusion. It is a popular technique used for image generation in recent years and powers website such as Midjourney. Due to there strong abilities to model data distribution and generate high-quality items, diffusion models have also been adopted for sequential recommendation.Distinguished Quantized Guidance for Diffusion-based Sequence RecommendationIn this work from Kuaishou, a famous Chinese company in short video/live recommendation track, the author proposed some augmentation to how diffusion model is being applied to sequential modeling. Traditionally, noise is added to the next item and user’s interaction sequence is used to progressively denoise it. But user’s interaction sequence usually contains some noisy items due to stochastic user behaviors, or the sequence could be pretty sparse (a.k.a short) to provide meaning information to the denoise process, which would hind this denoise process. One solution proposed in this work is to use vector-quantization to encode and augment the original user sequence.Given a user behavior sequence $s = [x_{1}, x_{2}, \\dots, x_{L-1}] \\in R^{(L-1) \\times D}$, which we have already converted the item id to the item embedding; we would match against to a semantic codebook which is defined as $C = \\{ c_m \\}$, and $c_m \\in R^{(L-1) \\times D}$. To find $m^*$ that best matches the to the original input, sampling from predicted vector distribution approach is used. In this approach, $s$ is feed into a MLP to generate logits of size $M$ (corresponding to the dimension of cookbook) and then Gumbel-Softmax technique (pretty similar to softmax with temperature, to resolve the in-differential problem of argmin) is used to find $m^*$. Then the quantized sequence is defined as $s_{q} = c_{m}$ and combined with origin input sequence $s$ with a controllable rate. The codebook is also trained via expectation-maximization approach, which is a commonly used optimization algorithm for such clustering-alike process. During the training process, we would update $c_i$ using the average of all $s$ that is assigned with $i$-th codebook. The picture below highlights the overall process of this vector-quantization. Through this process, we reduce the noise in the original sequence by dragging it towards a more common representation across the entire user space (similar to a cluster centric, all user that assigned to the same codebook belongs to certain pattern); as well as using this common representation to augment for the sequence that is sparse.Another technique proposed in this work is to add one additional contrastive loss in the original reconstruction loss so that we could enforce the diffusion process to be less bias to popular items in the data and yield more personalized interests for each user.The original paper contains lots of mathematic, which might be a little bit intimidate to read if you are not familiar with the original stable diffusion work. I would recommend this blog to learn the basics.Diffusion Model for Interest Refinement in Multi-Interest RecommendationThis work is from Xiaohongshu, which is a famous Chinese company similar to Pinterest. In this work, the diffusion process to improve the multi-interest embeddings extracted from user sequence, which is called Diffusion Multi-Interest Model, to make each individual interest embedding is more clear and contains less noise polluted from users’ multi-interest convoluted interest.The work starts with apply self-attention on the user history sequence $H \\in R^{T \\times d}$ by using learnable parameters to compute the attention scores $A \\in R^{K \\times T} $ as\\[A = softmax(W_2 tanh(W_1H^T))\\]where $K$ stands for the number of interests. Then the interest vector could be obtained as $V = AH$. Once interest vector is obtained, the next step is to leverage the diffusion step to denoise it. The logic here is a little bit complicated and has lots of mathematics, but the overall flow could be described as follow: For a given item, find the interest vector that is most close to it (out of total of $K$) as $v_0$ Compute $v_t$ based on $v_0$ and sampled step $t$, this is the forward step Reconstruct $\\hat{v_0}$ via the denoising module. This module uses the cross-attention with the original user history sequence embedding and an item-pruning strategy The reconstructed $\\hat{v_0}$ and the original interest vector is combined together as the final user representation This final user representation is used for loss computation against the item during training, or used as query vector during inferenceI still have some questions regarding how a transformer architecture is used in the reverse process. I will share more details later once I found more resources.Multi ModalNoteLLM-2: Multimodal Large Representation Models for RecommendationFor multi-modal scenarios, such as the post in Xiaohongshu platform which contains both image and text, a traditional approach to model the information from the post is to have encoder to encode the image and text data separately and then use mechanism such as cross attention or weighted fusion to learn. However, this might not be the best option due to the isolation of encoding process and sometimes the vision information might not learnt well in the post hidden state.This paper from Xiaohongshu proposed a new prompt based approach to better learn the vision information and established a new type of multi-modal large language model. First, a special format of prompt is used, which is as followNote content: {'image': &lt;IMG&gt;}, Compress this note into one word: \"&lt;IMG_EMB&gt;\".Note content: {'title': t_i, 'topic': tp_i, 'content': ct_i}, Compress this note into one word:where the &lt;IMG&gt; token is a special token, which is going to be replaced with the output of a vision encoder after the tokenization step, and &lt;IMG_EMB&gt; is another special token, which is used to extracted the LLM processed vision embeddings. The embedding of the last token of the prompt is also extracted which is used at the note multi-modal embedding. This step lets LLM leverage its internal knowledge to aggregate the vision and text information. Secondly, to make sure the vision information is preserved, a late fusion approach is adopted to combine the original vision embedding and LLM processed vision embedding, where a gate mechanism is used to fuse them in a learnt way. Finally, contrastive learning is used on both the fused vision embedding and note multi-modal embedding to learn from the data.The model is deployed offline to process the embedding for the new posts published and store the embeddings into an embedding table. This embedding table is used to extract embedding queries based on user history sequence as well as for building ANN index.LLM Embedding &amp; ID EmbeddingFull-Stack Optimized Large Language Models for Lifelong Sequential Behavior Comprehension in RecommendationOne novel idea in this paper is how they combined the LLM embedding and id embedding from traditional recsys model: Train a traditional recsys model to learn the collaborative signal from the dataset and get the id embedding, for item $i$ we have $c_i$ Use LLM to encode the text associated with the item into LLM embedding, which is $E_i = [v_1, v_2, v_3, \\dots, v_L]$ Train a projector (e.g. MLP) to project the id embedding into LLM embedding space and append it to the last via soft-prompt $\\hat{E_i} = [v_1, v_2, v_3, \\dots, v_L, MLP(c_i)]$, the soft-prompt here is just use a special token in the prompt and replace it with the id embeddingIn this approach, we could merge the collaborative signal and the semantic signal together and let’s LLM to learn from the augmented input.Semantic Retrieval Augmented Contrastive Learning for Sequential RecommendationIn this paper from Tecent, the researchers proposed a new way to find positive samples for contrastive learning, especially for the case of recommendation system scenarios where data sparsity is common.Traditionally, clustering approach is used for cross-sequence CL and masking is used for intra-sequence CL. However, both methods would be affected by the data sparsity issue. In this paper, semantic embedding obtained from LLM plays a critical role to ensemble positive samples, which depends on the natural text information and is not affected by the sparsity issue.The idea is relative straight forward. First, user’s sequence is hard prompted into text and then use merchant LLM to summarize; the summarized text is feed into LLM and convert to a semantic embedding. Then the similar users could be identified via this semantic embedding and for each user, we could generate a candidate set. However, this set is generated purely via the semantic information and the actual collaborative signal is missing. Thus additional processing is required so that we could weight each candidate in the set correctly based on the training data we have. The author used a simple attention mechanism and softmax to compute the weight. And the final user positive sample embedding is represented as\\[h_{u}^{+} = \\sum_{u^\\prime \\in N_{u}} p_{u, u^\\prime}h_{u^\\prime}\\]Similar approach is used on item side as well, but the positive samples are directly sampled from the candidate set instead of using a learnable approach to merge them together.Bridging Textual-Collaborative Gap through Semantic Codes for Sequential RecommendationThis paper from Renmin University proposed a new approach to fuse the text embedding and id embedding, to achieve a better trade off between the semantic information and collaborative information. In this method, the semantic code, which is generated via product quantization or residual quantization is used to merge with the text embedding: For each item, convert each attribute’s raw text into text embedding via certain encoder model, $Z^t = [z_1^t, z_2^t, \\dots, z_m^t]$ Based on the attribution embedding, use PQ/RQ to quantize and obtain the semantic embedding $Z^c = [z_1^c, z_2^c, \\dots, z_n^c]$ For semantic embedding, first apply multi-head attention, and then use attribution embedding as KV to perform cross-attention, which eventually generate $H \\in R^{n \\times d}$, use a pooling module to convert the hidden state into a single embedding vector, also combine the pooling of semantic embedding to enforce the learning of collaborative signal in those embeddingsI think this is the most interesting part: for the semantic coding part, similar item would be enforced to share similar codex and thus the collaborative signal would be shared among these items, achieving shareability; the self-attention and cross-attention provide a more sufficient compute capability for model to learn across different attribute of the item and enforcing the model to learn semantic embedding well instead of solely relay on text embeddings.For the learning part, the work adopted idea from masked language model, that some semantic code and item in the sequence would be masked and ask the model learn to reconstruct.DistillationExternal Large Foundation Model: How to Efficiently Serve Trillions of Parameters for Online Ads RecommendationDistillation is a technique used to transfer knowledge from large model into smaller model to achieve a trade off between accuracy and efficiency. For example, in LLM area it is a hot topic to transfer the knowledge in lager language model into smaller language model which is targeted to run on device. In short, the model with larger capacity could learn and memory more patterns from the data which model with small capacity is hard to learn well; thus the output from the larger model (which we also usually called teacher model) could be used as a type of soft label which is easier for the smaller model (which we also called student model) to learn.In recommendation system, there is also such trend. This paper from Meta pushed this direction to the extreme: they trained a foundation model (FM) billions of parameters, compute intensive architectures and 15x ~ 100x volume of training data and covers all verticals/domains compared to the vertical model (VM) that is serving online. In the paper, they shared the system architecture of their online distillation system, which is called Data Augmentation Service (DAS): The training data is generated in streaming approach, which is for online training Once the streaming training data is generated, one additional model inference call is sent to offline FM inference to obtain the soft label; once label is get, it is joined with the streaming data again as teacher supervision (my guess here is that they are reusing the same online joining framework) The data is stored in shared dataset, which could be filtered by VM based on their traffic/sector To make sure the FM could obtain the latest production traffic, it needs to be regular refreshed with the shared dataset; new snapshot would be published regularly and DAS is responsible for identifying and loading the latest snapshot for offline inferenceRecsys ModelingLLM-Alignment Live-Streaming RecommendationThis paper from Kuaishou introduces some challenges in live streaming recommendation, which is some problem that I haven’t encountered. For example, the live streaming is realtime, which means that pre-generating embeddings is not an option and the embeddings has to be generated on the fly when it is alive; also audience might join live at different timestamp and see different screens of the live, which makes it harder to modeling users’ behavior. The author introduced some technique they have been successfully deployed online, which worth to learn about, especially if you are also working in live recommendation: A 7B VLM model is used to generate embedding for live streaming every 30s; the 7B model is fine tuned use the data that is annotated by a powerful 100B in offline A gating mechanism (similar to the NoteLLM-2 in the above section) is used to fuse the semantic embedding generated from VLM and the author id embedding (for collaborative signal) To make the online serving efficient, the fused embedding is further quantized and only the codex id to save the online serving storage. The codebook here is generated via a hierarchy K-means approach where a group of author embedding is first clustered into 512 categories, and then the residual part is clustered into 256 categories, and so on. A total of 3 layers of codebook is used. The codex id is used as feature into the deep cross network, which is also used for attention computation.I’m pretty enjoy reading this paper as there not much fancy and complex mathematic involved. Everything is relative straight forward to explain and understand. Highly recommend for a read (and I will also try these tricks in my side project :smiley_cat:) If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" }, { "title": "How to Design Slack", "url": "/How-to-design-slack/", "categories": "Distributed System", "tags": "distributed system, stateful service, websocket", "date": "2025-02-01 00:00:00 -0800", "snippet": "In this post, I would like to discuss how to build slack, a very popular realtime messaging application, especially for group messaging (a.k.a channel) in cooperation messaging scenario.Functional ...", "content": "In this post, I would like to discuss how to build slack, a very popular realtime messaging application, especially for group messaging (a.k.a channel) in cooperation messaging scenario.Functional Requirements Channel messaging (group messaging) and thread message (reply to a message in a channel or DM) Direct messaging (1 to 1 messaging) Support sending message to both online and offline usersNon Functional Requirements High availability High scalability Low latency for realtime message delivery Causal consistency for best user experience At least once delivery semantic to avoid missing messagesAssumption In this design, we would leave out the discussion how user would login and how user would join a channel. We assume that there is dedicated service to handle the user login verification, and user could join a channel via different approach such as invitation or search. We would also simplify our discussion to persist user’s channel membership information. In reality, these information could be stored in a dedicated channel_membership table. Based on the read/write ratio, relational database such as MySQL or column database such as Cassandra could be adopted. We would assume that the message would be persistent on the server side as well, instead of only storing them locally on client side. This is a legit use case for the cooperation scenario, but might not be an option for other scenarios. For example, WeChat and Whatsapp does not store the message on the backend side but just temporarily buffer them. Once the receiver is online and the message is delivered, the message on server side would be deleted.Key Challenges How does clients communicate with backend services When one user send a message in a channel, how this message gets fan-out to other users in the same channel How to guarantee the casual consistency within the channel messagingHigh Level DesignHere is a high level design of Slack where we adopted a hierarchy broadcasting design. There are some alternative options with different trade off and we would discuss them shortly in the sections below.Client ConnectionClient (e.g. desktop client or mobile app) connects with our backend service via websocket connection, which is handled by the Websocket Server. There are different approaches for long live connection as well (short live connection is not very appropriate in this scenario due to large overhead on repeating building connections), e.g. HTTP long polling, SSE. However, realtime messaging is usually bidirectional communication and websocket would be good option in this scenario. Within each Websocket Server, there would be an event loop that is listening on the port it exposed, as well as an internal data structure to manage the channel and websocket connection relationship (a.k.a a subscription table, see example below). When Websocket Server receive message for a channel, the event loop would check this data structure to find the websocket connections that are subscribing this channel and the message would be handled to the websocket connection to delivery to the client.{ # channel_id: list[websocket_objects] 'gongzuoqun_1': [websocket_obj1, websocket_obj2], 'gongzuoqun_2': [websocket_obj1, websocket_obj2, websocket_obj3],}After the user has login, the user would build connection with one Websocket Server and the selection of the server is handled by the load balancer. There could be some different strategy, such as round-robin routing, workload based routing, or even sticky routing if it is just a short-time disconnection. Once the connection is built, there are 2 important tasks that need to be done: Retrieve the channel information of the user from the channel_table, update internal subscription table, and send subscribe request to the Channel Server (to be expanded later) Retrieve the message sent by others during the user is offline from the message_tableWhen user is offline, the Websocket Server would also update its internal subscription table. If for some channels there are no clients subscribing, the server could send a unsubscribe request to the Channel Server so that we could manage the workload and avoid unnecessary resource waste.Message DeliveryLet’s first discuss the option to deliver channel message. There are 3 aspects that we need to discuss: How does user send message to the channel How does other users receive the message that are sent to the channel How does user receive message when the user has been offline for a whileHow does user send messageTo send a message to a channel, there are two approaches: 1. use the websocket connection we have already established; 2. send it as a HTTP Post request to another Web Server (a.k.a functional sharding). Both of the choice are workable solution (Slack used the 2nd approach according to their blog). Here is a quick comparison between the 2 options: The benefit of reusing websocket connection is minimized latency and reduced complexity on the infrastructure; however, the downside is that the websocket server needs to keep live connections with clients and would not be easily to horizontally scale based on the QPS of the traffic; also websocket requires customized retry logic when message sending is failed The benefits of using HTTP Post request to send message is that it offers better horizontal scalability and native retry support; however, the downside is that the latency would be slightly higher and there is some more complexity on the infraAnother practical consideration is the processing of the message. To make sure that we don’t lose any message, when our backend receive the message, we would first write it into the Message DB and then fan-out (or broadcast) the message to other users in the same channel. There would be additional business related logic need to run over the message such as validation. All of these process would put additional pressure on the host machine. Thus, it might be a legit option to have dedicated Web Servers to handle message write instead of reusing the Websocket Server to avoid putting too much load on them.How does other users receive messageLet’s first take a look at the channel messaging scenario. There are several options: Dispatcher: In this option, we would have dedicated Channel Servers which play as a dispatcher role (I talked about this design in my pervious post on how to design auction service). Each channel service would handle a portion of channels (we will discuss this in scalability section) and maintains a subscription table similar to one in Websocket Server which records the channel_id -&gt; websocket server id information. This subscription table is updated by the request sent from Websocket Servers. Once the web server receive messages, it would send this message to the channel server the manage the channel and then the message would be broadcast the websocket servers that is subscribing to the channel. You might ask why we need such a Channel Server, why not just use the Websocket Server? One reason is that the subscription table, which is a stateful information, is not easy to be directly maintained on Websocket Server; another one is to follow the Single Responsibility Principle; last but not the least, is that we could scale Channel Server and Websocket Server individually based on the number of channels and number of active users Pub/Sub: In this option, we would use message queue for pub/sub style message exchanging. For each channel, a dedicated topic would be created within the message queue. For the Websocket Server who is connected with the user in the channel, they would subscribe to the corresponding topic. Once message is published onto the topic, message queue would deliver the message to the Websocket Server who is subscribing. Note that we could choose a message queue that support poll or push mode: for poll mode, Websocket Server needs to periodically poll data from the queue; for push mode, message queue would be responsible for delivering message. Given the load and the realtime scenarios, adopting a in memory push mode message queue might be a good choice. Fan-out In this option, we maintain a queue similar to a inbox for each user, and the channel message would be fan-out written to this inbox. The exact choice of the inbox could be message queue (each user’s inbox is a topic) or database (each user’s inbox is a table). Websocket Server would periodically poll data from the inbox for new message. One benefits of this solution is a unified flow for both online and offline message delivery scenario. However, this solution would probably incur higher latency, as well as the causal inconsistency issue (because we have multiple copy of message stored for the same channel). We also need to consider how could be provide the at least once message delivery guarantee as we mentioned in the non functional requirements. When client send a message to a channel, if the HTTP post request failed, client side could retry it. With retry, there could be duplicated requests sent from client. We could adopt an idempotency key and cache layer so that the web server could dedup the already succeed post requests. In our design, each message needs to be written to Message DB first for persistent. Thus we could also use upsert when writing to DB to avoid writing duplicated message with the same idempotency key. This strategy is also applicable to the Fan-out option. In the Dispatcher or Pub/Sub option, when Web Server forward messages to Channel Server and Channel Server broadcast messages to Websocket Server, we could also adopt retry until we receive ACK from the destination server. We could add a dedup on the client side when receive message from the websocket and allow Web Server or Channel Server to forward duplicated requests.In the remaining design, we would assume to use the Dispatcher option, which is also the option that Slack has deployed in production.How to receive offline messageIf a user is offline, then he would miss the message that is broadcast by the Channel Server. To receive these messages, we need to leverage the Message DB. When a user is offline, we could store the timestamp as last_active_time in a dedicated table (e.g. user_activity_table or in user_profile_table); and when user is back online, we could retrieve the last_active_time and do a query in Message DB against all channel that user is in, pulling out all messages. Another option is to keep a client side snapshot of the local message status, and send it to server when user come back again; server would use the snapshot to compute the delta message the user is missing and send it back. Both of the solution has some drawback: the last_active_time solution requires additional DB write and also need the time within the fleet of machine is synchronized (e.g. NTP) or read more data backward; using snapshot would have some additional challenge when dealing with multi-device scenario.One more thing to mention is that we need to appropriately synchronize offline message receiving and online message receiving when user is just back online. We should confirm that all the offline message is read and shown to user before showing the message from websocket in order to guarantee the causal consistency and good user experience.How to support direct messagingDirect messaging and channel messaging does not have too much difference, except that channel messaging is broadcasting to multiple users while direct messaging is “broadcasting” to only one users. However, it might be less efficient to let the Channel Server to forward the message. We could directly forward the message to the Websocket Server that the receipt user is connected. Which Websocket Server the user is connected to could be store in a in memory KV store for fast read/write. Another consideration is to treat it as service discovery and use some dedicated solution such as Zookeeper or Consul; but since user would online/offline pretty often, this could put a lot of write pressure for the distributed consensus.How to support thread messageThread message does not have a big difference compared with regular message. The only difference is that they are embedded within another message in channel. To support it, we could create a field in the message_table to indicate which message is its parent. And when we broadcast the message, we also include this information in the request payload.message_tableid: intchannel_id: intauthor_id: intparent_message_id: Optional[int]content: texttimestamp: int# message payload{ \"content\": \"abc\", \"parent_message_id\": 123, \"id\": 1234}High AvailabilityLet’s discuss about the high availability. We would skip the discussion on Message DB as the general rule of how to make DB fault tolerance is applicable (you could checkout my Youtube video here).For the Websocket Server, they are stateless and don’t need to persistent any data on the server. If one Websocket Server is dead, then all connections on that server would be lost. The clients need to reconnect with other Websocket Server. The new Websocket Server would also need to refresh the information stored in the KV store (could use last write win strategy) to make sure direct messaging could still work.For the Channel Server, they are stateful as they need to maintain the subscription table so that channel message could be correctly broadcasted. First, this subscription table could be maintained locally on the host disk using WAL. If the process that responsible for forwarding the message crashes accidentally but the host is still alive, we could rebuild the subscription table quickly by reading the WAL. In the situation that the host is gone, we need to replicate the subscription table to somewhere else to make sure the message could still be broadcast. One option would be storing this information to KV store; another option is to directly replicate the subscription table to other Channel Server. In this option, we could have a single leader to handle all read/write request, and have multiple followers to store the replicated data. We also need a coordination service to help maintain the leader/follower information as well as the leader election.High ScalabilityThe Message DB could be horizontally scaled. For sharding, there is key-range sharding and hash based sharding. Key-range sharding is not very suitable in this case. We could apply hash based sharing on the channel id, which should be able to relatively evenly distribute the message across all hosts. To support efficient query data for offline case, we could build index on timestamp column (since each channel’s message is co-locate on the same host, local secondary index is sufficient). Another consideration is to partition based on both channel id and the bucketed timestamp similar to how Discord store their data initially.The Websocket Server is relative simple to scale because they are stateless, we could add more servers if the network bandwidth is limited. The Web Server that handles the HTTP post requests for sending message could also be handled in the similar strategy.The Channel Server could be scaled by horizontal scaling as well, to partition the channel_id and have each server to handle a portion of the channels. However, since we are dealing with realtime messaging scenario, it is pretty critical to minimize the latency during the time that one node is down or we add additional node into the cluster to handle more traffic. That is to say, minimizing the data we need to transfer during rebalancing is critical. Thus, adopting Consistent Hashing would be a great option here. Which server handles which range of ids on the consistent ring is a consensus that needs to be made among all servers. This could be achieved by running a gossip protocol among all servers or use a dedicated coordination service to manage this assignment information. The downsides of gossip protocol is that there would be some performance penalty when the number of nodes in the cluster is large; the cons of using coordination service is that we need to manage and maintain another service within our system. To figure out which Channel Server to send the message to, Web Server could first check the coordination service or any channel server to get the assignment information and cache it locally to reduce the volume of requests. Since this information is not changed that frequently, we could refresh this information on a cadence or use read-repair strategy (refresh the info when channel sever refused the request due to staled information)Although we use Consistent Hashing and channel_id to balance the data, there could still be unbalanced load on each server, especially when the number of physical node is relative small. We could consider using virtual node to make the data on each node more balanced; or consider manual configuration for some hotspot channel.Causal ConsistencyIn group messaging scenario, it is pretty critical to guarantee causal consistency; otherwise users in the group might see wired conversation. In our design, we could guarantee this with a single leader replication strategy. Since the message writing is first written to Message DB and then broadcast to users, and we shard the message_table based on channel_id, we could guarantee that the order of the message has and only has a single “version” and there would be no conflict. If we make db write and message broadcast async, then there would be teh case that the order we written to DB is not the same with the order we broadcast, and the causal consistency is break. One potential solution to support async DB write and message broadcasting is to associate a version vector to capture the causality among messages.Additional FunctionalitiesWe have finished the discussion on the majority part of the functionality, let’s have some light discussion on other functionality but going to deep. How to support online presence feature There are different options to implement this, in push mode or in poll mode. In the push mode, we could have a dedicated queue for each user to receive the online signal. When user is online, the client could send a active message periodically via the websocket and forward this message to all users’ friend (or is viewable in client); when user is offline, we could also send such a message to the queue to indicate that the user is offline. This online signal is similar to heartbeat that is widely used in distributed system. In poll mode, the Websocket Server could poll information from the coordination service where we store the user id &lt;-&gt; websocket server id info and use that to indicate if user is online. How to support *XXX is typing…** We could reuse the websocket and send a typing message to the Websocket Server, and then have this message to be broadcast by the Channel Server. Since this type of message does not need persistent, we don’t need to go through the Web Server for simplicity. How to support global service To support global service, we could deploy the Websocket Server to each global region and have user connect to the region that is closet to them. The Channel Server could be in a dedicated region to have the single leader setup. But we could also consider multi leader setup to further scale, with the drawback of write conflicts in some cases. How to support Slack emoji Emoji could be treated as a special type of thread message. We could reuse the data schema defined for thread message, by adding a message_type field with enum type such as text, emoji. If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference 从0到1：微信后台系统的演进之路 Real-time Messaging Redis Pub/Sub In-Depth How Discord Stores Billions of Messages Cassandra - Dynamo - Virtual Node" }, { "title": "Recsys 2024 Paper Summary", "url": "/Recsys-2024-Paper-Summary/", "categories": "Machine Learning", "tags": "machine learning design, sparse features, embeddings", "date": "2025-01-04 00:00:00 -0800", "snippet": "In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research...", "content": "In this post, I would provide a quick summarization to some papers from Recsys 2024 which I think is pretty interesting and worth a read. The list of paper is selected based on my personal research &amp; study interest and is highly objective. I highly encourage you to review the accepted papers as well and I’m happy to discuss if there is some great paper I missed out.There are around 20 papers I have selected and proofread, with a mixing of long/short format paper. For short paper, there is not much details due to the limitation on the number of pages. We would just go over there high level idea and compare it against with some other papers.Here is the list of all papers (PS: the papers are from ACM which might have limited accessibility, sorry): Embedding Optimization for Training Large-scale Deep Learning Recommendation Systems with EMBark AIE: Auction Information Enhanced Framework for CTR Prediction in Online Advertising EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based Recommendations CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation A Multi-modal Modeling Framework for Cold-start Short-video Recommendation Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction Toward 100TB Recommendation Models with Embedding Offloading Short-form Video Needs Long-term Interests: An Industrial Solution for Serving Large User Sequence Models Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other? Scaling Law of Large Sequential Recommendation Models LLMs for User Interest Exploration in Large-scale Recommendation Systems Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation System AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash Attention Bridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking Systems The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential Recommendation Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale RecommendersThe paper FLIP and Better Generalization with Semantic IDs has been discussed before and thus would be skipped in this post. Please refer to my pervious post for details. I would categorized these papers based on their high level domain and group them in individual sections.LLMLarge language model is definitely the most popular topic in recent years. In Recsys 2024, there are several papers related to how LLM could be integrated into the recommendation system.EmbSum: Leveraging the Summarization Capabilities of Large Language Models for Content-Based RecommendationsThis work comes from Meta and one novel idea is to leverage LLM to generate a summary of user interest explicitly before generating the user representation. Some key ideas in the work: The user history (interaction with items in the past) is chunked by session to get ride of context length limitation (besides chunking, actually perform certain compression to the original prompt probably would also work, such as how HLLM optimize to support super long user history). Each session is encoded via a T5 encoder model, and the &lt;SOS&gt; token, which is the special token stands for start of sentence is extracted as a intermediate representation of this user session (classical approach in BERT) Leverage commercial model to generate a summarization of user engagement (they adopted Mixtral-8x22B-Instruct, curious why not Llama 3.1). Then all encoded intermediate representation of user sessions are concatenated together and input into T5 decoder and then have the model to predict the user interest summary (in a NTP fashion) Then the last token from T5 encoder, a.k.a EOS, is concatenated together with the encoded session representation. They go through multiple individual attention block to learn more fine-granularity user representations (similar to multi head attention). And these final representation is going to be used for CTR prediction Candidate is processed similarly as user history, the only difference is how the input prompt is constructed The CTR prediction is computed through a inner product between the final user and item representation, together with a attention mechanism to compute wight to aggregated all matching scoresCALRec: Contrastive Alignment of Generative LLMs for Sequential RecommendationThis work from Google proposed a new framework to fine-tune LLM for sequential recommendation and they adopted a generative recommendation strategy, which means directly leverage LLM as the main model for recommendation. Some key insights of this work is: They defined the main task as Next Item Generation Objective, which is try to predict the next item’s text description based on the pervious item in user’s history, which is pretty similar to the NTP task The main task only focus on the token level information, and might miss the big picture on the user/item level. They adopted 2 contrastive learning to overcome this and forge the collaboration signal into LLM: one is to add a contrastive loss between the item embedding from LLM and user history representation from LLM exclude the item, as well as the item embedding conditioned on user history One more interesting part is how the model is inferred online. Since the LLM is generating text output, they adopted one additional BM25 matching to retrieve the actual item from the candidate set that is closet to the LLM output One finding from the work is that LLM could quickly memorize the item description it has seen during fine tuning and thus we could even the BM25 matching stage; but this would cause huge issue for item cold start The work does not beat one of their baseline which adopt item id + text (actually this is very common case in current recommendation system)After reviewing the work, one question I have for this work is how does the model get online trained so that it could learn new item descriptions or new user/item interactions from the production traffic. In the traditional recommendation system, we could enable online training to update the embedding table continuously; while updating LLM parameters would be more challenging even with PEFT technology.A Multi-modal Modeling Framework for Cold-start Short-video RecommendationThis work from Kuaishou is to use the multi-modal model to help resolve the item cold start problem within recommendation system. Their approach is similar to the FLARE paper we have introduced before from Google in my pervious post, which is also combine the content embedding to help boost the new short-video created. Some key differences are as follow: A dedicated modal encoder is designed to encode different type of input, such as text, visual and acoustic. Since the embedding is generated using pre-trained model and these embeddings are freezed, they also introduced a trainable cluster id embedding, which is obtained via K-means algorithm, to be fused with the original modal embedding In the user tower, user embedding interacts with the item embeddings from user behavior sequence via MHA (user embedding as Q and item embeddings as KV); besides, user embedding is also going to interact with the item’s multi-modal embedding from the model encoder, and all these output would go through a multi-modal interest intensity learning network (this is sightly different strategy how id embedding and content embedding is fused compared with FLARE) to aggregate embedding from different modal with a learnable weight (maybe some user are more text focus, while some enjoy visual more) The item tower is similar with one unique gate component. This gate component is used to control how id embedding and multi-modal embedding is merged together. In the beginning while item is still in cold start stage, we could use more multi-modal embedding; while as the item get more exposure and the id embedding carry better collaboration signal, we could increase its effect by turning up the gate parameterUsing LLM definitely provide a good direction to resolve the item cold start problem in recommendation system, as text, image these semantic information is generic and does not require collaboration signal to learn (like we are recommending items based on some common sense from the world). However, user side cold start problem is still there, how could you get the flavor of new user as soon as possible and target them with the right item is pretty critical for their retention on your platform.Bridging Search and Recommendation in Generative Retrieval: Does One Task Help the Other?This is a research paper from Spotify where the researchers would like to verify the hypothesis that, under generative retrieval scenario (not traditional DL based model), would having dedicated model to train on specific task better or having a single model that trained on the joint task is better.In this paper, they only discussed the recommendation and search task, which might due to these 2 scenarios are the most popular one on their platform. Their conclusion is that joint task is better than separated task one. And their explanation is as follow: The joint task would make the overall training data more smooth, which perform a type of normalization. This is relative easy to understand as usually more data points could make the overall distribution less skewed (central limit theory) The joint task would make the intermediate representation learned more regulated, as the representation needs to be perform well on both task, which might help avoid some local minimalThis conclusion might be case by case and need to verified via the actual data of your problem. In my pervious company, we have identified contradict scenarios that the joint trained model would lift the metrics for one task while hinge the other task.LLMs for User Interest Exploration in Large-scale Recommendation SystemsThis is a short paper from Google where they explored the potential of LLM to inference user interest. First they build a interest cluster and assign item with cluster ids. Then based on user’s past behavior, inject these cluster id as prompt and let LLM to predict what could be some novel cluster id that user would be interested in. Once get these novel interest cluster id, traditional sequential recommendation is leveraged to generate the recommendation.In order to control the output of LLM and let it understand the term of interest cluster as while as the collaboration signal from their domain, they get some high quality data online and adopted fine tuning on the LLM to improve its quality.Sequential ModelingI have a personal view on the evolution of deep learning model used in recommendation system (highly subjective): in the ancient age, it was a three kingdom period, where collaborative filtering, GBDT and logistic regression share the world next comes the deep neural network, which quickly take over the leading position in recommendation realm due to its superior performance the following trend is the integration of sparse features into model, which dramatically increase the capacity of model after that, we have different types of feature interaction techniques blossom to improve the expressivity of modelIf there is no advance of LLM, it would be sequential modeling’s world.Scaling Law of Large Sequential Recommendation ModelsScaling Law has been in LLM area for a while, and there are many more different style of scaling law coming out. This paper verified that scaling law also exists in the sequential recommendation models, regarding the data volume, model capacity and training epoch.They also verified that larger model also performs better on the downstream task. However, I have some questions regarding the conclusion here. The better performance might comes from the memorization of the model instead of generalizability.The Elephant in the Room: Rethinking the Usage of Pre-trained Language Model in Sequential RecommendationThis is another research paper study how LLM should be adopted in sequential recommendation. Based on their study, they purposed a framework to balance the quality and cost of adopting LLM: fine tune LLM with sequential data and use the embedding output as the initialization parameters for the ID embedding in traditional DL models.One thing they observed during their study is that, the embedding from deep layers’ head behaves similar to the ID embedding in traditional SR models. Also, they found that they could get similar evaluation result with a LLM only fine tuned a few layers using sequential data, which means there are lots of redundancy in LLM parameters.Training &amp; InferenceEmbedding Optimization for Training Large-scale Deep Learning Recommendation Systems with EMBarkThis work is from NVIDIA about how to optimize the large embedding tables within the traditional deep learning recommendation. I’m still learning this area and just share this paper here for awareness. Their work is incorporated in HugeCTR repo.Toward 100TB Recommendation Models with Embedding OffloadingThis is a short paper from Meta which introduce CPU offloading during large model training. With offloading, we could train model with large embedding tables that exceed the available GPU memory. One key optimization here is to overlap the data transfer between CPU and GPU with computation to minimize the latency impact. Thus they adopted pipelining and pre-fetching to avoid GPU waiting for data to compute. They also optimize the sharding algorithm to balance embedding offloading across all ranks (they used pin-backing algorithm to achieve this).Short-form Video Needs Long-term Interests: An Industrial Solution for Serving Large User Sequence ModelsThis work from Google introduced how to integrate user sequential model into the main ranking model in a resource efficient way. The key idea is to adopt pre-computing idea: moving the user sequential model inference out of the critical path. They designed a User Behavior Service to pre-compute the embedding and export them to offline storage. During inference, this pre-computed user embedding is used. However, the user sequential model and the main model is co-trained.Enhancing Performance and Scalability of Large-Scale Recommendation Systems with Jagged Flash AttentionA quick introduction to Jagged Tensor and Jagged Flash Attention. The work is implemented via Triton which is a very popular library for authoring CUDA kernels.LLM AgentsI have the fortune to work on some LLM code agent as part of my project in the new company and have the chance to holistically review the work in this area. From my current experience, developing coding agent is not as simple as writing some good prompt and make a call to LLM. There are lots of things need to be considered to make the agent generate high quality result consistently.AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code RecommendationsThis is the only one paper I have found related to LLM coding agent in Recsys. The overall architecture is a RAG system. They designed a context engine to find the best context to be integrated into prompt. This context engine would retrieve from multiple sources to improve recall and then go through a ranking model to find the best candidate.The author also introduced some high level challenge they have faced with when developing their coding agent. One is regarding the data privacy. Lots of context data only exists on users’ devices and could not be logged to server. Another one is the data sparsity regarding high quality labeled data. They have to have experienced engineering to label them manually, which is pretty inefficient (I also encountered similar situation in my project).Recommendation SystemThere are still lots of study and research fall into the old school style.AIE: Auction Information Enhanced Framework for CTR Prediction in Online AdvertisingThis is the first paper I have read that talks about auction bias which exists in the Ads delivery system. In Ads delivery system, the final rank usually is computed as $eCPM = pCTR * bid$, which meas that if a very low quality ads (no one like it, pCTR is low) always places a high bid (I have nothing but money), then our system would rank these ads high, leading to the system would only collect negative feedback (no one is clicking on our recommendation). This is pretty dangerous as our model need positive feedback signal to learn well. If the low quality ads slowly overwhelm the system, then it would be hard to improve our model to provide accurate pCTR estimation.The paper mentioned 2 techniques to resolve the issue. One is to use the auction information as additional input during model training to help model debias. This is pretty similar to how we resolve the position bias. Note that since these information is only available after the model inference, they falls into the definition of privileged features (please refer to this paper for details). Another one is to introduce a auxiliary task to predict the price of the ads and compute a weight for the positive samples in the main CTR prediction task (similar to IPS).Co-optimize Content Generation and Consumption in a Large Scale Video Recommendation SystemThis paper from Google discussed about the problem of how to incentive users to generate more content, which means could we recommend some videos to user that could motivate them to become a video uploader. This is one kind of down funnel problem, similar to ads click -&gt; website conversion.They primarily adopted the multi-task modeling framework. Some main technique used in their work is as follow: to overcome the sparsity of label, they adopted proxy label which is known to be highly positive correlated with content generation conditional loss is used to reduce the noisy, such as the logout user session MMoE is adopted in the main architecture of the model, where the gate is controllable; resnet is also adopted for representation learningBridging the Gap: Unpacking the Hidden Challenges in Knowledge Distillation for Online Ranking SystemsThis is a pretty good paper to learn about the industrial practice of adopting knowledge distillation in recommendation system. The paper mainly focused on three problems: how to handle the distribution shift between teacher and student (the teacher might has learnt some bias during offline training, and this would also be learnt by student) how to efficiently manage the configuration of teacher model (MLOps) how to reduce the maintenance cost of KD infra (MLOps)Regrading the first problem, they adopted a auxiliary task for knowledge distillation instead of having the student model directly trained on teacher’s label. For the second problem, they do it a hard way which is similar to a grid search to find the best configuration in one shot. For the last problem, they use a single teacher model to distill several student model, where the output from the teacher model is written into a columnar database.Self-Auxiliary Distillation for Sample Efficient Learning in Google-Scale RecommendersThis paper introduce a new way of doing knowledge distillation, which could help avoid the mis-calibration issue caused by teacher’s soft label. The main idea is to add a auxiliary head which works as a student. The main head is used for searching to to generate teacher soft labels, and this label is considered together with the ground truth label via a label selector (e.g. curriculum learning) and then decided the label that the student head should learn.One thing that has impressed me a lot is that they used this new distillation for the signal loss scenario and achieved pretty good result. If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" }, { "title": "Trend of Sparse Features in Recommendation System", "url": "/Machine-Learning-System-Design-Sparse-Features/", "categories": "Machine Learning", "tags": "machine learning design, sparse features, embeddings", "date": "2024-11-03 00:00:00 -0700", "snippet": "In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as revi...", "content": "In my pervious post, I have briefly mentioned about sparse features and how they could be used in recommendation system. In this post, let’s have a deeper look into sparse features, as well as reviewing some new ideas about how sparse features are being used in modern recommendation system, especially in the realm of LLM world.What is sparse featuresSparse features are usually composed as some entity ids that a user has taken certain action during a period of time. For example, the facebook post ids that a user has viewed in the last 24 hours or the amazon product ids that a user has purchased in last week. These ids would be feed into model as input to generate predictions. Usually, we would go through a component called Embedding Lookup Table to convert these raw meaningless ids into some dense vectors so that they could be numerically computed with other features or layer weights. The Embedding Lookup Table would be updated along with other model parameters during training stage. A simple illustration is as followIf you come from XGBoost world, you might be as surprised as I was when seeing how ids are being used as feature into a model. XGBoost could not directly use id as input features. To leverage these ids, we usually need to manually crafting a dense version of these ids, for example, aggregating the number of posts that the user has viewed in the last 24 hours. If we need to have some more fine granularity view, a common practice in industry is to add additional breakdowns, e.g. breakdown on the category of posts.However, one obvious limitation to this feature engineering methodology is that it lose memorization of user interactions and lose subtle differences among entities. For example, only knowing a user has viewed 3 posts related to sports is much less predictive than knowing the user has viewed 3 posts from NBA official account; and even though the post is related to basketball, users reaction to NBA and to CBA could be pretty different. Representing entities in their raw ids, converting them to dense vectors (a.k.a embeddings) and training with the target dataset (e.g. user clicks) could help memorize user interactions and learn a better representation of these entities in the target domain.Recently I have watched a talk given by Hyung Won Chung in MIT and one opinion from him impress me a lot regarding why GPT4 or other large language model suddenly demonstrate such powerful performance: We should enforce less structure in learning; instead, we should leverage the more and more easily accessible computing power to learn from unstructured dataI think the adoption of sparse features in recommendation system also matches to this, where we deprecate the structure part (feature engineering) and move towards less structure (raw ids) and more computing (more capacity of the model).Recent works on sparse featuresAs we have a brief review on what is sparse feature, let’s move to some recent works on sparse feature, or embedding, and see what kind of problem they are trying to resolve. I hope that this could help you borden your ideas and benefit you either for the preparation of machine learning design interview or for ML projects that you are working on.Shareability of embeddingsMaking the embeddings learnt from one domain to be reusable in another domain has been a pretty popular research topic in industry due to a practical reason: memorization of history knowledge without full retraining. For example, reuse the same sparse features’ embedding from CTR model to CVR model; or reuse the embeddings from last month’s model to new release candidate to carry-over the past knowledge. However, naively extracting the embeddings from source model and integrating them into target model does not work well. A technique called transfer learning is usually used to mitigate this type of incompatibility issue.In this paper from Meituan (a Chinese company similar to DoorDash and has pretty strong technique in cross-domain recommendation), the author proposed a new way to transfer learning from source domain to target domain. They leveraged the idea of pre-trained model dominating LLM world and the hierarchy philosophy for information compression: First, a model called tiny pre-trained model (TPM) is trained. TPM is trained with simple model architecture, less number of features but large volume of data (past 6 months). Each month’s snapshot of embedding is extracted and stored separately Second, a model called complete pre-trained model (CPM) is trained. CPM is trained with the exact same architecture and features against the target domain model on past 1 month of source domain data. The embedding from TPM is adopted for CPM model training (bootstrapping) via an attention pooling. The training of CPM help mitigate the issue of full-qualified-name mismatch issue during model parameter initialization with the cost of flexibility Finally, the target domain model is trained. The model is initialized with the parameters from CPM (for both embeddings and dense parameters, batch norm layer’s is dropped due to different traffic) and then do incremental training on the past several days data on target domain TPM helps memorize long term history, and is refreshed monthly; CPM helps to fuse long term and short term memory, and is refreshed weekly; target domain model adapt to latest online traffic, and is refreshed dailyThe idea of this method is not very complex. In my pervious company, we have been trying to use the embeddings from production model to bootstrap the performance of new model candidates so that it could catch up more quickly. This method requires less cost compared to knowledge distillation and data augmentation, but putting more pressure on MLOps as well as making the model development cycle much more complex.Generalizability of embeddingsAs mentioned above, embeddings in recommendation system are usually learnt from the training dataset, which is composed of user’s behavior information against the candidates. Actually, we could assume that the embedding learnt is trying to capture the collaborative signal from users’ behavior. It is similar to the non-model based approach such as Collaborative Filtering, which is also trying to compute a vector representation for each user/item for the matrix filling task. This actually puts some limitation on the generalizability of embeddings. For example, the learnt embedding of a pizza restaurant in Sunnyvale might be quite different from a pizza restaurant in Beijing because of the difference in the population or culture (which is observed via data we collected), even though their branding name includes pizza text which is an obvious information for humans to understand that these two restaurant should be the same (I was asked a similar question during my ML interview with Airbnb :joy_cat:). In the realm of LLM, we have more powerful tool to process such text/image information and extract their semantic information. How we could better integrate such semantic information into traditional id embeddings has been a popular topic recently.In this paper LARR from Meituan (yes, besides cross-domain problem, they also have cross-scene POI problem), the author proposed one approach to align the semantic embedding generated from LLM with the collaborative id embedding learnt from recommendation system, to improve the performance under realtime scene recommendation scenario. The main idea is to first generate the semantic embedding via text constructed via heuristic rules and then leverage contrastive learning to align the embeddings: First, a LLM is fine-tuned with the rich textual data available from the entities, e.g. the name and description of the restaurant. During the generation of the input into LLM, different textual feature is separated with different special token to help LLM tell apart them (a pretty common practice). In their setup, the input text is the name and location of the restaurant, and they try to predict the description and menu of the restaurant. This helps LLM to learn the connection between location and associated dishes In the second stage, the fine-tuned LLM would be used to generate embedding for different entities and further trained via contrastive learning. The embedding of last token from the input sequence would be projected via a MLP layer, and then scored through a similarity function (e.g. cosin) with another embedding. They preformed user profile &lt;-&gt; user profile, POI &lt;-&gt; POI and user profile &lt;-&gt; POI contrastive learning. The final loss of this training stage is a linear combination of these 3 contrastive learning loss, which are all similar to\\[L = - \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\exp(s(u, u_{+}))}{\\exp(s(u, u_{+})) + \\sum \\exp(s(u, u_{-}))}\\] In the last stage, the parameters of LLM are freezed and used to generate semantic embedding from realtime scene text features. However, these embeddings lack collaborative signal and thus require some additional processing. Due to the inference limitation, only 10 realtime scene text feature is used, each of them would be processed through LLM, and then one additional bi-directional encoder is used to aggregate these 10 embeddings, with the information summarized into a special token &lt;agg&gt;. Finally a MLP is used to project the aggregation into the same dimension of collaborative id embeddings, and then one additional contrastive learning is adopted (so many CL :laughing:).Google recently also published one paper on how to better leverage the text information of ids to boost the traditional collaborative id embeddings. In this work, they adopted masked language modeling and CLIP style contrastive learning to make the long tail ids benefit from the popular ids. For example, Pizza Hut and Coding Monkey Pizza are both pizza restaurant, but Coding Monkey Pizza would not receive much exposures (a.k.a impressions) similar to Pizza Hut, and thus we could not learn a good embedding for Coding Monkey Pizza (lack of training samples). However, leverage the text data, such as the description of the restaurant, both Pizza Hut and Coding Monkey Pizza would share similar semantic embeddings, and this would help connect them and help Coding Monkey Pizza to steal some information from Pizza Hut. There high level approach is as follow: One part of the training objective comes from the masked language modeling type loss. For a sequence of item ids (e.g. the product id user has purchased), some of them would be randomly masked, and then the ids would go through the embedding lookup table, and then go through the transformer to predict the masked id. The embedding is a combination of the collaborative id embedding and LLM processed text embeddings Another part of the training objective is the alignment between the collaborative id embedding and its corresponding semantic embedding, via contrastive learning One additional thing besides the &lt;id, text&gt; pair is the critique string, which is also some text information, but separately encoded via LLM. This information is not masked during MLM and the reason for that is to encourage model to learn to predict target id via semantic information instead of the memorizing idsThe last example trying to align semantic embedding from LLM and id embedding comes from Huawei. In this work, they proposed a framework to align this 2 types of embedding, which is suitable for most of the model architecture. Similar to the work from Google, they also adopted MLM for training, but their method steps more towards the CLIP style modeling: For any traditional sparse features, they are going to convert it to a text narrative (different from the Meituan’s work where the text is already available), similar to a combination of feature name: narrative of feature value pair. Then all sparse features’ text narrative would be concat together. During the training, some random feature is going to be masked. And then, given the LLM embedding on the text narrative, and the masked id embedding, predict the masked feature value (called MTM); or given the id embedding and masked text narrative, predict the masked text tokens (MLM) In MLM, the id embedding is concat to all text embeddings for prediction; in MTM, cross-attention, pre-feature MLP and InfoNCE is used for prediction Besides the alignment within instance (feature level), instance level contrastive learning is also used After aligning both semantic embedding and id embedding, they are fine-tuned with downstream task via a tow-tower architectureCardinality of embeddingsOne hidden story I haven’t talk about is how actually a raw id get converted to a dense vector through the embedding lookup table. In general, the raw id would be converted to an index within the embedding lookup table and retrieve the corresponding vector. If the total number of raw ids is not that large, we could have a 1-to-1 mapping between ids and index (in another world, the column size of the embedding lookup table is the same as the number of ids). However, if we have much much more number of ids, it is impossible to have a that large table. In this scenario, we would apply what is called hashing trick: apply a hash function on the id and mod the total column number. This means that a single vector actually represent multiple different ids which might be totally irrelevant with each other: they might be contradict with each other; or the vector is overwhelm by popular ids. In my pervious company, I have asked about this issue and proposed if we could infuse certain category information into id or id hashing function to alleviate this collision issue, but didn’t work it out due to “ruthless prioritization” :pensive:.Google recently proposed a new solution similar to categorization, which is called semantic id. The high level idea is to learn a implicit hierarchy structure to index ids, so that the ids sharing similar semantic information would be grouped closer: The solution is composed by 2 parts: the first part learns a model to encode ids; the second part freeze the model learnt and encode ids for downstream model training In the first part, they adopted RQ-VAE to encode the content embeddings; the codex id within each layer of RQ is concat together to compose the semantic id. Via this approach, the ids that share similar content (because of similar content embedding) would share a similar prefix within the semantic id, but still would preserver some slightly difference in the tail part of the id. The training part follows the VAE, where the codex id’s corresponding embedding is retrieved and summed together and try to reconstruct to the input content embedding In the second part, based on the semantic id, we would learn another set of embedding based on them. The semantic id is a sequence of codex id, and we could use different strategy to convert them to dense vectors, we could use ngram to create different combinations, or we could use Sentence Piece Model to dynamically group them, and then go through the embedding lookup table (here we could have a 1-to-1 mapping similar to LLM, instead of using hashing trick again)My bet on the future trendAfter reviewing these work, here is some of my bet on the future of how sparse features or embeddings going to involve in recommendation system: Using LLM’s output as additional input into traditional DRS model would still be the main-stream. More exploration would be done here, such as fine-tune LLM for specific downstream task to generate better semantic embeddings, adopt multi-modal LLM to ingest richer information into RS, leverage more semantic embeddings to resolve the cold-start problem There would be more work studying how to better combine embedding generated from LLM and traditional id embedding to improve the generalization capability of recommendation system In the future, the embedding might take more and more responsibility for memory and the dense layers take more on computing and reasoning; better structure of embedding would help encode more implicit information which could be unveiled during computing Due to the cost of inference of LLM, semantic embedding would probably still be precomputed, and this would put some new challenge on the feature serving and management infrastructure If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeAcknowledgementThanks Yunzhong and Yitong for the great discussion on these papers.Reference Efficient Transfer Learning Framework for Cross-Domain Click-Through Rate Prediction LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding FLARE: Fusing Language Models and Collaborative Architectures for Recommender Enhancement FLIP: Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations" }, { "title": "How to Design Online Chess Game", "url": "/How-to-design-online-chess-game/", "categories": "Distributed System", "tags": "distributed system, stateful service, message queue", "date": "2024-10-10 00:00:00 -0700", "snippet": "I have been a fan of online game since my parent bought me a PC when I was in 5th grade. I could still feel the excitement when every Thursday night I rushed home and played DaHuaXiYou 2 with my fr...", "content": "I have been a fan of online game since my parent bought me a PC when I was in 5th grade. I could still feel the excitement when every Thursday night I rushed home and played DaHuaXiYou 2 with my friends. This is also the direct reason why I decided to major in computer science in college. Today, let’s discuss something related to online game: how to design an online chess game.Functional RequirementsHere is a minimal set of functional requirements for our online chess game: Two users could play chess against each other, 1-1 game, take move by turns Player could revert their last move if their opponent hasn’t made a move yet Player could resume from temporary disconnection and join back the same gameNon Functional Requirements High availability, no single point of failure Low latency High scalability Consistency, both player should see the same board stateAssumptionsThese are some basic assumptions we would made in our design. In real world system design interview, these would be some good questions to get clarification from the interviewer: In this discussion, we would only focus on the core logic of the game. Additional features, e.g. player ranking, profile system and player matching, would be out of the scope for today’s post and I plan to have a separate post to discuss them We also assume that each player could be at most one game at a time, this would help us to simplify our discussion. However, it should be relative straightforward to integrate session managementBackend Envelope EstimationWe would assume that our online chess game is pretty popular and have a DAU of 100 million based on this source. And since its a chess game, user need to take turns to make a move, we could assume that on average each player takes 10 second to make a move. We assume each player would play 10 games every day. This translate to about $100 million / 10^5 * 10 (games / day) * 0.1 (move / sec) = 1000 QPS$. We could assume that at peak time, the QPS could be 5x which gives us 5000 QPS.On the storage part, we actually don’t need to have too much storage (which we would discuss more in details in the next section), as either the game state nor the moves take much space to store. For the visualization element such as the picture of the board, they could be shared across all games and is some statistic resource, which could be stored in CDN. We assume that each game would require 10KB storage, which gives us about 1TB storage per day.Key ChallengesHere is some technical challenges on top of my mind, we would discuss how different design choices going to affect the answer of these questions: How to maintain the state of the game? How to keep users connected with each other to paly the game? How to handle difference racing conditions? (e.g. Player A make a move while Player B tries to revert move) How to handle different failure mode? (e.g. Player A is disconnected due to network issue)High Level DesignIn this post, we would introduce 2 different architectures. There are 2 different philosophies on how to represent game state: keeping a latest view of the game, which is pretty common practice in database community; or keeping a series of event that has happened, a.k.a event sourcing, which is a relative new practice in the world of distributed system. These 2 different philosophies lead to 2 different design choices: stateful game server and pub/sub based architecture.Stateful Game ServerA high level design of this approach is shown below:The Game Server component would be responsible for maintaining the game state and connection with players. The game state is relative simple, it could some something like this:# game stateboard: str # A 8x8 board represented as a string, different characters represent different pieces, e.g. 'Q' for queencurrent_turn: bool # True if it's player 1's turn, False if it's player 2's turnis_win: bool # True if the game is won, False otherwiseThe game state is not large and we could maintain it in memory together with other game metadata such as game_id, player_id for the lowest latency. The game logic would also be implemented on the game server, which we would check if the move is valid and update the game state accordingly. Upon the creation of the game and both players have connected with the Game Server, the server would write a record into the Game DB. The schema of the table is shown below:The game_table records all critical metadata about the game, such as the player_id, status and winner information. Once the record is created, the status of the game would be set to RUNNING and once one player wins, the Game Server would update the status to DONE and record the winner information. You could also notice that we have created a game_history_table to record all the moves of the game. This table is optional in this stateful design (but would be critical for the next design choice), it captures a trajectory of the game state (each move that is accepted by the game server) and we could rebuild the game state from this table in certain failure scenarios (we would explain more in the next section).We could use SQL or NoSQL database to store game_table and game_history_table. The read/write ratio of the game_table is relative low and it is keeping critical information about the game, we could use relational database to handle it (although we loss the flexibility of things such as schemaless, horizontal scalability). For the game_history_table, it would expect relative high write QPS as each move needs to be recorded, we could consider using columnar database to handle it (relational database in append only mode would also work).Having talked about the game state and the storage, let’s move our focus onto how players are connected and how the game is played. There are multiple approaches for connection, such as long polling, server sent event and websocket (you could refer my pervious post for a comparison of websocket and SSE). Since all players need to continuously make moves and receive updates (bi-directional) during the game, websocket would be a good fit in this case (this is different from the auction system’s choice due to the scenarios are different, check here for more details).The Game Server would maintain a websocket connection with players. When one player make a move, the client would send a message to the server via the websocket connection. The Game Server would then check if the request is valid or not and apply it to its in memory game state. If we would like to record the step into game_history_table, then we need to make sure the write is confirmed by the database and then apply the change to in memory game state to keep it consistent. Once the game state is updated, then the server would send the updated game state to both players to make sure both players are in sync.The benefits of this stateful game server design is that, for each game we have a single brain to decide what the game should be like in the case of concurrent move requests from players. If player A tries to revert move while player B tries to make a new move and they both send out the request at the same time, then the Game Server would be the single place to make a decision on which request to accept and which to reject (maybe based on which request first hit the Game Server). For example, if player A’s revert request hit first, then player B’s move request would be rejected as the move would be illegal; the new game state would be sent back to both player and it would still be player A’s turn to make move, vice verse. However, any coin has two sides, the beneficial of handling concurrent request gracefully comes with the cost of a single point of failure. We would discuss more on this in the high availability section.Besides the Game Server, we also included a Coordination Service component. This component is responsible for keeping the liveness information of the Game Server as well as playing a role on service discovery by storing associated metadata about the game_id and server_id. The liveness is used for high availability discussed later. The service discovery is used for the player to rejoin the game after they have been disconnected. Upon the creation of the game, the Game Server would also register this serving information into Coordination Service, which essentially is a mapping from game_id to server_id. When player disconnected, they could rejoin the same game as follow: We could cache the server id information in the client side as well (although this is going to expose some internal information about our cluster and expose certain security issue), if player disconnected due to network issue, then they could reconnect to the same Game Server by this cached information We could also not caching any server id information in the client side, or the client’s application crash and the cached information is all lost. In this case, the player would be directed by load balancer to any Game Server (which might not be the one that is hosting the game), and the Game Server would check if the player’s game is on itself. It send a request to Game DB to retrieve the RUNNING game that the player is still in and then check the server_id from the Coordination Service. Once obtain the correct server_id, it would then redirect the player to connect to that server. And once connected, the game could be resumed from where it is left (the rejoined player would be sent with latest game state to make sure in sync)On the client side, since the game state is synced from the Game Server, we could implement a simple logic that only if the current turn is the player’s turn, then we would send player’s move to the Game Server. This could help us to reduce the volume of the message sent over the websocket connection to save the network bandwidth.Pub/Sub Based ArchitectureA high level design of this approach is shown below:In this design, there is no Game Server to maintain the game state. Instead, all player’s move are represented as an event and published to a message queue and the game state is obtained by applying this sequence of events from beginning to end. Here we would adopt message queue that support message persistence, such as log based message queue, so that we could support certain failure mode such as player disconnection.Upon the creation of the game, both players would connect to a Websocket Server, which is responsible for keeping the websocket connection with the player, and publish/subscribe to the topic that stores each move. Along with the creation of the game, a dedicated topic for this game would also be created in the message queue, with the format such as chess-game-{game-id}. The Websocket Server would publish player’s move to this topic and also subscribe to this topic to obtain other player’s moves. When player makes a move, a message would be sent over the websocket connection to the Websocket Server and then publish to the topic. To guarantee the consistency of the game, Websocket Server would wait for ack from the message queue to make sure that the message is published successfully. Once Websocket Server received one new message from the topic, it would relay this message to the player via the websocket connection. From the client side, after sending the move to the server, it would wait for certain threshold time to receive the message back from the server. This message could be the player’s pervious move, or the opponent’s concurrent move. Based on what is received, game state is changed and corresponding logic would be applied (e.g. lock player’s move and wait for opponent’s move, game is done or still player’s turn and pervious move is ignored). Upon timeout, the client would retry sending the move again.Notice that in this design, there is no game server on our backend that have a current view of the game. We delegate this responsibility to the client side, which means that the client would maintain the game state, as well as game logic such as checking if the player’s move is legal or not. Once one player win, a special message would be sent to Websocket Server, and the server would update the game_table in Game DB to record the winner information.One benefits of using message queue to record the event is that, we could still maintain a single view of the sequence of the events happened and avoid the potential split brain problem in the world of concurrency (but this also depends on message queue’s replication strategy). Let’s discuss a little bit about different scenarios and see how we could still maintain a valid game state: Player A tries to revert move while Player B tries to make a new move In this case, we would write 2 message into the message queue: one is a revert message and one is a new move message. Even though they are conflicting with each other, the single topic would still guarantee that only one sequence would be stored, player a, player b or player b, player a (depending on which message first get to the message queue broker); thus on the client side, we would still receive the same sequence, and thus we could have logic to ignore the invalid message to make sure both player’s are in sync and the game state is valid Player A made a move, but there is some network delay to receive if the message is published successfully, and thus retried; in the meanwhile, Player B received A’s move already and made a move This is a slightly more complicated concurrency issue. There are 2 possible approaches to solve it: 1. use the vector lock to capture the casual relationship among the events and ignore the message that is invalid; 2. use an idempotence key and deduplicate the message that has been processed already. Both solution works, one add additional complexity to the game logic and the other add complexity to the design as additional component need to be added. With vector lock, the player would bump the version number from the previous player’s version. In the above scenario, we would see message sequence like the code below in the topic. Retried message could be ignored as the version number is smaller than the pervious one [ (p1-1, move-1), (p2-2, move-2), (p1-3, move-3), (p2-4, move-4), # delay happens here (p1-3, move-3), # retry (p1-3, move-3), # retry ... ] With idempotence key, we could use a simple cache to record if the move has been already processed or not and avoid publish the retried message into the topic Player A made a move, received the message back and wait for Player B’s move, but Player B haven’t received Player A’s move yet and is waiting for Player A’s move In this case, it is similar to a deadlock situation where both player is waiting for the move due to the out-of-sync. The good news is that there is no new message being written into the topic and the game state is still valid. The reason of the deadlock could be that the Websocket Server connected with player B is temporarily portioned from the message queue broker. We could add a retry mechanism in the client side if there is no new message received from the server, we terminate the current websocket connection and establish a new one through Load Balancer. In this design, once player is disconnected, they don’t have to connect to the original Websocket Server as the game state is stored in the message queue. The player could connect to any Websocket Server, and query game_table to retrieve the ongoing game_id information, and then subscribe to the topic, consuming the messages from the beginning of the topic to rebuild the game state.As message queue is not intended for persistent storage, we could move the message to a database table such as game_event_table to free up the resource in message queue. This could be done via a offline daily job, that scan the game_table to find all game that has already been finished, and them move the message data from the message queue to game_event_table. This could be done via a dedicated in-house service, or some solution offered by the eco-system of the message queue.High AvailabilityIn this section, let’s discuss how to achieve high availability for our online chess game.Game DBThe Game DB is one critical component as it stores all metadata about the game. We need to make sure the data stored is replicated to avoid it being the single point of failure. There is different strategy to achieve this: Single Leader Replication, Multi Leader Replication and Quorum Based Replication, as well as Synchronized and Asynchronized message replication among the replicas. I would not expand too much on this topic, you could refer to my video on DDIA Chapter 5 for more details.Message QueueThe message queue is another critical component in the pub/sub based architecture as all events data are stored there. We could use the similar strategy we used in Game DB to achieve high availability here. However, if we adopt Multi Leader Replication or Quorum Based Replication, there could be write conflict (e.g. player A’s revert and player B’s move) and it is challenging to resolve it (we could not directly merge them like a shop cart, nor we could ask client to do a custom fix as there are 2 players). Using Single Leader Replication would be a good choice for us to easier handle the consistency of the game, which is critical to online chess game. Also if we use Asynchronized replication, we might loss some events as the message is not replicated from leader to followers yet. Players would see that the game state transition back to the pervious status, and this might cause some issue especially in such turn-based competing game (you read the opponent’s move :joy_cat:). So in this case, adopt Synchronized replication might be a better option.Game ServerThe Game Server component is critical in the stateful game server design as it is maintaining the state of the game. There are several options to make it fault tolerant: WAL, this might be useful in the case that the Game Server is managing each game via multiple threads or processes, and the thread/process crash accidentally. The Game Server in this case could quickly rebuild the game state by reading the content in WAL file KV Cache, if the Game Server crash, then we need to store the game state somewhere aside from the server to make it possible to resume the game from a new server instead of waiting for the original server to become alive. We could use a KV store to store the game state periodically. If the original game server crash and we need to have a new server to take over, the new server could read the game state from the KV store and resume the game Replicate on other Game Server, nothing blocks us from replicating the game state to other game servers similar to what we do for Game DB and message queue. We could run a simple leader election upon the creation of the game to decided which server would be the leader to handle all the read/write requests, and replicate the game state some some other selected game servers. These information would be stored in Coordination Service. Also each game server would also send heartbeat to Coordination Service for the liveness check. If one replica find that the leader is dead, it could initiate a new leader election and nominate itself as the new leader. Once new leader is elected, players (right now should be retrying connecting) would be redirected to the new leader and the game would resume from there. Similar to the discussion in the above section, adopting Synchronized replication might be a better option to avoid the traverse back issue. If we don’t use the Coordination Service to handle the leader election, we could also run a consensus algorithm such as RAFT under the hood to select a server as the leader of a game and other servers could be followers. However, running consensus algorithm on a large cluster would cause high performance cost.Adopting KV store seems to be a attractive option, but there are some additional points we should bear in mind to have a more comprehensive understanding on this option. One thing is the frequency to store the game state, a.k.a snapshotting. We could snapshot at each move, but this would cause large delay; or we could snapshot after certain batch of moves or certain period of time, but this has the risk of losing part of the game state. This is a trade off we need to discuss based on the scenario. In the case of online chess game, since majority of time is spent by players thinking about their next move, thus the delay incurred by snapshotting after each move is relatively low.In both the KV store and Replication on other Game Server approach, it is critical to have a single server as the leader on the game and have the single authority to determine the game state as well as store it. This is the reason why we have Coordination Service to help achieve this. Replication on other Game Server could leverage the Coordination Service for the leader election (e.g. use sequential/ephemeral node in Zookeeper or use etcd leader election API). In the KV store approach, the Coordination Service would act as a lease manager to make sure that at any time, only one server could hold the lease and thus be the leader of the game. For example, if Game Server A is the current leader part suddenly portioned from the Coordination Service, the Coordination Service would wait for certain time to make sure the lease has passed to guarantee that any remaining write from Game Server A has been applied to the KV cache, and then a new server could acquire the lease and become the new leader, start reading the snapshot from the KV cache and resume the game. We also need to be able to pause the game from the old server and redirect the player to the new server, this could be done by having a check to see if the game server still receive heartbeat ack from the Coordination Service, if not, then after the lease timeout, it would disable the connection it is holding so that players would trigger the reconnection logic to be direct to the new game server.Coordination ServiceThis is usually some third party service that is running on a small quorum of nodes with some distributed consensus algorithm such as RAFT to achieve high availability. Discussion into the details of these algorithm is usually out the scope of traditional system design interview. But I still recommend to learn about these algorithm as they are critical to understand the underlying principle of distributed system.Here is a final view of the stateful game server design with high availability consideration:Websocket ServerThere is no special treatment for the Websocket Server to make it highly available. As their primary goal is to keep the websocket connection with players. If one server is down, the player could connect to another server and resume the game from there.High ScalabilityThe scalability of the 2 design choice is slightly different. The pub/sub based architecture is more scalable due to the following factors: The decoupling of player connection and game state storage. As the number of players increasing, the number of websocket connections and the game state need to be stored also increase. In pub/sub based architecture, as we have decoupled these 2 properties into different components, we have the freedom to scale them independently. While in the stateful game server design, these 2 properties are coupled on the same game server and make it hard to scale appropriatelyIn both design, the data could be distributed across multiple nodes by sharding on game_id with hashing functions. Key range based sharding might not be a good choice here due to the imbalanced distribution of the data. For all the tables we stored in Game DB, such as game_table and game_event_table, we could also shard them by the game_id.ComparisonOverall, stateful game server is relative easier to maintain and achieve the consistency requirement; while pub/sub based architecture is more scalable.   Stateful Game Server Pub/Sub Based Architecture Game state Final state Sequence of events Complexity Low, easier to reason about High, need to reason about the events and handle the potential illegal ones Latency Low, single round of connection Higher, 2 step of connection High Availability Achieved by snapshotting game state Achieved by replication strategy of message queue Scalability Need to trade off between connection, CPU/Mem usage Easier to scale horizontally and individually Debugability Low, need additional setup such as game_history_table High, natively support that If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference Zookeeper Etcd 我用消息队列做了一个联机小游戏 Raft Python Websocket Pulsar" }, { "title": "How Workflow Get Scheduled via Plugins in Flyte", "url": "/Flyte-How-Workflow-Get-Scheduled/", "categories": "Distributed System", "tags": "open source, job scheduler", "date": "2024-09-12 00:00:00 -0700", "snippet": "Reading open source code has been a recommended approach for software engineers to learn. However, in my past 8 years career, I didn’t do a good job on that. After working in a startup for 1 year, ...", "content": "Reading open source code has been a recommended approach for software engineers to learn. However, in my past 8 years career, I didn’t do a good job on that. After working in a startup for 1 year, I accidentally foster the habit to read open source code XD. In this post, I would like to share one open source project I have been learning recently, and hope you would enjoy this journey as well.I have been working in ML pipelining for a long time in Meta Ads. However, I didn’t have a comprehensive understanding across the entire stack, especially on how the underlying infra schedule the model training job and execute it over a fleet of machines. Recently, I have been exposed to an open source project: Flyte, which is a orchestrator for ML pipeline built on top of Kubernetes. I think this might be a good opportunity for me to gain some deep understanding in this area.I have always been a believer of “Learning by Doing”. My ultimate goal on learning this open source code is to implement a simplified version of ML pipeline orchestrator on my own. Next, let’s see what problem we are going to discuss in this post.ProblemIn Flyte, we could use something called Plugin for distributed training, e.g. PyTorch Plugin. In this post, we would discuss how these plugins are getting invoked, so that the distributed training job we defined could get executed. In this post, I would simplify the discussion and only laser eye on the main flow, for other important topics such as storage, pipeline definition and compilation, availability and scalability, I plan to defer it to later posts.High level architectureThe key component that is responsible for scheduling and monitoring the workflow in Flyte is called FlytePropeller. It tries to push FlyteWorkflow, which is defined as a Custom Resource Definition in k8s, to the desired state leveraging k8s reconcile mechanism. The official document of Flyte has provided a pretty good high level architecture on FlytePropeller’s design, here is a list of the core components: Controller: overall brain of FlytePropeller WorkQueue/WorkerPoll: where worker lives and take jobs to do, a very classic design in job scheduling system WorkflowExecutor: responsible for high-level workflow operations, such as tracking the status of workflow NodeExecutor: responsible for process the node within the workflow and decide the action need to take NodeHandler: different type of handler to execute different type of node in the workflow, e.g. TaskHandler for execute Plugins and WorkflowHandler to execute embedded sub-workflowsKnowing what to do is one thing, and knowing how to do is another thing! Next, let’s jump into the code and see how these components are working internally and see how the logic defined within Plugin could be invoked.Components Deep DiveControllerLet’s get our journey starts with the controller. Controller is the starter for FlytePropeller, it is responsible for initializing other components: In the New function of controller, we would create workqueue here. And then, we would create workerpool here. Note that workerpool requires the workqueue we have created before as part of its initialization (because worker needs to consume the jobs from the queue), and one PropellerHandler notably, the PropellerHandler is initialized with WorkflowExecutor and the WorkflowExecutor is composed of NodeExecutor NodeExecutor requires a nodeHandlerFactory as part of the construction As of now, all the key components we have mentioned in the high level architecture is ready. We would go deeper into them to understand how are they getting invoked.Besides the New function, there is also a run function which plays a critical role on launching the controller. It launches things such as the workerpool, gc and metrics monitors. run function is called within another function Run, in Run, one interesting part is that it is going to leverage the leader election functionality provided by k8s and only let leader to trigger run function. We would discuss this topic more in details in a future post.As the controller would launch workerpool, let’s then move our view to workerpool and workqueue to understand how these 2 components work.WorkerPool/WorkQueueThe workerpool essentially is composed of workqueue and several workers, each are actually goroutines (this is also why Flyte could be pretty scalable on a single CPU, we would discuss this in the future). The Run function in workerpool is the most critical function, which is the one get invoked by controller. The main logic is the for loop here, where we launch multiple goroutines and each goroutine would make a call to runWorker function. The runWorker function is relatively simple, just an endless while loop to call processNextWorkItem function. processNextWorkItem function gets an item from the workqueue and then invokes the PropellerHandler we perviously passed in during initialization. As we could see, the key processing logic resides within PropellerHandler’s Handle function, which is defined as part of the interface here, then let’s move on and see how this Handle works.PropellerHandlerThe Handle function defined by the Propeller struct is the entry point of the reconcile process (Here Propeller has implemented the Handle interface, thus it could be considered as type Handler although there is no explicit inherit, this is how interface implementation works in Golang). The key logic is within this for loop, where we call streak function up to a max trial. The streak function would try to do a single mutation to workflow, and return the mutated workflow upon succeed, otherwise no update made if failed. The workflow here is the CRD FlyteWorkflow and the mutation operation is done via TryMutateWorkflow. TryMutateWorkflow makes calls to workflowExecutor’s HandleFlyteWorkflow function to see if we could reconcile the workflow towards it desired status. We left out other details, such as how to handle failure, how to handle aborted workflow etc. From the code in PropellerHandler, we could observer that the Handler is just doing some high-level logic and the actual workflow processing logic is delegated to workflowExecutor. Now, let’s move to workflowExecutor.WorkflowExecutorThe HandleFlyteWorkflow function called within PropellerHandler is a router function. It invokes other actual logic function based on the status of the workflow. For example, if the workflow status is in WorkflowPhaseRunning, then it would invoke handleRunningWorkflow function. In these functions, a common pattern is that they would setup the context, invoke nodeExecutor’s RecursiveNodeHandler function to get the new status and then update the status. The new status is passed back and used to transit the workflow’s status (which is the reconcile process). Notice that the FlyteWorkflow is passed as parameters for executors.DAGStructure and executors.NodeLookup, as well as the startNode.There is some different operation based on the new status RecursiveNodeHandler passed back. For example, if the new status is partial completed, then the workflow would be enqueue again and return running status.The WorkflowExecutor handles the operation of workflow and decided what action to take. In ML pipeline, we know that workflow is usually composed by several nodes, and these nodes encapsulate the actual computation. Let’s take a look at nodeExecutor, which is responsible for handling this part.NodeExecutorThe RecursiveNodeHandler function is one of the most important function in NodeExecutor. It is the entry point to execute a node within a workflow. It uses actor model and modified version of DFS to traverse the DAG and to execute non-blocked nodes. Based on different status queried based on the starter node passed from input, it applies different logic to proceed. For example, if the node status is already succeed, skipped or recovered, then it would invoke handleDownstream function; while if the node is in status that could be handled, then the key logic happens here: first, based on the node’s kind, a dedicated handler is retrieved from nodeHandlerFactory; then HandleNode function would be invoked to execute the node.The handleDownstream is where the aforementioned modified DFS implemented. The logic is relatively straightforward: starting from the input node, we retrieve all downstream nodes; then we iterate each node and invoke the RecursiveNodeHandler function on each of them, with self as the new input start node; keep the status to check if all downstream nodes have been processed, and return the status accordingly.The HandleNode function of nodeExecutor is also a router function, where different processing function is invoked based on the status of the current node. The most important functions are handleQueuedOrRunningNode and handleNotYetStartedNode: In the handleNotYetStartedNode, the most critic logic is the call to preExecute, where we check if the node could be queued to be further processed. The checking logic is relative simple, where we check the upstream nodes are all in succeed status or not In the handleQueuedOrRunningNode, we would first try to check if there are cached result given the current handler, and trigger the execute function if there is no cache hit. The core part of the execute function is to trigger the Handle function of the input NodeHandler, which is obtained from RecursiveNodeHandler and passed along the stack here, what a long journey!Now, we have hit the most underground part of FlytePropeller’s architecture. Next, we need to dive into NodeHandler to understand how the Handle function is implemented (here we would focus on how the handler used to fulfill the operations we need in distributed training).NodeHandlerFrom the section above, we know that we retrieve node handler from nodeHandlerFactory in RecursiveNodeHandler, through the GetHandler function. Here is a step by step explanation on how we trigger the logic defined within plugins: The GetHandler function returns node handler based on the type of the node. Most of training job is defined via @task, which is of Task type in Flyte Here is the setup of the node handler for Task type. In Flyte, all Task is treated as a dynamic node and handle through dynamic node handler. However, we would still pass a task node handler into dynamic node handler In dynamic node handler’s Handle function, by default, we would make a call to handleParentNode, and in this function, we would make a call to TaskNodeHandler interface’s Handle function The logic of task node handler’s Handle function is pretty complex. First, it tries to find the Plugin based on task type; then if there is no cache hit on result, it would invoke plugin Within invokePlugin function, the core part is to invoke the Handle function ResolvePlugin search plugins through pluginsForType, where we initialized within the Setup function; the initialization is essentially sweeping the enabledPlugins, and we get it from WranglePluginsAndGenerateFinalList In WranglePluginsAndGenerateFinalList function, we get all plugins related to k8s through PluginRegistryIface interface; in task node handler, there is a data member pluginRegistry of this type, and the construction is here, where we call the PluginRegistry function from pluginMachinery module For each k8s plugin, they would be wrapped within a PluginEntry, which is further wrapped in an object called NewPluginManagerWithBackOff All k8s plugin would use the RegisterK8sPlugin function within the module pluginmachinery.register to register them into the system. For example, the Pytorch Plugin is registered here. However, all of these plugins actual do not provide a Handle function, which should be called by the node handler. What happened? Actually, the Handle function is implemented within PluginManager. Since PluginManager implement all interface defined in pluginCore.plugin, we could treat PluginManager as a plugin to invoke (this is a class Strategy design pattern, where PluginManager defines the main logic and expressed via several step functions. And we could use composition to fulfill these step functions with different implementation) Within the Handle function in PluginManager, we would check the current status, if the status is not started, then we would call launchResource, otherwise we would call getResource and checkResourcePhase to obtain new transition information In launchResource function, we would call BuildResource function which is defined in the plugin. This function is used to construct a kubeflow job. Then it make a create request via kubeClient to create this resource In checkResourcePhase, we would call GetTaskPhase to get the current status of the job Here is the point where Flyte is leveraging kubeflow and k8s to request resource and start the training job; both kubeflow and k8s would be huge topics, and I plan to discuss more in details in separate blog Here, we reach the end of our journey and the remaining job is delegated to k8s. What a complex flow!SummaryIn this post, we focus our discussion on how Flyte would invoke the distributed training job which is defined through plugin, we could see some common practice that is adopted in the design, such as utilization of queue and multithreading for scalability; separation of workflow executor and node executor for single responsibility principle; factory design for extensibility, etc.In next topic, we would focus on the storage used in Flyte, which is also another critical component, as we need to store the status of the workflow, node and even intermediate result; as well as leveraging caching to speed up the execution by avoiding duplicated computation. Once we have a better understanding on the storage part, we could start to evaluate the availability, scalability and persistence of Flyte." }, { "title": "LLM Training 101", "url": "/LLM-LLM-Training-101/", "categories": "Distributed Training", "tags": "llm, llm training", "date": "2024-09-01 00:00:00 -0700", "snippet": "这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家...", "content": "这个是读完这篇综述 Efficient Training of Large Language Models on Distributed Infrastructures - A Survey 之后的一个产出，这篇综述文章针对 LLM 的 training 介绍的已经很详细了，但是同时内容过多也不可能全都学完。这里针对自己整理的一些笔记来列一个之后学习的提纲，这个提纲肯定是非常主观的，推荐大家去读读原文来根据自己的情况针对性的准备 PS: 后续会不定期的更新这篇 blog 来争取与时俱进，同时会有专栏来介绍这篇 blog 里面打算深入研究的项目概念性知识 LLM 训练的一些特点 模型架构的一致性，基本都是堆的 transformer, 虽然现在有一些不一样的尝试比如 Mamba 和 TTT, 但是主流的模型还是 transformer 训练的规模和时间也是空前绝后的 Specialized software, 比如 Megatron (这个听说过，去了解一下) LLM 训练的 pipeline 也发生了变化（这一点说的还蛮有道理，我在这个领域有比较多的经验，可以向这个 LLM 的方向研究一下看看有什么机会）。传统的机器学习都是针对某一个问题用对应的数据来训练（domain specific），但是现在 LLM 的主流是在大量的数据做自监督学习，然后再进行 fine-tuning, alignment 等 在 LLM 训练的各项因素之中，Communication overhead 是一个主要痛点 LLM 训练的 infrastructure 相关的内容 PCIe 由于 bandwidth 的问题导致其不是很合适 LLM 的训练，现在更多的是使用专用的链接比如 NVLink 等，同时能使用不同的网络连接拓扑结构来进行进一步的优化，比如 cube-mesh 或者 switch-based fullly-connected The Clos network architecture, commonly known as a Fat-Tree topology, is widely used in LLM training clusters. In a Closbased cluster, each server, equipped with one or more NICs, is organized into racks connected to leaf switches. These leaf switches link to spine switches, providing inter-rack connectivity and forming a pod. The pods are further interconnected with core switches, facilitating any-to-any communication across servers within the cluster. Parallel file systems such as Lustre, GPFS, and BeeGFS are frequently deployed on leading high performance computing systems to ensure efficient I/O, persistent storage, and scalable performance. 听说过 distributed file system, 但是这个 parallel file system 是啥 打算去学习了解的框架和技术 RDMA: 可以去学习了解一下 InfiniBand DeepSpeed-Chat, parallel strategy https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat uses Hybrid Engine to seamlessly switch model partitioning between training and inference, such as using tensor parallelism to improve throughput during inference and using ZeRO or LoRA to improve memory utilization during training, providing outstanding system efficiency for RLHF training HuggingFace TRL, parallel strategy https://huggingface.co/docs/trl/en/index make full use of various parameter-efficient fine-tuning (PEFT) methods, such as LoRA or QLoRA, to save memory cost, and use a dedicated kernel designed by unsloth to increase the training speed of RLHF. FlashAttention, 内存优化 https://github.com/Dao-AILab/flash-attention an IO-aware tiling algorithm is proposed to reduce the number of memory reads/writes between slow HBM and fast on-chip SRAM based on the online softmax. 看能不能自己实现一遍这个算法，网上应该有一些简化版的 kernel 教程，可以参考学习一下 Selective-checkpointing selectively discards the activations of memory-intensive attention modules. FlashAttention fuses the attention module into a single kernel, and also employs selective-checkpointing to reduce memory consumption. 这个看一下具体是怎么做的 FlashAttention 2: 内存优化, efficiently handles variable-length inputs by parallelizing the sequence length dimension inseparably 这个是怎么实现的，去学习一下代码 FlashAttention 3: 内存优化, An interleaved block-wise GEMM and softmax algorithm is redesigned based on FlashAttention-2 to hide the non-GEMM operations in softmax with the asynchronous WGMMA instructions for GEMM. Besides, by leveraging the asynchrony of the Tensor Cores and Tensor Memory Accelerator (TMA), overall computation is overlapped with data movement via a warp-specialized software pipelining scheme. Blockwise Parallel Transformer (BPT) further reduces the substantial memory requirements by extending the tiling algorithm in FlashAttention to fuse the feedforward network 需要学习了解一下 WGMMA, Tensor Cores, Tensor Memory Accelerator, Blockwise Parallel Transformer Triton, 用来写 kernel, 计算优化，听说现在很多公司内部在大量的使用这个写 Kernel, 可以学习一下 #kernel #CUDA https://github.com/triton-lang/triton ZERO, 通过 fully sharding 来进行内存优化, ZERO1, 2, 3 https://arxiv.org/pdf/1910.02054 ZeRO-3 employs per-parameter sharding to shard the full model and utilizes All-Gather and ReduceScatter for unsharding and sharding communication, respectively ZERO++ 感觉也算是 ZERO 家族的一员，但是是一种 partial sharding 的办法，在 ZERO3 的基础之上, further introduces a secondary shard of parameters within subgroups of GPUs and uses quantization to compress parameters and gradients, effectively diminishing communication volume with a trade-off in accuracy ZeRO-Offload concentrates on multi-GPU training. It holds model parameters on GPU, and stores optimizer states and gradients on CPU memory. In addition, it offloads optimizer update computation to the CPU. Ring AllReduce 算法: https://github.com/baidu-research/baidu-allreduce Horovod: replaced the Baidu ring-AllReduce implementation with NCCL and designed a user-friendly interface for distributed training Pytorch DPP: fuse multiple sequential AllReduce communication operations into a single operation. This method avoids transmitting a large number of small tensors over the network by waiting for a short period of time and then combining multiple gradients into one AllReduce operation during the backward phase. 通信优化的一种办法，可以看看代码学习一下 FSDP: https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html GPipe 是之前听说过的一种方法，貌似是目前比较流行的方法，但是仍然会在开始和结束的时候有大量的 bubble 出现 https://github.com/kakaobrain/torchgpipe 一些比较主流的和重要的概念Parallelism Strategy Tensor parallelism: partitions the parameter tensors of each layer along multiple dimensions, effectively distributing the model parameters across the available GPUs. 感觉 tensor parallelism 没有 data/model parallelism 那么常见，在工作中没怎么看到用这种方法的 it is challenging to overlap the communication with computation, necessitating the use of high-bandwidth connections. Consequently, tensor parallelism is more commonly employed in a single GPU node. Pipeline parallelism: pipeline parallelism only necessitates the exchange of intermediate tensors at designated cutting points, resulting in less frequent communication requirements, pipeline parallelism 算是比较常用的东西了 但是 pipeline parallelism 也有两个问题，一个是 pipeline bubble, 一个是 memory consumption imbalance Sequence parallelism: It divides the input data into multiple chunks along the sequence dimension and each chunk is fed to one GPU for computation. 没怎么听说过这种方法，可以找来一些 code 来学习一下 MQA 和 GQA 就是属于这个范畴, 可以好好的学习一下 Ring Self-Attention leverages sequence parallelism and calculates the self-attention with ring-style communication to scale up the context window of LLM training. It first transmits the key tensors among GPUs to calculate the attention scores in a circular fashion, and then calculates the self-attention output based on the attention scores and value tensors transmitted in a similar fashion MoE parallelism: MoE 的结构在目前主流的 LLM 里面都得到了大量的使用，可以看看下面的这几篇文章里面介绍的针对 MOE 的 parallel strategy 的方法 #MOE GShard: extends the idea of MoE to Transformers in distributed settings, where experts are distributed across different workers and collaborates with All-to-All communication DeepSpeed-MOE: proposes a new distributed MoE architecture that applies shared experts in each worker and places more experts in deeper layers to balance communication costs with training accuracy Since General Matrix Multiplications (GeMMs) require the size of all experts’ inputs to be consistent, existing MoE training frameworks often perform token dropping and padding to match the same expert capacity, which wastes computation. General Matrix Multiplications (GeMMs) 的工作原理可以参考: https://spatial-lang.org/gemm Token dropping and padding 的常用方法是什么？有没有具体的实现代码样例 针对 MOE parallel strategy 中 communication 的优化 Tutel: divides the input tensors into groups along the expert capacity dimension and overlaps computation and communication among different groups to hide All-to-All overhead Tutel: optimizes the All-to-All kernel implementation by aggregating small messages into a single large chunk inside the nodes before exchanging data among different nodes #Batching Lina analyzes the All-to-All overhead of MoE during distributed training and inference systematically and finds that All-to-All latency is prolonged when it overlaps with AllReduce operations. Lina proposes prioritizing All-to-All over AllReduce to improve its bandwidth and reduce its blocking period in distributed training 很有意思的发现，可以去学习一下原文里面是怎么发现这个问题的，然后应用在自己以后的工作中 This heterogeneity is also reflected in model architectures, particularly with Reinforcement Learning from Human Feedback (RLHF). Utilizing heterogeneous hardware and diverse model architectures has become essential for the efficient training of LLMs 再重新学习一下 RLHF，来理解这里面提到的 异构性 的特点 Memory Optimization Rabe 这篇论文中证明了自注意力只需要 O(logn) 的内存就可以了，学习一下这篇论文里面的工作 了解一下 FP16 和 BF16 的工作原理，内存优化 LLM training 的过程中主要吃内存的部分 Model States: Model states encompass the memory consumed by the optimizer states, gradients, and model parameters Activations refer to the tensors generated during the forward pass Temporary Buffers: Temporary buffers are used to store intermediate results Memory Fragmentation: Memory fragmentation can lead to scenarios where memory requests fail despite having a large amount of available memory, 这个在 Pytorch 里面由于内存分配机制会出现这种问题，可以再找一些额外的资料详细的了解一下 Deep learning frameworks typically use a caching allocator with a memory pool to enable fast memory allocation and deallocation without requiring device synchronization. 一个用来估算所需要的内存的简易办法 When training a model with Φ parameters,4Φ bytes are needed to store parameters and gradients. The 32-bit copies of the parameters, momentum, and variance each require 4Φ bytes, totaling12Φ bytes. Therefore, the overall memory requirement for storing model states is 16Φ bytes，这个再好好看一下理解一下 一些用来进行 Memory 优化的整体大方向 Activation re-computation strategies, which trade increased computation for reduced memory usage, 这个是现在最主流的方法之一，可以找一些代码来看看是如何实现的，这个方法的一个关键就是节省的内存和额外计算之间的 trade off Redundancy reduction methods that minimize data duplication across training processes Defragmentation techniques that optimize memory allocation and deallocation to reduce fragmentation and improve memory utilization GMLake and PyTorch expandable segments propose to mitigate fragmentation by utilizing the virtual memory management (VMM) functions of the low-level CUDA driver application programming interface. 可以看看 PyTorch 里面这个工作 Swap and offload approaches that leverage CPU memory and NVMe SSDs to supplement GPU memory CPU offloading: static/dynamic SSD offloading, 这个在之前的 GPU training paper 里面好像看到过 Communication Optimization一些和通信相关的优化 NVIDIA’s NCCL and AMD’s RCCL are highly optimized libraries that typically outperform MPI-based collective communication libraries on their respective AI accelerators. These libraries usually select pre-defined algorithms to perform collectives based on conditions such as network topology and input tensor size. 可以去学习一下 NCCL 通信的不同算法: Ring, Tree, Hybrid Conventional frameworks simultaneously perform gradient computation for both weights and outputs. Out-of-order backpropagation (ooo-backprop) decouples the gradient computations for weights and outputs, scheduling the weight gradient computations flexibly out of their original order. This allows more critical computations to be prioritized and scheduled accordingly. Consequently, ooo-backprop optimizes overall performance by scheduling communications based on this out-of-order computation strategy. 这个工作很有意思，把 activation 和 gradient 的 communication 拆开然后进行类似不同的 priority 的 communication In-network aggregation (INA) uses the computational capabilities of network devices to perform aggregation operations like summing gradients of deep learning models.Fault ToleranceFailure tolerance 主流的还是使用 checkpoint Synchronous checkpoint Check-N-Run decouples the snapshot and persist phases. It achieves atomic checkpointing by stalling training only during the snapshot phase and asynchronously persisting snapshots using dedicated background CPU processes. DeepFreeze applies both lightweight (snapshot) and heavy(persist) persistence strategies in the background, sharding checkpoints across data-parallel GPUs to distribute I/O workload. Gemini proposes checkpointing to CPU memory for faster failure recovery, along with a checkpoint placement strategy to minimize checkpoint loss and a traffic scheduling algorithm to reduce interference with training. Tectonic: Meta’s distributed filesystem, enables thousands of GPUs to save and load model checkpoints simultaneously, providing efficient and scalable storage solutions for extensive training operations 现在貌似主要用来对 checkpoint 用来存储的都是 object store, 这个可以去研究下看看各个公司都用啥（比如 AWS 是不是都上 S3） Live migration leverages the inherent redundancy present in distributed LLM training setups, particularly the model replicas across different data parallel pipelines, to restore model states in case of failure. 这个感觉其实有点类似使用 Cassandra 里 consistency hashing 里面的 hinted hand offIf you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" }, { "title": "读书笔记 - Patterns of Distributed System", "url": "/Book-Pattern-of-Distributed-System/", "categories": "Distributed System", "tags": "system design, reading, 读书笔记", "date": "2024-07-05 00:00:00 -0700", "snippet": "最近读了一本和 distributed system 相关的书籍，介绍了在 distributed system 里面常用的一些 pattern. 这是一篇简要的读书笔记，把书中提到的几个 pattern 总结了下来; 我计划会经常更新这篇 blog, 把我新学习到的或者总结出来的一些 pattern 记录在这里; 希望能起到一个引导性的作用，给大家提供一个提纲挈领的思路PatternsWr...", "content": "最近读了一本和 distributed system 相关的书籍，介绍了在 distributed system 里面常用的一些 pattern. 这是一篇简要的读书笔记，把书中提到的几个 pattern 总结了下来; 我计划会经常更新这篇 blog, 把我新学习到的或者总结出来的一些 pattern 记录在这里; 希望能起到一个引导性的作用，给大家提供一个提纲挈领的思路PatternsWrite Ahead Log把命令存储到一个 append only file 里面去，当挂了之后可以重新读 WAL 来 rebuild 内部的 state #Message-Queue #KV-Store #持久化 Flushing 来保证命令真的写到 physical media，好处是 persistent，代价就是 performance; 可以使用 batching 等方法来进行优化 #Batching CRC record 来防止 corrupted entry #CRC Log 里面可能有 duplication，每一个 request 需要一个 unique identifier 来进行区分 #Deduplication 可以用来实现 transaction，用来保证原子性 #Transaction 工业界里面的具体例子 #RocksDB #Kafka #Cassandra Key/Value pairs that needs atomic store, write into a batch, and then batch is add into data store; the data store first create a WAL entry for the entire batch, once log is created successfully, the batch is added into datastore Segmented Log把单一的 log file 切分成更多的 log 从而方便对老的数据进行 cleanup; 当数据超过一定的阈值之后就 rollover 到一个新的 log file 里面去, 业界的例子 #Kafka #Cassandra #RaftLow Water Mark帮助保证 log 的大小不会无限制的增长，通过 low water mark 这样的一个 index，对 log 进行压缩 (通常是一个 background job 在进行这个操作) Snapshot-based #Raft Time-based #KafkaLeader and Follower使用单一的 server 来 coordinate 多个 servers 的 replication #Replication Small cluster: leader election, #Zab #Raft Large cluster: consistent core, 需要的几个核心功能 #Zookeeper #etcd compareAndSwap to set a key atomically heartBeat to expire the key if no heartBeat from leader, and trigger new election notification mechanism to notify all servers if key is expired Heartbeat 可以使用 separated thread 来异步发送 heartbeats #Consul 在 large cluster 里面，1-to-1 的 heartbeat messaging 效率太低了，这个时候一般可以考虑使用 Gossip Protocol #Gossip-Protocol 两种主流的实现方式，Phi Accrual failure detector 和 SWIM #Cassandra #Consul Majoruty QuorumFlexible quorum, 我们可以通过动态的调整读写的 quorum size 来提高性能，只要能保证读写之间会有一个交集就行; 比如说一共有 5 个 node，然后我们有 90% 的读和 10% 的写，那么我们可以要求读只需要 2 个 quorum, 写需要 4 个 quorum #Quorum #CassandraGeneration Clock也可以叫做 Term, Epoch, 这个是 Lamport Clock 的一个具体样例 #Lamport-Clock Each process maintains an integer counter, which is incremented after every action the process performs. Each process also sends this integer to other processes along with the messages processes exchange. The process receiving the message sets its integer counter by choosing the maximum between its own counter and the integer value of the message. This way, any process can figure out which action happened before the other by comparing the associated integers. The comparison is possible for actions across multiple processes as well, if the messages were exchanged between the processes. Actions which can be compared this way are said to be causally related. 工业界的例子, Cassandra 里面的 server 在 restart 的时候会自增 1, 这样在 gossip 的 message 里面其他的 server 会知道这个 server restart 了，从而会把关于这个 server 的 stale 的 data drop 掉，然后要新的; Kafka 里面的 epoch number 会存在 Zookeeper 里面，每次一个新的 controller 被 elect 的时候，就会增加这个 epoch number; 同时 leader 也会 maintain 一个 Leader Epoch 来看是否有 follower 太落后了 #Cassandra #Kafka High Water Mark也被称作是 CommitIndex #Replication #Raft #Kafka Client 最多只能读到这里，因为在 high water mark 之后的 entry 都还没有被 confirm 已经 replicate 了 这个在 stream 里面处理 delayed event 时候也叫这个，只不过那个 high water mark 是多等一段时间Paxos这个太难了，等以后专门开一个总结一下吧 #Paxos #Consensus-Algorithm #2PC #Quorum #Lamport-Clock We can ensure liveness or safety, but not both. Paxos ensure safety first 工业界的具体应用: Google Spanner 使用的是 multi-paxos, which is implemented as a replicated log; Cassandra uses basic Paxos to implement lightweight transactions #Spanner #CassandraReplication Log 在 MongoDB 中，每一个 partition 会有一个自己的 replication log #MongoDB #Partition 在 Kafka 的 Raft 实现中，使用的是 pull 模式，也就是 follower 从 leader 那里 pull replication log #Kafka #Push-Pull Read request optimization via bypassing the replication log, 可以使用两种不同的方法, 一个是 leader 再发送一个 heartbeat 然后看能不能得到 majority 的回复，来确认自己仍然是 leader; 另一个是使用 leader lease #Read-Optimization #Lease #etcdIdempotent Receiverclient 可能会 retry request, server 端需要进行 deduplication, 这个在多种系统中都很常见 #Event-Aggregation #Payment 给每个 client 一个 unique id, 在 server 端进行注册，注册之后 client 才能开始给 server 发送 request; 这个数据也需要被 replicated 从而保证高可用性 Expiration of saved request, request number, next request only when received response, number of max in-flight request with request pipeline #KafkaSingular Update Queue一种用来高效处理 concurrent request 的方法，向比较使用 lock 的话效率更高；具体的实现方法就是实现一个 work queue, concurrent 的 request 都放到 queue 里面，但是只有一个 worker thread 来处理 queue，从而实现 one-at-a-time 的保证 #Concurrency #Message-Queue #Coordination 工业界的例子有 Zookeeper, etcd, Cassandra 可能会用到这个思想的 system design: Booking, Google doc (OT)Request Waiting List一个 node 可能要和其他的 node 进行异步的 communication 之后才能返回 request, 保存一个 waiting list 来 map 一个 key 和一个 callback function #Concurrency #异步 工业界例子: Kafka 里面的 purgatory 来保存 pending request #KafkaFollower Reads也就是大名鼎鼎的 read replica; 即使是在用 Raft 这种 consensus 算法来进行 replication 的系统中也会有 replication lag, 因为 leader 需要一个 additional 的 network call 来让所有的 follower 都 commit; read your own write，可以使用 lampart lock 来解决，写了之后传回去一个 version number，再读的时候要带着这个 version number 来看 read replica 上面的 value 是不是已经是更新的了Version NumberTo store versioned key values, a data structure that allows quick navigation to the nearest matching version is used, such as a skip list, 之前在 lucene 里面也看到了这个 skip list，需要研究一下 #数据结构在 RocksDB 里面，一个重要的原因需要把 key sorted 的是因为它们 underlaying 存储的都是 bytes array, its important to keep keys sorted when they are serialized into byte arraysVersion Vector在 Cassandra 里面，除了 value 以外，还把 timestamp 也当做一个 column 来存储了，从而实现了 LWW，但是代价就是 Cassandra 的 cluster 需要正确的设置 NTP, 否则的话 latest value 仍然可能被 old value 给 overwrite 掉 如果每一个 cluster client 有一个 unique id 的话，那么我们也可以使用 client id 来存 version vector (但是这样的话怎么进行 conflict resolve 呢) 一篇 Riak 里面讲针对使用 client id 还是使用 server id 来存储 version vector 的文章Fixed Partition先 create logic shard，然后再把 logic shard map 到 physical shard 上面去; 这些 metadata 都可以通过一个 coordination service 来负责 (分 partition 和 存储相应的 metadata); 另外一种做法是每个 physical node 上面的 partition 数量是固定的，也就是 propositional to number of nodes Kafka 里面的每一个 topic 就是一个 fixed size partitionsClock Bound WaitWhile reading or writing, cluster node wait until the clock values on every node in the cluster are guaranteed to be above the timestamp assigned to the value Google TrueTime, AWS Time Sync Service, 使用 atomic clock 和 GPS 来确保 clock drift across their cluster node is kept below a few milliseconds 这个概念有点复杂，需要再找一个好的资料学习理解一下这里的思想LeaseUse time-bound lease for cluster nodes to coordinate their activities, 这个在 GFS 里面就使用到了, 在 Facebook 的 Memcache 里面也有涉及到 Lease 的思想, Lease 一般可以通过一个 coordination core 来实现，由 leader 来进行 lease 的 replication 和 checkState Watch可以参考一下是怎么实现的, 在 server 端我们需要存储下来 event 和 client 的 connection, 在 client 端我们要存储 event 和对应的 handlerEmergent Leader直接用在整个 cluster 里面最老的那个 node 作为 coordinator node, 相比较 consistency core 所采用的 leader election 的方法， favor availability over consistencySingle Socket Channel在 follower 和 leader 之间保持一个能够支持 retry 而且能够保证 message order 的通讯，可以通过 TCP 来实现; 在 Kafka 和 Zookeeper 里面使用了这种方式 #KafkaRequest Batch把多个 request 放在一起从而提高带宽的利用率; 在 client 端可以 maintain 一个 queue 来维护 request, 然后再放在一个 batch 里面一起发过去 (这个其实跟之前写的 batch commit logger 是一样的)Request Pipelineserver 在发出去 request 之后不需要等待 response, 又另外一个 thread 来负责接受和处理 response (有点像是 webhook 的思路); 为了防止 request overwhelming, 一般会有一个 upper bound on max in-flight request; 同时针对 retry 和 out-of-order 的 request 也需要针对性的处理 (比如 assign unique request id 等)Reference Patterns of distributed system" }, { "title": "那些年，我们追过的 Feature", "url": "/Features-in-Recommendation-System/", "categories": "Machine Learning", "tags": "machine learning design, feature", "date": "2024-06-24 00:00:00 -0700", "snippet": "在今天的 blog 里面，我将结合我前一阵子面试 machine learning engineering 的经验，跟大家唠唠在 ML design 里面的 feature engineering 相关的问题。在 ML design 里面，我们可能会被问到可以使用什么样的 feature, 以及具体一些 feature 可以被怎么处理以及怎么使用在模型之中。由于我之前主要做的是推荐和广告相关...", "content": "在今天的 blog 里面，我将结合我前一阵子面试 machine learning engineering 的经验，跟大家唠唠在 ML design 里面的 feature engineering 相关的问题。在 ML design 里面，我们可能会被问到可以使用什么样的 feature, 以及具体一些 feature 可以被怎么处理以及怎么使用在模型之中。由于我之前主要做的是推荐和广告相关的内容，所以我在这里将主要介绍一下在设计推荐系统的时候，围绕 feature 可以聊的一些点，来给大家提供一些思路，帮大家更好的准备面试Feature 的种类在推荐系统里面，我们经常要处理的场景是 给定一个用户和一个物品（以及一些可能的 context），预测用户会喜欢（或者其他的 action）这个物品的概率基于上面这个简单化的概括（我们在之后会对这个问题进行适当的展开），我们其实可以比较容易的归纳出 feature 的种类 User Side Feature Item Side Feature User-item Interaction Feature Context Feature除此之外，还有一些相对比较特殊的 feature, 我们之后会单独介绍User Side Feature这种类型的 feature 有时候也会被称作 request level feature, 针对推荐系统来说，一个 request 基本上就代表了一次用户请求，所以这么叫没什么毛病。在 user side feature 里面，有下面几种比较常见的形式 Demographic feature, 也就是常说的一些基本用户属性，比如用户的年龄，性别，职业，local 等等; 这些 feature 是最最常用的 feature 了，不过由于现在针对 Machine Learning fairness 查的严，很多这种 demographic feature 被禁止用于推荐了 User behavior feature, 也就是用户的一些行为特征，比如用户过去一个月买过的物品 id，用户过去一周 follow 别人的 account 的个数，用户的历史 CTR 等等 这里其实列举了三种不同的“数据类型”，也是在推荐系统中 feature 的常见形式: id feature (比如用户 like 过的 post id, 用户之前 follow 过的用户的 id 等等), count feature (比如 follow 的个数，click 的个数等等), ratio feature (比如 CTR, CVR 等等) 一般针对 user behavior feature, 我们都会有一个窗口来进行 aggregation (比如过去一周的 sum 或者 average; 在工程实现的时候，经常是多个窗口一起实现了，比如 7D, 14D 等，因为可以通过一个 pipeline 就全部搞出来，比较方便高效); 随着模型技术的进步，现在有很多尝试开始用 user sequential behavior modeling 来取代这种 feature engineering 的方法了, 比如 DIN 直接拿用户的 behavior 数据和 candidate item 做 attention 处理 User 的 behavior 多种多样，这部分需要针对具体的推荐系统所解决的 business problem 来进行处理，是最需要 domain knowledge 的部分; 比如在给用户推荐其他用户来 drive growth 的推荐系统中，你用用户过去点击过的 post id 可能效果就差一些，但是如果用点击过的 post 的 author id 可能就是一个不错的选择 User embedding, 或者叫 user profiling, 也是现在非常主流的一种 user feature, 当在面试中提到这部分的时候，基本上一个 follow up 是如何计算出来 user embedding, 以及如何在 inference 的时候进行 serving, 一般的 user embedding 的计算方法有这么几种: 直接用 item 本身，或者通过一些 heuristic rule 来表示: 比如说，我们可以把所有的 post 分类成 1024 个 category, 那么根据用户曾经看过的 post, 我们就可以用一个 1024 维度的向量来表示用户, 如果用户看过 GUNDAM 类型的 post, 那么 GUNDAM 类别在 1024 向量中的值就设置成 1, 否则就是 0; 这种方法其实非常类似 NLP 里面以前经常使用的 bag of words 的方法来做 sentence embedding; 这种方法的好处就是实现起来比较简单，但是缺点就在于捕捉到的信息比较粗糙，只能根据我们定义的 heuristic rule 来决定，不能根据用户跟物品的交互学习出一些 semantic 的信息（比如喜欢 GUNDAM 的用户可能对于游戏也感兴趣） 一个这对上面的方法提高的方法，是基于 item 的 embedding 去学习一个 user embedding, 这个方法在 Pinterest 的这篇论文中得到利用，他们基于 PinSage 出来的 item embedding, 结合用户的 sequence engagement 数据，利用一个模型来学习 user embedding 通过其他的一些方法来训练出 user embedding, 比如利用全体用户的 interest follow graph 作为数据，利用 collaborative filtering, dedicated model (比如 graph neural network) 来计算出 user embedding, 然后这个 embedding 可以被用作输入送进其他的模型; 比如 Facebook 就采取了用一个专门的模型来对大量的 user feature 进行学习，然后用这个 dedicated model 去生成 user embedding 然后给下游的模型使用，详情可以参考这篇论文; 这种方法的好处在于能够学习更具有表征能力的 user embedding, 但是缺点在于 user embedding 需要进行单独的额外维护，以及不能很好的基于 downstream 的 task 来进行微调; 将用户 id 作为一个 sparse feature 输入到模型里面，然后通过一个 embedding lookup table 来 convert 成一个 dense vector, 然后再和其他的 feature 输入 concat 在一起; 这个就是一个经典的如何在 Neural Network 里面使用 sparse feature 的方法，比如 LinkedIn 的这篇 blog; 这种方法的一个好处是在于 user embedding 和模型是一起训练的，所以 embedding 可以理解为针对某个问题的专门优化，那么代价也显而易见，一反面是模型训练的参数量上升，另一方面对于 user embedding 的 reuse 也不是那么的方便了 Item Side FeatureItem side feature 其实和 user side feature 本质上是类似的，比如我们也可以有一些 item 的属性信息，比如物品, 产地，物品价格等等，或者 item 的一些历史交互数据，比如过去一周的 impression 数量，过于一周的 CTR 等等；同时我们也可以使用和 user embedding 类似的方法来生成 item embedding 然后用作模型输入; Item side feature 的一个特点是 feature value 变化不是很频繁，因此这部分的 feature 一般都会采用一些类似 pre-compute 的方法来提高性能 一种常见的方法是使用双塔模型来对 user 和 item embedding 进行训练，然后 user tower 可以在线的去 server request 来实时的计算 user embedding, 但是 item tower 可以离线的把所有的 candidate 的 embedding 计算好然后缓存起来，等需要的时候直接读 pre-compute 好的而不用再实时的计算了 相比较 user feature, item 可能也会有一些其他形式的数据可以利用，比如物品的图片，广告语等等，这些数据都可以经过专门的处理然后用作模型输入，比如利用 pre-train model 来把 image 转换成 embedding; 或者利用 object detection 模型来 predict 图片中包含物品的种类信息; 利用 BERT 来做 sentence embedding 等等Item side feature 比较特殊的一种 feature，是可以利用一些 Owner 的信息，而不单单是物品本身的信息: 比如说我们要推荐广告，那么我们可以拿 campaign level 或者 advertiser level 的信息(这些信息相比较 ads id 本身可能会更加的 stable, 这个是 ads 里面的一个问题，因为 ads 本身的 TTL 都相对比较短); 如果我们推荐 post, 那么我们可以拿 post owner 的一些信息，比如 post owner 过去一周看过的 post id 等，作为 additional signal 来做推荐User-item Interaction FeatureUser-item interaction feature 就是具体到 user-item pair 的 signal。 在我们上面所描述的 feature 中，都是根据 user 或者 item 进行了 aggregation; user-item pair 可以理解为是 aggregation 之前的信息，能够给我们提供最 fine-granularity 的 signal， 比如，我们可以看 user_a 跟 item_a 过去 24 小时的 view count, time spent 等等; 或者 user_a 跟广告商 advertiser_a 过去 24 个小时的 impression 的数量, CTR 等等这一类的 feature 的 serving 是最困难的，原因就在于其 cardinality 非常的大（约等于 O(num of user) x O(num of item) 的数量级），因此我们很少在 early stage (这里其实涉及到了推荐系统里面的多层架构设计，可以理解为我们在层层做 filtering 从而来减少需要考虑的 candidate 的数量) 大量的使用这种类型的 featureContext Feature这种类型的 feature 相对于前三种讨论到的而言不是那么的常见，context feature 有两种不同的理解形式 一种是在 server 端我们能够拿到的一些 context feature，比如现在很多的推荐系统开始 adopt list ranking, 也就是考虑所推荐的物品彼此之间的影响，把彼此当做 context 来进行优化（彼此都是彼此的缺口 XD） 另外一种是在 server 端无法获取，只有在 device 上才能拿到的一些信息，比如用户当前的网络环境， 用户当前手机的电量等等，这些 context feature 也会对用户的行为产生影响，比如如果我的手机没什么电了，我可能会先收藏几个视频等之后再看 针对上面说的这两种情况，在快手的这篇论文里面都有提及，详情可以参考阅读一下原文其他 Feature 的碎碎念除了上面提到的 feature, 还有一些比较特殊的 feature 承担着一些特殊的使命 Privileged feature, 这种类型的 feature 是属于在 training 的时候 available 但是在 serving 的时候不 available 的，一个例子是在做 CVR 的预测的时候，用户在一个物品上面的逗留时间会是非常 powerful 的一个 signal, 但是我们在给用户展示物品的时候，是需要先预测 CVR 出来对物品排序，然后再给用户展示，所以在 serving 的时候这个逗留时间是不知道的，但是在 training 的时候我们可以根据 client 端的 logging 知道这个信息。在淘宝的这篇论文中，他们采用了 teacher-student knowledge distillation 的方法来学习这类 feature Position feature, 这个是在推荐系统中非常常考的一个 follow up. 在推荐系统中，存在这一种系统性的 bias, 也就是 position bias: 用户点击了某个物品，可能并不是因为我们的推荐做的有多么好，而是单纯的因为这个物品被排在了前面，导致有更多的 impression 引来更多的用户 action. 传统的解决办法是把物品的 position 当做一个 feature 放在模型中跟其他的 feature 一起训练学习，从而能够让模型学习出 position bias, 从而起到一定的 calibration 的作用; 在 Youtube 的 recommend next video to watch 的论文中，position feature 被单独的加到模型的一个 tower 里面来进行专门的学习，从而提高模型针对 position bias 的解决能力常见的 Feature 处理办法上面的章节我们主要说了一些不同类型的 feature, 在介绍的时候我们也稍微提及了一下 feature 的不同的数据类型，这里我们再稍微复盘一下 numeric feature, 我们也俗称 dense feature; 这里面也细分成两类，一类就是最普通的 float number, 比如 ratio/count 等等；另一类是 categorical feature, 比如性别，国别等等。这两类 feature 最主要的一个区别在于他们的 scale 是否具有意义 id feature, 我们也俗称 spares feature; 主要就是一些 entity id, 比如 post id, ads id 等等；具体的表示情况也有两种，一种就是单独的一个 id list, 另外一种则是给不同的 id 一个 weight, 比如说我们可以根据用户跟这个 entity 交互的时间来做一个 weight decay, 从而实现一种简单的 recency 性质的 sparse feature, 让更近的 id 有更高的 weight, 一般我们管这个叫做 id score list; 所有的这种 id feature 都需要通过一个 lookup table 来转化成 dense vector embedding feature, 一般就是其他模型输出的一些 dense vector; 一般整体直接使用，很少有只使用其中的某一些维度的情况在这几种类型的 feature 当中, id feature 和 embedding feature 的处理办法一般都比较的统一，比如使用 lookup table 来转化 spares feature 到 dense format, 或者使用一些 clipping 来防止 embedding 里面的某些值过大; 针对 numeric feature 的处理比较常见，我们下面就主要聊一聊这些Normalization这个是针对 numeric feature 来说最常见的一种处理方式了，主要的目的就是让所有的 numeric feature value 的 scale 尽可能是统一的, 比如 ratio feature 的 scale 一般都是 0 ~ 1, 但是房价这个 feature 的 value 就可能 scale 在 0 ~ 10M, 这样会让 ratio feature 的 weight 跟房价 feature 的 weight 不在同一个数量级上，从而导致了 “vanish” 情况的发生。针对这种情况，一般我们可以采用 min-max scale 的方式来 normlize另外的一种情况，是 feature 本身的 distribution 是 highly skew 的，在 ML 里面，我们都是尽可能希望 feature 的分布尽可能的接近正态分布（为什么呢）。针对这种 data skewness 的情况，我们可以采用一些 transformation 比如 log-transform 或者 cox-box transform 来进行处理，从而让 value 尽可能符合正态分布One-hot/multi-hot Encoding这种处理办法是 categorical feature 的一种常见处理办法。在这种方法中，针对 categorical feature，我们会把它展开成一个 sparse vector，里面只有一个或者几个 element 是 1, 其他的位置上全都是 0。One-hot 和 multi-hot 的主要区别在于在这个 sparse vector 里面能有几个位置上有 1，one-hot 只有 1 个，而 multi-hot 可以有多个Feature transformation除了上面的处理办法，针对 feature 还有一些额外的变换，从而增加模型能够学习的 signal 的数量 在 Youtube 的这篇论文中，他们把一些 feature 用上了 sqrt, pow 等变换，作为新的 feature 和原本的 feature 一起送入模型进行学习 我们还可以把不同的 feature 彼此之间做 bi-gram，从而显形的增加 feature interaction (2nd order interaction, 这个也是之前很多研究的一个重点，因此退出了诸如 DCN 等模型架构，不过在这些模型里面， feature interaction 是通过模型表征，而不是在输入上面做文章) 另外一种比较常见的做 feature transformation 的办法是给原本的 feature 添加 breakdown, 比如说我们算一类 item 的 CTR 数据，那么可以根据用户的性别添加一个 breakdown (说个题外话，这种 breakdown 现在也被用于处理用户数据上，从而实现 privacy preserve); 更加放飞自我的一种办法，我们可以先 train 一个 gbdt, 然后把 gbdt 的 leave node 作为一个 feature 输入模型（可以考虑作为一个 binary sparse vector，或者带着 leave node 上的 weight）; 在 Facebook 的早期 ads 模型里面，大量的使用了这种方法，详情参考这篇论文 We found that boosted decision trees are a powerful and very convenient way to implement non-linear and tuple transformations of the kind we just described. 针对这一点，我在和 reddit 的 ads ranking 面试的时候，跟 hiring manager 有一个比较激烈的讨论（可能也是因为这样她把我拒了把吧）; HM 一开始没有很理解使用 gbdt 来做 feature transformation 的思路，认为这不可行；在被我 convenience 这样做可行之后，又抛出来认为这样做没必要，因为可以调整模型架构来直接学习 tree 本身所 capture 到的一些复杂的 feature interaction; 针对这一点我是同意的，而且我也有幸参加到了 ads ranking 的 gbdt deprecation 的工作之中; 不够我们当时之所以 deprecate gbdt 更多的是从 maintenance cost 和 simplify tech stack 的角度，gbdt 本身仍然能够带来很不错的 model gain, 这也是为什么 deprecation 整了很长时间也没有完全取得胜利 Missing/Sentinel Value如何处理 feature 的 missing value 或者 sentinel value 也是作为 machine learning engineer 我们经常要实际面对的问题。针对 missing value，大部分的时候我们会直接用 0 来取代，但是如果 0 也是一个合法值的话，会给模型造成困扰; 这个使用可以考虑使用 sentinel value 来表示 value 是否 missing, 把这个 sentinel value 做成一个新的 categorial feature 用在模型里面; 这样一来，原本的 feature 和这个新的 categorical feature 结合着使用，模型就能区分开来 feature value 是真的等于 0 还是单纯的 missingFeature Optimization这一部分一般属于 bouns point, 在实际的面试中，很少有机会会真的讨论到这里，我之前在 Facebook 的时候带领很多这方面的项目，因此也借着这个机会给大家简单的介绍一下Feature Importance当我们有大量的 feature 之后，一个头痛的问题就是我们要选用哪些 feature 进入到模型里面; 由于 infra 的一些 limitation, 把所有的 feature 都放入到模型中开销过于巨大，无法支持，因此一般我们会选择一个 subset feature 来放到 production 模型中，而选取 feature 的一个重要指标，就是 feature importance 这里插入一些题外话，除了由于 infra limitation 导致我们不能使用所有的 feature 以外，另外一个考虑的因素是 feature coorelation，我们通过 empirical 的研究发现，把 correlated feature 放入到模型里面不会给模型带来任何效果上的提升，而且反而可能会降低模型的效果；不过 correlated feature 倒是有助于提高模型的 robustness，所以这也是一个实际中我们需要 make 的 trade off; 在 Facebook ads 我们是尽可能 minimize feature correlation，从而把尽可能多的有 additional gain 的 signal 放入到模型中Feature importance 可以从算法层面和工程层面两个角度讨论，工程层面的话主要就是涉及到各种 feature metadata 的管理，如何保证 feature candidate pool 是正确的（在复杂的系统中，feature serving 也是一个被高度优化的部分，导致有些 feature 只有在特定的环境下才可用），如何保证 feature 有足够的 coverage 等等；从算法层面，可以用来计算 feature importance 的方法有: shuffling algorithm, SHAP, integrated gradient 以及 binary stochastic neuron 等，这里就不再展开讨论了当通过上面提到的方法得到 feature importance score 之后，我们就可以使用 top k 的方法来选取 feature，除了这种简单的 selection strategy 之外，我们还可以引入其他的一些 metrics，比如 feature serving cost, feature storage cost 等等来进行 joint optimizationHash Size Tuning我们在前讨论 id feature 的时候，大量的提到了 embedding lookup table 这样一个重要的 component. 它有两个很重要的 hyper parameter，embedding dim 和 cardinality, 可以理解为 embedding lookup table 的的行数和列数. 由于 id space 巨大，如果给每一个 id 都提供一个单独的 embedding 的话，模型的 paraemter 会过于巨大，因此我们通常采用 hash trick, 也就是通过一个 hash function 把原本的 id 映射到另外一个空间上面，然后这个空间的 cardiality 就是我们 embedding lookup table 的列数，一般我们也把这个 cardinality 称作 hash size. 如果 hash size 设置的太大，那么会浪费空间，如果太小，又会导致太多的 collision 从而影响性能（尤其是对 tail id 来说），因此如何找到一个比较好的 hash size 也是一个玄学 这里再插入一个题外话，目前主流的一些 hash function 基本上是 semantic-less 的，也就是说很有可能一些非常 popular 的 id, 比如大V们的 post 和一些 tail id 比如我的 post 是被 hash 到同一个 bucket 里面去了，这会导致这个 embedding 会被这些大V们的 post overwhlem，而不会有太多的我的 post 的 representation. 这其实是一个 known issue, 业界也有一些尝试用更复杂的 hash function, 比如基于 culster 的 hash 来处理有两种 hash size 的方法可以使用，一种是在 traning dataset 上 sample 一部分数据之后做分析，看不同的 hash size 情况下 collision 的情况，然后动态的调整（比如使用二分搜索）使得 hash size 能够满足一定的 collision rate 的阈值需要；另外一种方法是先给一个很大的 hash size，然后我们在模型训练的时候同时保存一个 counter 来记录各个 column 的 hit rate, 然后再模型训练完毕之后根据这个 hit rate 来做一个 post-processing 针对 embedding dimension, 目前主流的做法就是用一个 single dimension, 但是有一些研究尝试用 variable dimension, Google 曾经有一篇 paper 但是我现在找不到了，之后找到了再 update 过来 If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeAcknowledgemment感谢 Yuan 大佬，Bing 姐和 Yuzhong 大佬提供的修改和补充意见，让这篇 blog 更加的完善Reference Deep Interest Network for Click-Through Rate Prediction Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta Enhancing homepage feed relevance by harnessing the power of large corpus sparse ID embeddings User Action Sequence Modeling for Pinterest Ads Engagement Modeling Evolution of Ads Conversion Optimization Models at Pinterest Real-time Short Video Recommendation on Mobile Devices Privileged Features Distillation at Taobao Recommendations Practical Lessons from Predicting Clicks on Ads at Facebook How we use AutoML, Multi-task learning and Multi-tower models for Pinterest Ads PinnerFormer: Sequence Modeling for User Representation at Pinterest" }, { "title": "How to Design Auction System", "url": "/How-to-design-auction-system/", "categories": "Distributed System", "tags": "system design, auction, realtime system", "date": "2024-04-06 00:00:00 -0700", "snippet": "In this post, let’s discuss a little bit how to design an auction system similar to the one on eBay, where owner could list their items in the system and others could place a bid on it. User with h...", "content": "In this post, let’s discuss a little bit how to design an auction system similar to the one on eBay, where owner could list their items in the system and others could place a bid on it. User with highest bid would be the winner of this auction and could buy it.In a real world auction system, there are lots of components involved, such as the search (user could search active auction based on their interest), payment (winner need to make the payment) and inventory (owner could add new items). We would not dive deep into these components, but would only focus on the auction service itself. For search and payment, I plan to have other posts to discuss them in depth.In this post, we would discuss 2 different ways to design the auction system, stateful and stateless, and see what would be their pros and cons. In reality, stateless is more common, while stateful design still play a critical role in different use cases, e.g. stream processing.Functional RequirementWe would assume the following functional requirement to be offered by our system User could start an auction User could view the active auction, and place a bid in the auction; user could also get realtime update on the current highest bid Auction is closed when there is no higher bid for 1 hour Winner of the auction would receive notification and has 10 minutes to make the paymentNon Functional Requirements High availability High scalability Low latency Eventual consistency is acceptable for live bidding part (we could discuss for higher consistency level), but when determine the winner of the auction, it needs strong consistency 1B DAU, 100k auctions per-day, on average 10% of user place 1 bid per day, assume 10:1 read:write ratioSome questions to clarify What if there are multiple bids with the same price, who would be the winner? The first bidder would be the winner Do we allow a bidder to place multiple bids within the same auction? No, each bidder could only place 1 bid, but they could increase their bid if their original one is not winner Do we need to record all bids that user placed during the auction? This is great question, let’s keep all bid that users have placed instead of just winners What shall we do if there is no bid for certain auction, do we need user to provide a TTL? Let’s simplify the problem as of now and assume there is no TTL required High level designNewton has said that If I have been able to see further, it was only because I stood on the shoulders of giantsIn this design, we would also stand on the shoulders of giants, which is live comment and cron job scheduler.Auction creationThis part is relative simple. We have Auction Service to handle the creation HTTP request from user. The auction service would write a new entry to the auction_table within Auction DB and update to cache. Below is an example schema of our auction table. Besides the regular metadata such as owner_id, item_id and created_at, there are 2 important fields, status and expire_at, which is critical for us to manage the transition of auction and handle the payment.When we create a new auction, we would also update it into the cache and mark it as a ACTIVE auction. This design choice actually makes our auction service stateless: it does not need to maintain any data on the server regarding the auction. If it needs to know the status of an auction, it would query the cache and then do the necessary processing. The cache is primarily used to help us improve the read performance regarding the highest bid for a given auction. If DB write or cache update fails, we would return failure to client and client would retry the creation.There might be issue that the status in cache and in Auction DB are inconsistent, we would dive deeper into this topic in Cache and Auction DB consistency section.Auction Bid Place and UpdateFor this part, there are 2 key problems we need to answer: the connection mechanism between client and our service the mechanism to route highest bid to users who are viewing the current auctionFor the first problem, we would use a combination of HTTP request and server sent event (SSE): to place a bid, we issue an HTTP request to Auction Service; while to receive highest bid from others, we leverage SSE connection with Bid Update Service. Other connection options are HTTP long polling and websocket. HTTP long polling is relative less efficient because client needs to repeatedly query the backend for new bids. Websocket is a little bit over killing in our scenario as we don’t expect each user viewing the auction actively place bids, thus a single direction connection is sufficient. However, websocket might also be applicable in some cases. A more detailed comparison between websocket and SSE is available in Websocket vs SSE.For the second problem, one naive approach is to write all bids into DB and let the Bid Update Service to poll the DB to see if there are new bids. This approach works if there is not much traffic, but is less efficient in our scale and would put too much pressure on DB (# of auction x 60 / # of granularity QPS from a single Bid Update Service). Here we would leverage a hierarchy fan-out mechanism to route the bids.When user first navigate to an auction page, we would retrieve the information about the auction through Auction Service via regular HTTP request. If the auction is still in ACTIVE status, user would build a SSE connection with one Bid Update Service (Load Balancer could randomly pick one). The Bid Update Service bus1 would update its in-memory subscription table to record that a user u1 is viewing auction a1. Also, this server would also make a request to Dispatcher specifying that itself is listening to a1 and Dispatcher would also update its in-memory subscription table.# bus1 subscription table{ 'a1': ['u1', 'u2'],}# dispatcher subscription table{ 'a1': ['bus1'], 'a2': ['bus2'],}When user make a bid, client would send a HTTP request to Auction Service, the node that handle the request would also make a request towards Dispatcher. The Dispatcher would check its internal subscription table to figure out which Bid Update Service (in this case bus1) needs this update. Once bus1 receives the request, it would also check its internal subscription table to figure out which connected user it needs to send this update.In the version we just described, Dispatcher is a stateful service because it needs to maintain the subscription table. If it is down, we won’t able to forward bid update anymore and thus making it highly available is critical to our system. The following options could be considered: Adopt write ahead log and snapshot to rebuild the state after failure Replicate the state to external storage (e.g. KV store) so that other nodes could pick it up Active standby node to be promoted to primary once original one failsAnother consideration here is that we might be able to remove dispatcher, and just use coordination service or a distributed kv store to maintain the subscription table. Bid Update Service would directly make update to coordination service, and Auction Service directly query it to figure out the Bid Update Service it needs to send update to.There are pros and cons of both approach Dispatcher pros: simplify Auction Service’s responsibility (SRP), could scale individually, handoff on retry cons: slightly more complex overall architecture Without Dispatcher pros: simpler architecture, less maintenance cost cons: Auction Service needs to handle forwarding and retry If we would like to achieve higher consistency, such as each update needs to be sent to all users that is within the same auction. We could enable ACK among the services. For example, if certain Bid Update Service does not reply ACK to Dispatcher, Dispatcher would retry the request. It is possible that on the client side we receive duplicated events, but it is pretty simple to dedup as we only need to keep the highest bid.It is still possible that certain bid update is lost during the transmission and it might not a big duel. The reason is that: During normal active auction, there would always new bids coming out, which overwrite the pervious one; so certain data loss on client side would not make a big issue. The only critical one is the miss of highest bid, which would be the last bid on the current auction. We could set a timer on the client side, and if it has been 10mins since we receive last update on bid, we could issue a hard pull to Auction Service to get the latest bid information.Having discussed about how bids are routed to other users, let’s take a look how we maintain the current highest bid. When user make a bid, one instance of Auction Service is going to handle the request. It first check if the auction exists in cache or not, and see if the status of the auction is still ACTIVE status. If there is a cache miss, it reads Auction DB to check the status of the auction (this could happen but should be some corner case). If auction is still ACTIVE, then Auction Service write the bid into the bid table in append pattern, which is great for write throughput. This choice would result in multiple bids for a single user given an auction, and we would use the latest one as user’s final bid (latest could be determined by insertion time, or we could have client side request id which would be more robust). Once DB write is done and if the new bid is higher than the current one in cache, we would also update the information in cache and Auction Service would also send request to Dispatcher to deliver this new update to all clients.It is possible that the DB write is failed or the cache update is failed. We would retry the request if is some transitional issue.In the cache, we would store the following metadataauction_id: (status, highest_bid, highest_bidder_id, updated_at, expire_at)status, highest_bid and highest_bidder_id is relative straightforward. updated_at is used to record the staleness of the cached entry, expire_at is used as timer to trigger the auction execution (see Auction Bid Execution). This state works because in our FR we assume that the same user could only modify his bidder to higher price instead of lower. If we allow user to bid lower, then we need to store all user’s bid or top 100 bid.Since we cache auction state by auction_id, we could suffer from hotspot issue. For example, Wing Gundam Zero is so popular that everyone tries to bid it and we have lots of concurrent update to the cache. Below are some options that we could consider To deal with high volume of concurrent write request, we could use lease to coordinate the update to avoid potential stale update. The downside is that the update might need to retry multiple times to succeed. If we choose quorum as our replication strategy, we could potentially set write commit to 1 to increase the write throughput and have customized conflict resolve (relative simple as larger-is-winner). This works because in our FR we assume that the same user could only place higher bid but not lower.Auction Bid ExecutionTo execute the winner’s bid after 1 hour, we have a Fulfillment Service. This service is similar to a cron job scheduler that it periodically scan the state we have in cache and see if there is any bid that needs to be executed by checking the status and expire_at. Once it identify one bid that needs to be executed, it would also send a request to Auction DB to double check if this is indeed the winner bid we need to execute: If not, it would make a write to cache to correct the information in cache. This is similar to read repair in quorum replication. If confirmed, then Fulfillment Service would update the status of the auction to be PAYMENT_PENDING in both DB and cache. The expired_at field in auction_table would be set based on the policy (e.g. 10mins in our case). The winner_id, winner_bid_id, winner_price would also be populated all together. And then send request to notification system to send a payment notification to the winner. This event update would also be sent via the Dispatcher to all live users in this auction.The actual payment would be handled by another dedicated system which we won’t discuss too much in details. But once the payment is done, the payment service would update the auction status to SUCCEED.The Fulfillment Service would also periodically check the auction that is in PAYMENT_PENDING status and see if there is any auction that exceeds the deadline but still not SUCCEED yet, and move them to FAILED status.Notice that in our design, the Fulfillment Service depends on the cache to trigger the bid execution. This requires us to have cache to be highly available (through strategy such as different replication mechanism). Another option is to directly have the Fulfillment Service to query the Auction DB where our ground truth data exists. It needs to perform a relative complex query to join auction_table with bid_table to find the wining bid of each ACTIVE auction and check if they need to be executed or not. This is one tradeoff we need to consider: use cache, pros is reduced latency, cons is potential inconsistency issue which cause missed execution directly read db, pros is accurate and no missed execution, cons is high latency and more pressure on DBFinal Stateless ArchitectureIn the final design, we also introduce a Reconcile Service which help us to detect certain abnormal situation. For example, the payment has succeed but the auction status is not correctly updated.Stateful ChoiceThe discussion above is mainly on the stateless design. In this section, we discuss a little bit about the stateful design and see how it would be different from the stateless one.We would make Auction Service stateful, which means that it would maintain all bid related data for an auction. Once owner create an auction, it would be randomly assigned to a Auction Service and all bid for this auction would be handled through this instance. To minimize the latency, we could make the state maintained in memory. But similar to Dispatcher, we still need to make it highly available. WAL + snapshot or rebuilding from Auction DB are available options.If user make a bid, we would leverage the load balancer to route this request to the right Auction Service instance to handle it (service discover). We don’t need another cron job scheduler to check if there is any bid needs to be executed, all these information is already available within the instance and it could handle that correctly.We could take a simple comparison between stateful and stateless   stateful stateless consistency easier to achieve high consistency as all data related to an auction is handled by a single server, for example we don’t need a separate fulfillment service to check if there is a bid to be executed more challenging because there could be concurrent data write on the same auction handled by different servers availability more challenging to achieve as we need to replicate the stateful data easier to handle as the server is stateless and all state data is handled by external storage scalability more challenging to scale, especially hotspot easier to scale as we could add more machines and evenly balance the traffic Additional DiscussionIn this section, we discuss about several additional points about the design.High AvailabilityDuring the high level design discussion, we have touched a little bit about how to achieve high availability in each component. In this section, we summarize the key points and add some additional ones. Auction Service in the stateless design, there is not much concern here, if the node is down before response back to client, the client would just retry and another node would help server the request. There might be duplicated write/update but it is fine in our case the auction creation could use upsert and check if there is the same user_id and item_id combination within a time rage for dedup the bid is designed to be in appends and only the last one (by request id or injection time) would be used as the user’s final bid the update to cache is fine regarding duplicated ones in the stateful design, we need to replicate the service state, by having follower node or snapshot the state to external storage Dispatcher: this is also a stateful service and the strategy is similar to the stateful Auction Service Cache, Auction DB, KV Store: different replication strategy could be discussed here, such as single leader, multi leader and quorum based. If you are not familiar with these concepts, please refer to the video below learn more details Bid Update Service: even though these service are also stateful because they need to maintain the connection with client, we don’t need to replicate them nor persistent the information similar to other stateful service. The reason is that: 1. this stateful information is coupled with the liveness of this service, if the node is down, the connection has to be rebuild with other nodes; 2. the stateful information is not shareable with other nodes Fulfillment Service: this is also stateless and we could have a active standby to take the work once the primary one is doneHigh ScalabilityWe didn’t talk too much about how the system could scale. Auction Service in the stateless design, it is pretty easy to scale as we could add more nodes to improve the request that we could handle in the stateful design, we could scale it via sharding by owner_id; sharding by auction_id is an option if we have a separate id generator to assign it upon the creation request Dispatcher: the size of the subscription_table is manageable (# of bid update server x 100k x 8 bytes ~ GB level), thus a single sever should be sufficient; however, the size of data is only one dimension we need to consider when scale the system, the QPS would also be a factor that we need to consider. For Dispatcher, it needs to deal with pretty high volume of request, thus we could add read replica to improve the throughput (sync replication for stronger consistency or async for eventual consistency), or we could also shard it by auction_id Cache, Auction DB, KV Store: different sharding strategy could be discussed here, such as partition by auction_id (which offers good co-locate property for the auction_table and bid_table but has the downside of hotspot); or partition by user_id (which might better distribute the write as is it relative rare for someone that becomes a hotspot and they could be rate limited) Bid Update Service: it is also easy to scale by adding more nodes because they only keep in-memory subscription_table Fulfillment Service: we could shard it by auction_id to evenly distribute the processing to more nodesCache and Auction DB consistencyIn our stateless design, we store all data into Auction DB, and also store highest bid related information for each auction in a cache. We adopted something similar to write through, in which we write DB first and then update the cache; another option to consider is write back, in which we update cache first, and then at sometime later right back to DB. Write back could be used if we decided to in real time update the winning bid into the auction_table to reduce the volume of write request.It is possible that we write to DB success but failed to update cache. For example, the request to update cache is failed or the node is down before try to update the cache. Retry could be used here, but it could still possible that the update to cache is failed after several retry. But since our Fulfillment Service reads the cache to execute the bid, it might read some outdated data because of the above potential failure. That is also why we have updated_at field to track if we should read from DB again to see if the data is up-to-date. Also upon serving request from client on pulling highest bid, we leverage updated_at to do a read repair to fix the potential out of date.Websocket and SSEWebsocket and SSE are 2 common way we build a connection with backend and keep it live to send/receive data; instead of repeatedly creating new request and sent it over. Below is a simple comparison of these 2 approach   Websocket SSE communication bi-direction single direction support most modern browser already supported limited browser support failure could not reconnect and need to establish a new one could reconnect data type support both text and binary data text data only application realtime messaging, online gaming stock monitor, live comment In our current design, we are establish a new SSE whenever user navigate to a new auction. Another design choice here is to let user establish a new connection upon login to our application. And keep a websocket connection. Whenever user navigate to another auction, it would send this event over the websocket so that the Bid Update Service could update the subscription table. Depends on the pattern of how general users are interacting with our system, we could optimize the choice of the connection mechanism. If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference Streaming a Million Likes/Second: Real-Time Interactions on Live Video How we designed Dropbox ATF: an async task framework Design Data-Intensive Applications Scaling Stateful Service Difference between Websockets and Server Sent Events Scaling Memcache at Facebook Stream Processing with Apache Flink DDIA Chapter 6 PartitionAcknowledgementThanks Rita and Celia for the great discussion and lots of idea." }, { "title": "How to use LLM for recommendation task", "url": "/How-to-use-GPT-for-recommendation-task/", "categories": "Machine Learning, Recommendation System", "tags": "llm", "date": "2023-12-12 00:00:00 -0800", "snippet": "Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would li...", "content": "Recently, I have been working with some of my friends (Dalao) on leveraging GPT to do recommendation tasks. This gives me an opportunity to review some paper in this field. In this post, I would like to summarize some of my learnings along the journey.Some key take away: LLM internally has encapsulated lots of knowledge about the world and it could leverage these knowledge to do some general recommendation (such as Movie) In context learning is a powerful technique to inject various information into promote to provide more context for LLM, such as user profile and user past interaction history Use training data that specifically constructed for recommendation task to fine tune LLM could further improve the performance of LLM We could directly use LLM to output candidate, or use LLM output as additional signal to inject into existing recommendation models PS: due to the rapid change of this area, the paper I read might have been outdated. Please feel free to leave comments on the latest work/idea in this domain. Also I’m reading the latest paper from arxiv and will potentially have a new series of post on summarizing the latest work in LLM and ML area, stay tuned!PPS: I would primarily summarize my understanding without to much technical terms and mathematic formula; the main goal is to grasp the high level idea of the paperContextIn classical recommendation system, we usually adopt a 2-stage architecture. In first stage, we adopt heuristic rule, or leverage some simple model to quickly identify some promising candidates from the entire eligible population (actually, there is indexing step before here as well, but for simplicity, let’s skip that). This first stage is called candidate retrieval, which we usually optimize for recall. In the second stage, we would rank the candidates we retrieved in the first stage, via more signals and more powerful model. This stage is usually called rerank, which optimize for precision.Pairwise Ranking via LLMIn paper “Large Language Model Are Effective Text Rankers With Pairwise Ranking Prompting”, the author proposed a new format of prompt that let LLM to rank a pair of candidates given a query, which outperforms the point-wise and list-wise format. The format of the prompt is as follow:f\"\"\"Given a query {query}, which of the following two passage is more relevant to the query?Passage A: {description of A}Passage B: {description of B}Output Passage A or Passage B\"\"\"For each pair of candidates, we use the above prompt to let LLM output the choice, and compute the final scores as\\[s_{i} = 1 * \\sum_{j \\neq i} I_{d_{i} &gt; d_{j}} + 0.5 * \\sum_{j \\neq i} I_{d_{i} = d_{j}}\\]and rank the document accordingly.Enrich the information for LLM to recommendPersonalized recommendation is critical to improve the conversion rate. Use profiling, user past’s item interaction history bring valuable signal for recommendation. In this section, we will take a look some idea on how to inject such information into prompt to let LLM “learn” the flavor of user and provide better personalized result.In “Is ChatGPT a Good Recommender? A Preliminary Study”, the authors proposed different type of prompt of different type of tasks. These prompt could be decomposed as task descriptor, user-specific injection, formatting restrictions. User-specific injection is the part where we add user’s past item interaction info. The format for sequential recommendation is as follow (content in bracket is comment)f\"\"\"Requirement: you must choose 10 items for recommendation and sort them in order of priority, from hightest to lowest. [task descriptor]Output format: a python list. Do not explain the reason for include any other words. [formatting restrictions]Given user's interaction history in chronological order: {[i_1, i_2, i_3, ..., i_n]}, the next interaction item is {i_n+1}. [In context learning]Now, if the interaction history is updated to {[j_1, j_2, j_3, ..., j_n]} and the user is likely to interact again, recommend the next item. [user-specific injection]\"\"\"In this prompt, a common technique, which is called in context learning, or few shot prompting , is used. By showing LLM some examples to follow in the prompt, we could change the underlying distribution of LLM model and bias it to generate the output conditionally on the examples we have given. This stanford blog is a great source to learn more on how in context learning works. In short words, the additional example we provided helps LLM to better locate concept internally, and thus more aligned. A Bayesian inference view on that is as follow, which is pretty easy to understand\\[p(output|prompt) = \\int_{concept}p(output|concept, prompt)p(concept|prompt)d(concept)\\]In “PALR: Personalization Aware LLMs for Recommendation”, author adopted similar approach to integrate users’ past interaction into prompt. One novel idea in this paper is to leverage LLM to generate user profile, which leverages the summarization capability of LLM. The prompt is as follow (use MovieLens-1M as example)f\"\"\"Input: Your task is to use two keywords to summarize user's preference based on history interactions.The output is an itemized list based on importance. The output template is:{KEYWORD_1: \"HISTORY_MOVE_1\", \"HISTORY_MOVE_2\"; KEYWORD_2: \"HISTORY_MOVE_2\"}The history movies and their keywords\"MOVIE_1\": KEYWORD_1, KEYWORD_2\"MOVIE_2\": KEYWORD_1, KEYWORD_3\"MOVIE_3\": KEYWORD_4\"MOVIE_4\": KEYWORD_1, KEYWORD_3, KEYWORD_4\"\"\"Then the user profile is also input into the prompt to let LLM recommend items from the candidate set.In context learning is a technique that I widely used during my project. It is much cheaper compared to fine-tune LLM, and the performance is also pretty good as long as you have high quality data. From my experience, formatting control is pretty challenge and sometimes could not be 100% solved by explicit instructions or few shot. Sometimes, we need to have some dedicated business code to do some postprocessing on LLM output to parse the part we interested most out.Go beyond In-Context Learning: Fine-tune LLM for recommendation taskIn context learning is a powerful technique, however, due to the fact that LLM is trained on NLP task instead of recommendation task, its performance is still sometime limited. Using some training data that is specifically constructed for recommendation to fine-tune LLM could help LLM to learn more for recommendation task.In TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation, the author proposed a 2-stage fine-tuning framework. In first stage, they leverage Alpaca Tuning to improve LLM’s generalization ability, and then in 2nd stage, they use recommendation training data to do rec tuning. The format of the training data is as followf\"\"\"Task instruction: Given the user's historical interactions, please determine whether the userwill enjoy the target new movie by answering \"Yes\" or \"No\".Task input: - User's liked items: GodFather. - User's disliked items: Star Wars. - Target new movie: Iron Man.Task output: No\"\"\"A high level flow is as followWork with existing Recommendation modelsBesides directly let LLM to output the recommendation from the candidates, we could also use LLM together with existing recommendation models. Use the output of one model as input to another model has been a widely adopted practice in the ranking world, e.g. using the GBDT leave as feature in NN. You could think of that we leverage model to do some compression and preprocessing on the signals, which is similar to traditional feature engineering.In LLM-Rec: Personalized Recommendation via Prompting Large Language Models, the author used different prompt to generate various text description from the original content, and then embedding them as additional signals and feed into MLP for ranking together with the original descriptions. Below is a high level architecture of their model" }, { "title": "How to Design Webhook", "url": "/How-to-Design-Webhook/", "categories": "Distributed System", "tags": "system design, webhook", "date": "2023-12-03 00:00:00 -0800", "snippet": "Today, let’s discuss about how to design a system that could let customer to register webhook and send webhook requests to destination.Let’s first align on some terms that we are going to use acros...", "content": "Today, let’s discuss about how to design a system that could let customer to register webhook and send webhook requests to destination.Let’s first align on some terms that we are going to use across this post: webhook provider: the platform that let customer to register webhook and send the webhook request webhook customer: they provide the endpoint they would like the provider to send the webhook request toWhat is WebhookFor readers that are not familiar with Webhook, it is a type of notification mechanism that communicates in one direction. This is a technique widely used in SaaS platform (e.g. Shopify, Strip, Slack) for external applications to receive data when some events they interested in happened on these platform.For example, codingmonkey.com is a website that I’m running (hosted on AWS maybe), and I have a shop on Shopify that sells awesome keyboards. I could register a Webhook on Shopify so that whenever there are some Shopify users purchase awesome keyboards, a purchase event would be sent to an endpoint that is hosting on my server to process (e.g. store it in database, issue an invoice to purchaser, or send a thank you email).Sounds similar? Yeah, it sounds pretty like a notification system. The difference here is that customer need to register webhook to express which event they would like to listen to and which endpoint URL the data need to be send to. There are also some other difference, such as we need to know if the webhook request is successfully received by codingmonkey.com or not, and additional security check to protect the data we are sending. Excited to learn more? Let’s dive deep and see how we could build such a system.Functional RequirementI didn’t find very crystal requirements on this, the following is some FR I summarized from the industrial examples Customer could register webhook and they could register multiple webhook Support retry of webhook and minimize lost webhook as much as possible Provide observability to customersNon Functional Requirements 1B events pre day, which is equivalent to 10k qps for webhook trigger and request sending High availability The design should scale SecuritySome questions to clarify Do we allow event loss? No, we should avoid event loss as much as possible. What delivery semantic do we provide? At least once, at most once or exactly once? At least once If we resend webhook, could we resume the endpoint to be idempotent? Yes, but we need to provide necessary info to achieve that High Level DesignI would skip the API design and the back envelop estimation for the sake of sanction of this post. We would start simple to first meet the functional requirements, and then improve the availability, scalability of our system.Webhook RegistrationWe need to be able to let user to register webhook in our system. Below is a simple design of this partThe design is pretty simple, which we have web server to handle request from client and store the information in the webhook metadata database. This metadata database is going to be used by the webhook delivery flow to figure out where to send the webhook request to.For each webhook, we would generate a unique webhook_id as the unique identifier of each webhook. Besides that, we also need to store the event_type that this webhook listen to, as well as the owner_id. The event_type is a list of per-defined events that are available on our platform, which could be revealed via API document provided to customer. Besides that, we also need to store the url and secret_token in the database, to know where we should send the request to, as well as sending the request safely. The secret_token could be used for authentication and encryption for sending the webhook requests. In this schema, customers could register multiple webhook within the system.One challenge here is that how to verify that customers have ownership on the urls they have provided. One common solution here is to send a test event to the endpoint they are providing, and ask them to verify they have received it; or by including a “challenge” in the request that the endpoints need to echo back (e.g. Dropbox webhook).Webhook DeliveryNext, let’s take a look at the webhook request deliver flow, which is the meaty part of the entire system. As mentioned earlier, the entire system is similar to notification system, and thus I use notification system as a template for this design. Below is a high level design of this flowIn this design, we adopted a single responsibility strategy and separate the delivery logic into several components Webhook Controller is responsible for processing the events (that are generated on our platform) and figuring out which endpoint we should send the data, as well as constructing the payload of the request. Here we assume that the events generated from our platform contains the event_type and owner_id information (because we don’t want the event that happened in our shop to be delivered to others’ endpoints). With event_type and owner_id, controller could retrieve the record from the metadata database and construct a webhook request task. Once the task is constructed, controller would write an entry into Webhook Delivery Log database to persistent this information, and set the request_status to PENDING, which we could leverage later for different retry strategy. Message Queue is adopted to store the webhook request task, which worker would consume. Using message queue bring the following benefits, which outperform the additional complexity they bring: controller don’t need to wait for the current webhook request to be delivered to process the next one (it is async okay). This not only saves resource, but also increases robustness (e.g. if worker failed, controller could still make progress and put job onto the queue instead of being blocked). if there is a burst of events come in, message queue could help buffer the increased volume of task so that worker won’t be throttling. Webhook Worker is responsible for consume webhook request task from the queue, and send the actual HTTP POST request to the endpoint. The payload of the HTTP POST request could be something like this{ \"id\": str \"event_type\": str \"created\": int, \"data\": { \"field_1\": value_1, \"field_2\": value_2, ... }}Worker would need to wait for the response from the endpoints, to know if the request has been successfully received. If received, then worker could update the record’s request_status in the Webhook Delivery Log database to SUCCEED; otherwise, different strategy of retry could be adopted to resent the webhook request. Supporting retry also means that we are providing at least once semantic, which could result in duplicated request sent to endpoints. We expect these endpoints need to be idempotent, which is doable with the id sent along with the HTTP POST request.Webhook Retry StrategyOne critical consideration for webhook system is the retry mechanism in case HTTP POST returns 4xx or 5xx code, or timeout. There are different retry strategies: Retry immediately upon failure within a time range repeatedly, or until max retry limit Exponential backoff within a time range (e.g. 24hrs), or until max retry limitFor example, Strip would attempts to delivery webhook up to 3 days with an exponential backoff. Option 1 is easy to implement, but the issue is that: if endpoint is returning error code, then it might take some time to mitigate the issue; immediate retry is likely to hit the same error, try again later time would be a better option.In order to achieve exponential backoff retry mechanism, we would use a cron version Webhook Controller, which dose not consume the events from upstream, but scan the Webhook Delivery Log database to identify the webhook requests that are still in PENDING status and have not exceed the max retry. For each of such request, the controller would bump their retry_count or retry_timestamp, and publish a new task into message queue.The addition of this cron version Webhook Controller could also help mitigate worker failure issue. For example, if one webhook http request is consumed and removed from the message queue by a worker, but suddenly the worker failed; since the task is already removed from the queue, other worker won’t able to get it and process it again. However, the cron controller would notice in from the log that there is one PENDING request and schedule it to retry. Another option is that if message queue provide the capability to persistent messages, worker could commit the position of the message in the queue they have processed, and if worker failed, it could resume from its last committed position and process the message againIf for some endpoints, the failure is consistent for a certain time and over the threshold, we could temporarily mark the endpoints as disabled in the metadata’s status field to prevent new events from further deliver to them. And we could send alert email to customers to have them investigate into the issue. Once the issue is mitigated, the status could be changed back, and we could consumer the delivery log to resume the webhook request; or use other channel, such as dump the entire data that need to be delivered during this time and send it over to customer.ObservabilitySince we have already log the status of each webhook request in Webhook Delivery Log database, it is easy to support the observability. This could be implemented via having web application server to send a query to the database to aggregate the data and render it as a dashboard for customers. They could know how many webhook request have been sent, what’s the failure rate, etc.SecuritySecurity is especially important in webhook system. In webhook registration section, we authenticate that the endpoints belongs to users, we also need to authenticate ourself that the HTTP request is from us.One common approach is to use HMAC to sign the request with a shared secret with the user and sent the signature along with the request(e.g. Strip uses this approach) and user could verify the signature with the shared secret. This shared secret could be auto generated upon user register webhook in our system, and show them to user in their monitor dashboard. This approach could also help us prevent replay attack, by including a timestamp used to expire webhook request.Another approach, which is less common, is to get a token from the consumer and add it to the Authorization header for validation. For example, if the owner of the endpoint has authorization server, then before sending webhook request, we could first obtain a JWT token and store it within our metadata table secret_token and use it each time we need to send webhook request.Besides the authentication problem, we also need to prevent the data we are sending could be read by others. There are also several options with different trade off: Avoid send sensitive information in the webhook payload. Instead, we could only send some entity id which is totally meaningless and ask customer to pull data again via other API. Pros is that this is the most safe approach, and the cons is that customer experience is worst Another option is to encrypt the data with a shared secret key, which is only known between webhook provider and webhook consumer. A follow up of this question is how could we share this secret key safely between customer and provider over the unsafe network? Here we could use RSA encryption. (This is a general practice, RSA is safe, since only yourself know the private key; but the amount of data could be transferred via RSA is limited. So it makes since to use RSA to send another secret key, which is used for encryption/decryption of large volume of data) Sending data with HTTPS and certificate pinning is also an option to safely transfer sensitive data, but this would have some performance hurt and require customer to have HTTPS setup such as CAHigh AvailabilityLet’s see if there is any single point of failure in our current design. What comes to us first is the database and message queue. There are multiple replication strategy here we could use, each comes with different trade off: For Webhook Metadata Database, we could adopt single leader strategy, and have 2 followers. The followers could use synchronized replication, which provides good consistency, but the write throughput on the leader would be low; while if we use async approach, leader could handle more write request while could lead to consistency issues among leader and followers. If we are building for a geo webhook system, we might also consider multi-leader strategy, with better write request severing based on location and annoy of write conflict. For Webhook Delivery Log Database, besides the aforementioned strategy, we could also consider the quorum based replication, which provides the best write throughput and eventual consistency is acceptable in this case. (Q: what would be the worst case here). For Message Queue, similar to the database, we could also have replica setup so that the message is written to multiple node instead of single one. Also, even if we only have a single node queue and it failed. Since we are storing all scheduled webhook request in the Webhook Delivery Log Database, the webhook controller (corn) would identify the abnormal ones and try to reschedule them.For other components such as web app server, webhook controller and webhook worker, they could be stateless. If a node fails, there would be other nodes available to continue the work.ScalabilityFor scalability, we could horizontally scale web app server, webhook controller and webhook worker by adding more nodes into the cluster. For database, we could shard it to scale if the total volume of data is too large to fit onto a single machine. Message queue could also be horizontally sharded by increase the number of partitions.There could be hotspot. For example, my awesome keyboard is so popular that lots of customer is visiting my shop and vast amount of events are triggered. To handle the hotspot, we could use a dynamic config to redirect the traffic of hotspot to specific cluster of machines, instead of starving the quote with other customers; or we could further shard the hotspot by some approach such suffix with numbers.Other optimizationThere are couple of other optimizations we could add to our system to make it more robust we could have load balances in front of webhook controller to route based on machine utilization; also we could integrate the rate limit here to prevent abuse of the system (such as bot triggered events) we could add a layer of cache to reduce the amount of read to metadata we could add a rate limiter to help control the http request we send to customers; for some customers that have high security requirement, they might only trust http request sent from specific IPs, we could have dedicated VPC to support that needs for observability, we could add some pre-compute mechanism to reduce the volume of data that the query need to scan; for example T-1 snapshot + on demand query on THere is our final design If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffeeReference Building Webhooks Into Your Application: Guidelines and Best Practices Strip Webhook Doc Dropbox Webhook Doc Shopify Webhook Best Practices Add Webhooks to Your API the Right Way Phoenix Architecture" }, { "title": "DDIA Chapter 11 Stream Processing Part I", "url": "/DDIA-Stream-Processing-I/", "categories": "Distributed System, DDIA", "tags": "system design, message queue, realtime system", "date": "2023-11-21 00:00:00 -0800", "snippet": "In this post, we would introduce stream processing. Since it is a large topic, we would break it down into 2 part, and in the first part, we would focus on the component that is related to the “flo...", "content": "In this post, we would introduce stream processing. Since it is a large topic, we would break it down into 2 part, and in the first part, we would focus on the component that is related to the “flow” of stream, a.k.a, delivery of message.What is EventStream is composed by sequence of event, which we also use message as an alternative term. Here is a quote from confluent on describing what is event An event is any type of action, incident, or change that’s identified or recorded by software or applications. For example, a payment, a website click, or a temperature reading, along with a description of what happened.Take the payment as an example, a payment event, could be User A paid X dollars to User B, for the purchase of an item C, on date X. This event would be recognized by our system to trigger the necessary processing (e.g. record in database, make third-party API call).How to deliver messageHow could we deliver message from machine A to machine B? There are multiple options.Direct connectionThe most straight-forward approach is to build a direct connection between A and B via network. Once the connection is published, B could receive the message from A in 2 different patterns Proactively asking A if there is new message with some intervals in between these ask Passively wait until A notify that there are some message for B to readThese 2 different patterns, more formally speaking, pull and push, is common approach on how message is delivered, or how consumer (B in our example) would receive the message.Direct connection works, but what would happen if B somehow offline for a period of time $T$? B would miss all the message A plans to deliver during $T$. One potential solution is to add the capability of storing the message temporarily within A, but that would increase the responsibility of A and make it more complexity. We need some sort of dedicated component to help us, this lead to message broker, or message queue, which is really good at this job.Message QueueMessage queue could be treated as some type of buffer in between of the message sender, a.k.a producer, and message receiver, a.k.a consumer. Producer would publish message to message queue, message queue would do some “necessary” processing on the message and hold it. Consumer could retrieve these message from message queue, by subscribing to some queue. Since message is buffered in message queue, it is okay that B is offline when A tries to send message, message queue would hold that message, and when B comes online, the message is not lost and could be consumed.When to use Message QueueMessage queue is pretty good to be used when the business involves certain async property, which means that user don’t expect an immediate response from the application, but could retrieve the result sometime in the future. Some typical case including: Job scheduler: user schedule a job (e.g. project building, model training) and expect it to finish sometime in the future Youtube video encoding: when user upload a video, the encoding job would be pushed onto a queue and be processed by some worker in the future Notification: a job to send some customer SMS/Email would be placed on queue and be sent in the futureIn the later section, we would see some more concrete example from industry on how message queue is being used in practice.Everything has two sides. The benefits of using message queue is that: 1. improve overall robustness of the system be decoupling different components; 2. balance the workload for upstream/downstream system (e.g. in case of burst of traffic). The downside of message queue is that, it would increase the complexity of the overall system (e.g. how to handle duplicated events gracefully).Industry practiceRabbitMQ &amp; KafkaRabbitMQ and Kafka is 2 commonly adopted message queue in industry. For a deeper dive into these 2 message queue, we would put it into another post. Here we would first summarize some highlight of them:   RabbitMQ Kafka Message Persistent control by request parameter persistent Message Delivery pull push Message Ack auto-ack or explicit ack no ack, consumer commit offset Scalability vertical horizontal Availability single node in general leader-follower replication Order Guarantee FIFO in general, special case: priority, sharded queue, multi consumer FIFO on partition level Consumer Load Balance priority or round robin different strategy specified by consumer group DoorDashIn this engineering blog, DoorDash introduced how they are using message queue in their business and why they migrate from RabbitMQ to Kafka. Several business task in DoorDash is done in async, such as order checkout, merchant order transmission and dasher location processing DoorDash use Celery + RabbitMQ as their initial async task processing infra. However, they identified several pain points: Availability is low. RabbitMQ would easily down during peak traffic. Traffic control needs to be enabled to prevent the issue that task consumption could not keep up with task publishing, which cause serious network lagging. Scalability is low. They are running the largest RabbitMQ node already (vertical scale). And they are using the primary-secondary HA mode, which also prevent them from scale (the down time could easily goes to 20mins to recover) They migrate RabbitMQ to Kafka to achieve better availability (partition replicated) and scalability (partitioned topic) They also mentioned on improvement on dealing with “straggler”: using one dedicated thread to read message from topic partition, and use multi-threading to process the message. Thus, if one message takes long time to process, then only one thread would be blocked, while other thread could continues to process the messages RobinhoodIn this blog from Robinhood, the author introduced how they are using Kafka to build their clearing service (which is one critical service to make sure the inside and outside account information is insync). Clearing service is not on the critical path of users (users don’t need to be aware of this), and thus they decided to build it as an async service. In their initial design, they use a monolith consumer, which contains a giant transaction to make update to several tables. This raise the contention issue and the efficiency is low. In their new design, they breakdown the original transaction into several smaller transaction to update only 1 ~ 2 tables. They also adopt the event source pattern that, once one job is done (e.g. user table update finished), it would fire one event to a Kafka topic, and one downstream consumer would consume the event and to the necessary update (e.g. update account table), and then fire another event. The benefit of this reduction in contention and overall throughput improvement But what if one consumer in the middle failed, how to resume and avoid duplicated write? Use Kafka commit log to resume where left When do the DB write, first update the lookup table, then the duplicated write would be no-op If you find this post helpful, feel free to scan the QR code below to support me and treat me to a cup of coffee" } ]
