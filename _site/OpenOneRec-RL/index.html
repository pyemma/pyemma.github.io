<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Learning VERL Part 1 - A Perspective from OpenOneRec" /><meta name="author" content="Coding Monkey" /><meta property="og:locale" content="en" /><meta name="description" content="This post documents my journey learning VERL (Volcano Engine Reinforcement Learning), a scalable and efficient reinforcement learning framework, through the lens of OpenOneRec’s implementation. OpenOneRec uses VERL to implement PPO-based training for recommendation systems with a two-stage generation approach (Chain-of-Thought reasoning followed by item ID generation)." /><meta property="og:description" content="This post documents my journey learning VERL (Volcano Engine Reinforcement Learning), a scalable and efficient reinforcement learning framework, through the lens of OpenOneRec’s implementation. OpenOneRec uses VERL to implement PPO-based training for recommendation systems with a two-stage generation approach (Chain-of-Thought reasoning followed by item ID generation)." /><link rel="canonical" href="https://pyemma.github.io/OpenOneRec-RL/" /><meta property="og:url" content="https://pyemma.github.io/OpenOneRec-RL/" /><meta property="og:site_name" content="Coding Monkey" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2026-01-31T00:00:00-08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Learning VERL Part 1 - A Perspective from OpenOneRec" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@pyemma" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Coding Monkey","url":"https://github.com/pyemma"},"dateModified":"2026-01-31T00:00:00-08:00","datePublished":"2026-01-31T00:00:00-08:00","description":"This post documents my journey learning VERL (Volcano Engine Reinforcement Learning), a scalable and efficient reinforcement learning framework, through the lens of OpenOneRec’s implementation. OpenOneRec uses VERL to implement PPO-based training for recommendation systems with a two-stage generation approach (Chain-of-Thought reasoning followed by item ID generation).","headline":"Learning VERL Part 1 - A Perspective from OpenOneRec","mainEntityOfPage":{"@type":"WebPage","@id":"https://pyemma.github.io/OpenOneRec-RL/"},"url":"https://pyemma.github.io/OpenOneRec-RL/"}</script><title>Learning VERL Part 1 - A Perspective from OpenOneRec | Coding Monkey</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Coding Monkey"><meta name="application-name" content="Coding Monkey"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.29.0/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return 'mode'; } static get MODE_ATTR() { return 'data-mode'; } static get DARK_MODE() { return 'dark'; } static get LIGHT_MODE() { return 'light'; } static get ID() { return 'mode-toggle'; } constructor() { let self = this;this.sysDarkPrefers.addEventListener('change', () => { if (self.hasMode) { self.clearMode(); } self.notify(); }); if (!this.hasMode) { return; } if (this.isDarkMode) { this.setDark(); } else { this.setLight(); } } get sysDarkPrefers() { return window.matchMedia('(prefers-color-scheme: dark)'); } get isPreferDark() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }get modeStatus() { if (this.hasMode) { return this.mode; } else { return this.isPreferDark ? ModeToggle.DARK_MODE : ModeToggle.LIGHT_MODE; } } setDark() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { document.documentElement.setAttribute(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { document.documentElement.removeAttribute(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); }notify() { window.postMessage( { direction: ModeToggle.ID, message: this.modeStatus }, '*' ); } flipMode() { if (this.hasMode) { this.clearMode(); } else { if (this.isPreferDark) { this.setLight(); } else { this.setDark(); } } this.notify(); } } const modeToggle = new ModeToggle(); </script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="/assets/profile.png" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a><h1 class="site-title"> <a href="/">Coding Monkey</a></h1><p class="site-subtitle fst-italic mb-0">I’m a staff software engineer with rich experience in recommendation system and machine learning infrastructure. I spent my last 8 years in both Big-Tech (Meta & LinkedIn) and startups (Aven), and I’m glad to see if my past experience and learnings could help boost your career growth <br><br> <a href="https://bit.ly/41vi77B"><strong>book a session now</strong></a></p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pyemma" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['pyemma1991','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Learning VERL Part 1 - A Perspective from OpenOneRec</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1"><header><h1 data-toc-skip>Learning VERL Part 1 - A Perspective from OpenOneRec</h1><div class="post-meta text-muted"> <span> Posted <time data-ts="1769846400" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 31, 2026 </time> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/pyemma">Coding Monkey</a> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="4973 words" > <em>27 min</em> read</span></div></div><div style="text-align: right;"> <span> <a href="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" class="popup img-link shimmer"><img src="https://hitscounter.dev/api/hit?url=https%3A%2F%2Fpyemma.github.io%2F&label=&icon=github&color=%23198754&message=&style=flat&tz=UTC" alt="Views" loading="lazy"></a> </span></div></div></header><div class="content"><p>This post documents my journey learning <a href="https://github.com/volcengine/verl">VERL</a> (Volcano Engine Reinforcement Learning), a scalable and efficient reinforcement learning framework, through the lens of <a href="https://github.com/pyemma/OpenOneRec">OpenOneRec</a>’s implementation. OpenOneRec uses VERL to implement PPO-based training for recommendation systems with a two-stage generation approach (Chain-of-Thought reasoning followed by item ID generation).</p><p>VERL provides a sophisticated abstraction layer over Ray for distributed RL training, handling complex orchestration between actor policies, reference policies, critics, and rollout workers. This analysis focuses on how OpenOneRec leverages VERL’s infrastructure to implement its reinforcement learning pipeline.</p><p>The introduction in this post is relative high level. I plan to dive deeper into the VERL framework and share more learnings in the future blogs.</p><h2 id="high-level-architecture-overview"><span class="me-2">High-Level Architecture Overview</span><a href="#high-level-architecture-overview" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>The overall PPO training flow in OpenOneRec consists of several key components working together:</p><h3 id="main-components-and-their-roles"><span class="me-2">Main Components and Their Roles</span><a href="#main-components-and-their-roles" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre>┌─────────────────────────────────────────────────────────────────┐
│                     main_onerec_ppo.py                          │
│                     <span class="o">(</span>Entry Point<span class="o">)</span>                               │
└───────────────────────────┬─────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────────────┐
│                    OneRecTaskRunner                             │
│  • Loads configuration                                          │
│  • Creates resource pools and dataloaders                       │
│  • Initializes worker <span class="nb">groups </span>and trainer                        │
└─────┬─────────────────┬─────────────────┬────────────────┬──────┘
      │                 │                 │                │
      │ creates         │ configures      │ configures     │ configures
      ▼                 ▼                 ▼                ▼
┌─────────────┐   ┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│RayPPOTrainer│   │Actor/Rollout/│  │   Critic     │  │   Reward     │
│             │   │  Ref Worker  │  │WorkerGroup   │  │WorkerGroup   │
│             │   │  WorkerGroup │  │              │  │              │
└──────┬──────┘   └──────┬───────┘  └──────┬───────┘  └──────┬───────┘
       │                 │                 │                 │
       │ orchestrates    │                 │                 │
       └─────────────────┼─────────────────┼─────────────────┘
                         │
                         │ implements
                         ▼
         ┌───────────────────────────────────────┐
         │  OneRecActorRolloutRefWorker          │
         │  <span class="o">(</span>Hybrid: Actor/Rollout/Reference<span class="o">)</span>    │
         └───┬───────────┬───────────┬───────────┘
             │           │           │
             │ uses      │ uses      │ uses
             ▼           ▼           ▼
    ┌────────────┐  ┌─────────────────────┐  ┌──────────────────┐
    │OneRecvLLM  │  │FSDPVLLMShardingMgr  │  │DataParallelPPO   │
    │Rollout     │  │                     │  │Actor             │
    │<span class="o">(</span>2-stage<span class="o">)</span>   │  │<span class="o">(</span>Param <span class="nb">sync</span><span class="o">)</span>         │  │<span class="o">(</span>Training<span class="o">)</span>        │
    └────────────┘  └─────────────────────┘  └──────────────────┘
</pre></table></code></div></div><ol><li><strong>OneRecTaskRunner</strong>: The orchestrator that initializes the entire training pipeline. It:<ul><li>Loads configuration and identifies the appropriate <code class="language-plaintext highlighter-rouge">ActorRolloutRefWorker</code> implementation<li>Creates <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> instances for different roles (actor, critic, reward)<li>Initializes <code class="language-plaintext highlighter-rouge">ResourcePoolManager</code> for GPU allocation and scheduling<li>Sets up the dataloader for training batches<li>Creates <code class="language-plaintext highlighter-rouge">RayPPOTrainer</code> and invokes the training entry point</ul><li><strong>RayPPOTrainer</strong>: The coordinator for the PPO algorithm. It:<ul><li>Manages the overall scheduling of the PPO algorithm<li>Delegates specific responsibilities (<code class="language-plaintext highlighter-rouge">Actor</code>, <code class="language-plaintext highlighter-rouge">Critic</code>, <code class="language-plaintext highlighter-rouge">Ref</code>) to worker classes<li>Initializes worker groups based on resource pool specifications<li>Maps roles to resources using <code class="language-plaintext highlighter-rouge">RayClassWithInitArgs</code><li>Optimizes resource usage through colocation (multiple roles can share the same resource pool)</ul><li><strong>RayWorkerGroup</strong>: The distributed execution engine. It:<ul><li>Takes <code class="language-plaintext highlighter-rouge">RayResourcePool</code> and <code class="language-plaintext highlighter-rouge">RayClassWithInitArgs</code> during initialization<li>Binds worker class methods to the WorkerGroup using <code class="language-plaintext highlighter-rouge">_bind_worker_method</code><li>Routes method calls (e.g., <code class="language-plaintext highlighter-rouge">generate_sequence</code>) to the appropriate worker implementation<li>Handles distributed communication patterns (similar to FSDP2’s parameter sharding/unsharding via dispathers and <code class="language-plaintext highlighter-rouge">@register</code>)</ul><li><strong>OneRecActorRolloutRefWorker</strong>: OpenOneRec’s customized worker implementation that:<ul><li>Overrides <code class="language-plaintext highlighter-rouge">_build_rollout</code> to use the custom <code class="language-plaintext highlighter-rouge">OneRecvLLMRollout</code><li>Integrates FSDP2 for distributed training and vLLM for efficient inference</ul><li><strong>OneRecvLLMRollout</strong>: Implements the two-stage rollout:<ul><li>Stage 1: Generate Chain-of-Thought (CoT) reasoning<li>Stage 2: Generate item ID sequences using beam search</ul></ol><h3 id="training-loop-data-flow"><span class="me-2">Training Loop Data Flow</span><a href="#training-loop-data-flow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
</pre><td class="rouge-code"><pre>┌─────────────┐     ┌──────────────────┐     ┌───────────────┐     ┌──────────┐
│ DataLoader  │     │ RayPPOTrainer    │     │Actor Worker   │     │Critic    │
│             │     │                  │     │Group          │     │Worker    │
└──────┬──────┘     └────────┬─────────┘     └───────┬───────┘     └────┬─────┘
       │                     │                       │                   │
       │ For each epoch:     │                       │                   │
       │                     │                       │                   │
  1.   │──── batch ─────────&gt;│                       │                   │
       │                     │                       │                   │
  2.   │                     │── generate_sequence ─&gt;│                   │
       │                     │                       │                   │
  3.   │                     │&lt;── sequences ─────────│                   │
       │                     │    <span class="o">(</span>with old_log_probs<span class="o">)</span>                   │
       │                     │                       │                   │
  4.   │                     │─ compute_reward<span class="o">()</span>     │                   │
       │                     │                       │                   │
  5.   │                     │─ compute_ref_log_prob ─&gt;                  │
       │                     │&lt;─ ref_log_probs ──────│                   │
       │                     │                       │                   │
  6.   │                     │─ compute_advantage<span class="o">()</span>  │                   │
       │                     │                       │                   │
  7.   │                     │─────────── update_critic ────────────────&gt;│
       │                     │&lt;────────── critic_metrics ────────────────│
       │                     │                       │                   │
  8.   │                     │─── update_actor ──────&gt;│                   │
       │                     │    <span class="o">(</span>after warmup<span class="o">)</span>     │                   │
       │                     │&lt;── actor_metrics ─────│                   │
       │                     │                       │                   │
       │                     │ <span class="o">(</span>repeat <span class="k">for </span>next batch<span class="o">)</span>                   │
       ▼                     ▼                       ▼                   ▼
</pre></table></code></div></div><p>The <code class="language-plaintext highlighter-rouge">fit</code> method in <code class="language-plaintext highlighter-rouge">RayPPOTrainer</code> orchestrates this flow:</p><ul><li><strong>Generate sequences</strong>: Call <code class="language-plaintext highlighter-rouge">actor_rollout_wg.generate_sequence(batch)</code><li><strong>Compute rewards</strong>: Apply reward function to generated sequences<li><strong>Compute log probabilities</strong>: Get old policy and reference policy log probs<li><strong>Compute advantages</strong>: Calculate PPO advantages using GAE or other estimators<li><strong>Update critic</strong>: Train value function (if using critic)<li><strong>Update actor</strong>: Perform PPO policy update after critic warmup</ul><p>This architecture enables efficient distributed RL training by separating concerns, optimizing resource usage, and providing clean abstractions for extending the system.</p><h2 id="fundamental-building-blocks"><span class="me-2">Fundamental Building Blocks</span><a href="#fundamental-building-blocks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Before diving into specific components, it’s essential to understand two fundamental building blocks that power VERL’s distributed execution: <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> and the <code class="language-plaintext highlighter-rouge">@register</code> decorator. These abstractions make distributed RL training manageable by hiding complex coordination logic.</p><h3 id="rayworkergroup-abstraction-over-ray-for-distributed-rl"><span class="me-2">RayWorkerGroup: Abstraction Over Ray for Distributed RL</span><a href="#rayworkergroup-abstraction-over-ray-for-distributed-rl" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>While Ray provides powerful primitives (<code class="language-plaintext highlighter-rouge">@ray.remote</code>, <code class="language-plaintext highlighter-rouge">ray.get()</code>, <code class="language-plaintext highlighter-rouge">Actor</code>), distributed RL training has specific requirements that would require significant boilerplate code. <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> provides a clean abstraction layer that handles:</p><h4 id="1-automatic-distributed-training-coordination"><span class="me-2">1. Automatic Distributed Training Coordination</span><a href="#1-automatic-distributed-training-coordination" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><strong>Without RayWorkerGroup</strong>, you’d need to manually manage:</p><ul><li>MASTER_ADDR, MASTER_PORT discovery<li>RANK assignment for each worker<li>WORLD_SIZE propagation</ul><p><strong>With RayWorkerGroup</strong>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1"># Automatically handles MASTER_ADDR, MASTER_PORT, RANK assignment
</span><span class="n">actor_worker_group</span> <span class="o">=</span> <span class="nc">RayWorkerGroup</span><span class="p">(</span>
    <span class="n">resource_pool</span><span class="o">=</span><span class="nc">RayResourcePool</span><span class="p">(</span><span class="n">process_on_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">]),</span>
    <span class="n">ray_cls_with_init</span><span class="o">=</span><span class="nc">RayClassWithInitArgs</span><span class="p">(</span><span class="n">ActorWorker</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Each worker automatically gets environment variables:
# - RANK (0-7), WORLD_SIZE (8)
# - MASTER_ADDR, MASTER_PORT (from register center)
# - LOCAL_RANK, LOCAL_WORLD_SIZE
</span></pre></table></code></div></div><p>The framework automatically injects these environment variables:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">env_vars</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">WORLD_SIZE</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">world_size</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">RANK</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">rank</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">WG_PREFIX</span><span class="sh">"</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">name_prefix</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">WG_BACKEND</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">ray</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">RAY_LOCAL_WORLD_SIZE</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">local_world_size</span><span class="p">),</span>
    <span class="sh">"</span><span class="s">RAY_LOCAL_RANK</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">local_rank</span><span class="p">),</span>
<span class="p">}</span>
<span class="k">if</span> <span class="n">rank</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">env_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">MASTER_ADDR</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_master_addr</span>
    <span class="n">env_vars</span><span class="p">[</span><span class="sh">"</span><span class="s">MASTER_PORT</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_master_port</span>
</pre></table></code></div></div><h4 id="2-smart-data-distribution"><span class="me-2">2. Smart Data Distribution</span><a href="#2-smart-data-distribution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><strong>Without RayWorkerGroup</strong> - manual and error-prone:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="c1"># Manual loop for distributing different shards
</span><span class="n">data_shards</span> <span class="o">=</span> <span class="p">[</span><span class="n">shard0</span><span class="p">,</span> <span class="n">shard1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">shard7</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">worker</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">workers</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">worker</span><span class="p">.</span><span class="n">process</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">data_shards</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>With RayWorkerGroup</strong> - automatic detection:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># Distribute different shards (automatic detection!)
</span><span class="n">data_shards</span> <span class="o">=</span> <span class="p">[</span><span class="n">shard0</span><span class="p">,</span> <span class="n">shard1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">shard7</span><span class="p">]</span>  <span class="c1"># Length matches worker count
</span><span class="n">results</span> <span class="o">=</span> <span class="n">worker_group</span><span class="p">.</span><span class="nf">execute_all_sync</span><span class="p">(</span><span class="sh">"</span><span class="s">process</span><span class="sh">"</span><span class="p">,</span> <span class="n">data_shards</span><span class="p">)</span>

<span class="c1"># Broadcast same data (automatic detection!)
</span><span class="n">results</span> <span class="o">=</span> <span class="n">worker_group</span><span class="p">.</span><span class="nf">execute_all_sync</span><span class="p">(</span><span class="sh">"</span><span class="s">process</span><span class="sh">"</span><span class="p">,</span> <span class="n">same_data</span><span class="p">)</span>
</pre></table></code></div></div><p>The magic happens through automatic inspection:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="n">length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_workers</span><span class="p">)</span>
<span class="k">if</span> <span class="nf">all</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">all</span><span class="p">(</span><span class="nf">isinstance</span><span class="p">(</span><span class="n">kwarg</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="k">for</span> <span class="n">kwarg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">values</span><span class="p">()):</span>
    <span class="k">if</span> <span class="nf">all</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="n">length</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="nf">all</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">kwarg</span><span class="p">)</span> <span class="o">==</span> <span class="n">length</span> <span class="k">for</span> <span class="n">kwarg</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">values</span><span class="p">()):</span>
        <span class="c1"># Split args and kwargs into shards
</span>        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
            <span class="n">sliced_args</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">(</span><span class="n">arg</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>
            <span class="n">sliced_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
            <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_execute_remote_single_worker</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_workers</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">method_name</span><span class="p">,</span> <span class="o">*</span><span class="n">sliced_args</span><span class="p">,</span> <span class="o">**</span><span class="n">sliced_kwargs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">result</span>

<span class="k">return</span> <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">_execute_remote_single_worker</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="k">for</span> <span class="n">worker</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">_workers</span><span class="p">]</span>
</pre></table></code></div></div><h4 id="3-placement-group-management"><span class="me-2">3. Placement Group Management</span><a href="#3-placement-group-management" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><strong>Without RayWorkerGroup</strong> - verbose manual setup:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="c1"># Manually create placement groups for 10 nodes with 8 workers each
</span><span class="kn">from</span> <span class="n">ray.util.placement_group</span> <span class="kn">import</span> <span class="n">placement_group</span>

<span class="n">pg1</span> <span class="o">=</span> <span class="nf">placement_group</span><span class="p">([{</span><span class="sh">"</span><span class="s">GPU</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">CPU</span><span class="sh">"</span><span class="p">:</span> <span class="mi">10</span><span class="p">}]</span> <span class="o">*</span> <span class="mi">8</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">"</span><span class="s">STRICT_PACK</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pg1</span><span class="p">.</span><span class="nf">ready</span><span class="p">())</span>

<span class="n">workers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">ActorWorker</span><span class="p">.</span><span class="nf">options</span><span class="p">(</span>
        <span class="n">scheduling_strategy</span><span class="o">=</span><span class="nc">PlacementGroupSchedulingStrategy</span><span class="p">(</span>
            <span class="n">placement_group</span><span class="o">=</span><span class="n">pg1</span><span class="p">,</span>
            <span class="n">placement_group_bundle_index</span><span class="o">=</span><span class="n">i</span>
        <span class="p">),</span>
        <span class="n">num_gpus</span><span class="o">=</span><span class="mf">0.125</span>  <span class="c1"># 8 workers sharing GPUs
</span>    <span class="p">).</span><span class="nf">remote</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="p">]</span>
<span class="c1"># Repeat for each node... Very tedious!
</span></pre></table></code></div></div><p><strong>With RayWorkerGroup</strong> - declarative specification:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># All placement groups created and workers assigned automatically!
</span><span class="n">resource_pool</span> <span class="o">=</span> <span class="nc">RayResourcePool</span><span class="p">(</span>
    <span class="n">process_on_nodes</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>  <span class="c1"># 10 nodes, 8 workers each
</span>    <span class="n">use_gpu</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">max_colocate_count</span><span class="o">=</span><span class="mi">8</span>  <span class="c1"># 8 workers share 1 GPU
</span><span class="p">)</span>

<span class="n">worker_group</span> <span class="o">=</span> <span class="nc">RayWorkerGroup</span><span class="p">(</span>
    <span class="n">resource_pool</span><span class="o">=</span><span class="n">resource_pool</span><span class="p">,</span>
    <span class="n">ray_cls_with_init</span><span class="o">=</span><span class="n">ray_cls</span>
<span class="p">)</span>
</pre></table></code></div></div><h4 id="4-method-binding-with-dispatchcollect-patterns"><span class="me-2">4. Method Binding with Dispatch/Collect Patterns</span><a href="#4-method-binding-with-dispatchcollect-patterns" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> automatically binds worker class methods and handles common distributed patterns. When you call a method on the worker group, it automatically:</p><ol><li><strong>Dispatches</strong> data appropriately (shard, broadcast, or pass-through)<li><strong>Executes</strong> the method on all workers<li><strong>Collects</strong> and merges results</ol><p>This is powered by the <code class="language-plaintext highlighter-rouge">@register</code> decorator system.</p><h3 id="the-register-decorator-declarative-distributed-execution"><span class="me-2">The @register Decorator: Declarative Distributed Execution</span><a href="#the-register-decorator-declarative-distributed-execution" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The <code class="language-plaintext highlighter-rouge">@register</code> decorator is a declarative system for distributed method execution. It allows you to specify <strong>how</strong> a method should be distributed and executed across workers without writing boilerplate code.</p><h4 id="how-it-works"><span class="me-2">How It Works</span><a href="#how-it-works" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><strong>1. Metadata Attachment</strong></p><p>The decorator attaches metadata to methods:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">register</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="o">=</span><span class="n">Dispatch</span><span class="p">.</span><span class="n">ALL_TO_ALL</span><span class="p">,</span> <span class="n">execute_mode</span><span class="o">=</span><span class="n">Execute</span><span class="p">.</span><span class="n">ALL</span><span class="p">,</span> <span class="n">blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">materialize_futures</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">decorator</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
        <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">materialize_futures</span><span class="p">:</span>
                <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="nf">_materialize_futures</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="nf">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="c1"># Attach metadata via magic attribute
</span>        <span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">dispatch_mode</span><span class="sh">"</span><span class="p">:</span> <span class="n">dispatch_mode</span><span class="p">,</span> <span class="sh">"</span><span class="s">execute_mode</span><span class="sh">"</span><span class="p">:</span> <span class="n">execute_mode</span><span class="p">,</span> <span class="sh">"</span><span class="s">blocking</span><span class="sh">"</span><span class="p">:</span> <span class="n">blocking</span><span class="p">}</span>
        <span class="nf">setattr</span><span class="p">(</span><span class="n">wrapper</span><span class="p">,</span> <span class="n">MAGIC_ATTR</span><span class="p">,</span> <span class="n">attrs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapper</span>
    
    <span class="k">return</span> <span class="n">decorator</span>
</pre></table></code></div></div><p><strong>2. Method Binding Process</strong></p><p>When <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> is initialized, it scans and binds decorated methods:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_bind_worker_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">user_defined_cls</span><span class="p">,</span> <span class="n">func_generator</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">method_name</span> <span class="ow">in</span> <span class="nf">dir</span><span class="p">(</span><span class="n">user_defined_cls</span><span class="p">):</span>
        <span class="n">method</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">user_defined_cls</span><span class="p">,</span> <span class="n">method_name</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">MAGIC_ATTR</span><span class="p">):</span>
            <span class="c1"># Extract configuration
</span>            <span class="n">attribute</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">method</span><span class="p">,</span> <span class="n">MAGIC_ATTR</span><span class="p">)</span>
            <span class="n">dispatch_mode</span> <span class="o">=</span> <span class="n">attribute</span><span class="p">[</span><span class="sh">"</span><span class="s">dispatch_mode</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">execute_mode</span> <span class="o">=</span> <span class="n">attribute</span><span class="p">[</span><span class="sh">"</span><span class="s">execute_mode</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">blocking</span> <span class="o">=</span> <span class="n">attribute</span><span class="p">[</span><span class="sh">"</span><span class="s">blocking</span><span class="sh">"</span><span class="p">]</span>
            
            <span class="c1"># Get dispatch and collect functions
</span>            <span class="n">dispatch_fn</span> <span class="o">=</span> <span class="nf">get_predefined_dispatch_fn</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="p">)[</span><span class="sh">"</span><span class="s">dispatch_fn</span><span class="sh">"</span><span class="p">]</span>
            <span class="n">collect_fn</span> <span class="o">=</span> <span class="nf">get_predefined_dispatch_fn</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="p">)[</span><span class="sh">"</span><span class="s">collect_fn</span><span class="sh">"</span><span class="p">]</span>
            
            <span class="c1"># Get execute function
</span>            <span class="n">execute_fn</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">wg_execute_fn_name</span><span class="p">)</span>
            
            <span class="c1"># Bind method to WorkerGroup
</span>            <span class="n">func</span> <span class="o">=</span> <span class="nf">func_generator</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">dispatch_fn</span><span class="p">,</span> <span class="n">collect_fn</span><span class="p">,</span> <span class="n">execute_fn</span><span class="p">,</span> <span class="n">blocking</span><span class="p">)</span>
            <span class="nf">setattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>3. Execution Flow</strong></p><p>When you call a bound method, it follows this pipeline:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">func_generator</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">dispatch_fn</span><span class="p">,</span> <span class="n">collect_fn</span><span class="p">,</span> <span class="n">execute_fn</span><span class="p">,</span> <span class="n">blocking</span><span class="p">):</span>
    <span class="k">class</span> <span class="nc">Functor</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="n">this</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="c1"># Step 1: Dispatch - transform/distribute input data
</span>            <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="o">=</span> <span class="nf">dispatch_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            
            <span class="c1"># Step 2: Execute - run method on workers
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">execute_fn</span><span class="p">(</span><span class="n">method_name</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            
            <span class="c1"># Step 3: Block (optional) - wait for results
</span>            <span class="k">if</span> <span class="n">blocking</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">ray</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            
            <span class="c1"># Step 4: Collect - gather and merge results
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">collect_fn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
            
            <span class="k">return</span> <span class="n">output</span>
    
    <span class="k">return</span> <span class="nc">Functor</span><span class="p">()</span>
</pre></table></code></div></div><h4 id="common-dispatch-modes"><span class="me-2">Common Dispatch Modes</span><a href="#common-dispatch-modes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><strong>Dispatch.ONE_TO_ALL</strong> - Broadcast same arguments to all workers:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nd">@register</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="o">=</span><span class="n">Dispatch</span><span class="p">.</span><span class="n">ONE_TO_ALL</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">init_model</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="c1"># Load model weights - same operation on all workers
</span>    <span class="k">pass</span>

<span class="c1"># Implementation
</span><span class="k">def</span> <span class="nf">dispatch_one_to_all</span><span class="p">(</span><span class="n">worker_group</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">args</span> <span class="o">=</span> <span class="nf">tuple</span><span class="p">([</span><span class="n">arg</span><span class="p">]</span> <span class="o">*</span> <span class="n">worker_group</span><span class="p">.</span><span class="n">world_size</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span>
    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">*</span> <span class="n">worker_group</span><span class="p">.</span><span class="n">world_size</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">items</span><span class="p">()}</span>
    <span class="k">return</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span>
</pre></table></code></div></div><p><strong>Dispatch.DP_COMPUTE_PROTO</strong> - Shard data across workers (data parallel):</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="nd">@register</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="o">=</span><span class="n">Dispatch</span><span class="p">.</span><span class="n">DP_COMPUTE_PROTO</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">update_actor</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">DataProto</span><span class="p">):</span>
    <span class="c1"># Each worker gets a different shard of data
</span>    <span class="k">pass</span>

<span class="c1"># Automatically shards DataProto and concatenates results
</span></pre></table></code></div></div><p><strong>Dispatch.ALL_TO_ALL</strong> - Pass-through (manual data distribution):</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nd">@register</span><span class="p">(</span><span class="n">dispatch_mode</span><span class="o">=</span><span class="n">Dispatch</span><span class="p">.</span><span class="n">ALL_TO_ALL</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">custom_method</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">already_sharded_data</span><span class="p">):</span>
    <span class="c1"># Data already distributed manually
</span>    <span class="k">pass</span>
</pre></table></code></div></div><h4 id="why-this-design"><span class="me-2">Why This Design?</span><a href="#why-this-design" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">@register</code> decorator system provides:</p><ol><li><strong>Declarative</strong>: Specify “what” (DP_COMPUTE_PROTO), not “how” to shard<li><strong>Consistent</strong>: All methods follow the same pattern<li><strong>Type-safe</strong>: Enforced dispatch/collect pairing<li><strong>Extensible</strong>: Can register custom dispatch modes<li><strong>Clean code</strong>: No boilerplate in worker methods</ol><p>Without this system, every method would need manual sharding, dispatch, collect, and merge logic. The decorator makes distributed execution transparent and automatic.</p><hr /><p>With these building blocks understood, let’s explore the core components that use them.</p><h2 id="core-components"><span class="me-2">Core Components</span><a href="#core-components" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="actorrolloutrefworker"><span class="me-2">ActorRolloutRefWorker</span><a href="#actorrolloutrefworker" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">ActorRolloutRefWorker</code> is a versatile hybrid engine that can serve three roles:</p><ul><li><strong>Actor</strong>: The policy being learned (runs forward, backward, and parameter updates)<li><strong>Rollout</strong>: Pure inference engine (forward only, generates training sequences)<li><strong>Reference</strong>: Frozen copy of the initial policy (for KL divergence penalty computation)</ul><p>This flexibility is crucial for efficient resource utilization in distributed RL training, where the same model infrastructure can serve different purposes.</p><h4 id="device-mesh-configuration"><span class="me-2">Device Mesh Configuration</span><a href="#device-mesh-configuration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The initialization creates device meshes for different parallelism strategies:</p><p><strong>1. FSDP Device Mesh</strong> - For distributed training:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">create_device_mesh</span><span class="p">(</span><span class="n">world_size</span><span class="p">,</span> <span class="n">fsdp_size</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">fsdp_size</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">fsdp_size</span> <span class="o">&gt;=</span> <span class="n">world_size</span><span class="p">:</span>
        <span class="c1"># Pure FSDP (all processes participate in sharding)
</span>        <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span><span class="n">device_name</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">fsdp</span><span class="sh">"</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Hybrid: DDP (across nodes) + FSDP (within nodes)
</span>        <span class="n">device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span>
            <span class="n">device_name</span><span class="p">,</span> <span class="n">mesh_shape</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span> <span class="o">//</span> <span class="n">fsdp_size</span><span class="p">,</span> <span class="n">fsdp_size</span><span class="p">),</span> <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">ddp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fsdp</span><span class="sh">"</span><span class="p">]</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">device_mesh</span>
</pre></table></code></div></div><p><strong>2. Ulysses Sequence Parallel Device Mesh</strong> - For KV cache optimization:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="c1"># Ulysses sequence parallelism distributes the sequence dimension across GPUs
# This reduces KV cache memory, which grows linearly with sequence length
</span><span class="n">self</span><span class="p">.</span><span class="n">ulysses_sequence_parallel_size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">actor</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">ulysses_sequence_parallel_size</span><span class="sh">"</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">dp</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">ulysses_sequence_parallel_size</span>

<span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">ulysses_sequence_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">ulysses_device_mesh</span> <span class="o">=</span> <span class="nf">init_device_mesh</span><span class="p">(</span>
        <span class="n">device_name</span><span class="p">,</span> 
        <span class="n">mesh_shape</span><span class="o">=</span><span class="p">(</span><span class="n">dp</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">ulysses_sequence_parallel_size</span><span class="p">),</span> 
        <span class="n">mesh_dim_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">dp</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sp</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="n">ulysses_sharding_manager</span> <span class="o">=</span> <span class="nc">FSDPUlyssesShardingManager</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">ulysses_device_mesh</span><span class="p">)</span>
</pre></table></code></div></div><p>The benefit of Ulysses sequence parallelism is KV cache memory reduction. Since KV cache grows linearly with sequence length, distributing it across multiple GPUs enables training with longer sequences.</p><h4 id="model-and-optimizer-initialization"><span class="me-2">Model and Optimizer Initialization</span><a href="#model-and-optimizer-initialization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">_build_model_optimizer</code> method initializes the model using HuggingFace APIs and sets up the optimizer. It’s called within <code class="language-plaintext highlighter-rouge">init_model</code>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="c1"># Model initialization with Flash Attention 2
</span><span class="n">actor_model_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">local_path</span><span class="p">,</span> 
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span> 
    <span class="n">attn_implementation</span><span class="o">=</span><span class="sh">"</span><span class="s">flash_attention_2</span><span class="sh">"</span>
<span class="p">)</span>

<span class="n">actor_module</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span>
    <span class="n">pretrained_model_name_or_path</span><span class="o">=</span><span class="n">local_path</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch_dtype</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">actor_model_config</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
<span class="p">)</span>
</pre></table></code></div></div><h4 id="fsdp2-configuration"><span class="me-2">FSDP2 Configuration</span><a href="#fsdp2-configuration" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>After model initialization, FSDP2 is applied for distributed training. The sharding strategy depends on the device mesh dimensionality:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">get_sharding_strategy</span><span class="p">(</span><span class="n">device_mesh</span><span class="p">):</span>
    <span class="kn">from</span> <span class="n">torch.distributed.fsdp</span> <span class="kn">import</span> <span class="n">ShardingStrategy</span>
    
    <span class="k">if</span> <span class="n">device_mesh</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># Pure FSDP: shard across all processes
</span>        <span class="n">sharding_strategy</span> <span class="o">=</span> <span class="n">ShardingStrategy</span><span class="p">.</span><span class="n">FULL_SHARD</span>
    <span class="k">elif</span> <span class="n">device_mesh</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="c1"># Hybrid: DDP across first dimension, FSDP within second dimension
</span>        <span class="n">sharding_strategy</span> <span class="o">=</span> <span class="n">ShardingStrategy</span><span class="p">.</span><span class="n">HYBRID_SHARD</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Device mesh ndim=</span><span class="si">{</span><span class="n">device_mesh</span><span class="p">.</span><span class="n">ndim</span><span class="si">}</span><span class="s">, but only support 1 or 2</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sharding_strategy</span>
</pre></table></code></div></div><p>For FSDP2, mixed precision and CPU offload policies are configured:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="n">mp_policy</span> <span class="o">=</span> <span class="nc">MixedPrecisionPolicy</span><span class="p">(</span>
    <span class="n">param_dtype</span><span class="o">=</span><span class="n">param_dtype</span><span class="p">,</span> 
    <span class="n">reduce_dtype</span><span class="o">=</span><span class="n">reduce_dtype</span><span class="p">,</span> 
    <span class="n">cast_forward_inputs</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Critical: CPU offload handling
# - Reference model: Always use CPU offload to save memory
# - Actor model: NEVER use CPU offload when using gradient accumulation
#   Why? FSDP's CPU-&gt;GPU copy creates clean parameter copies, losing grad info!
</span><span class="n">cpu_offload</span> <span class="o">=</span> <span class="bp">None</span> <span class="k">if</span> <span class="n">role</span> <span class="o">==</span> <span class="sh">"</span><span class="s">actor</span><span class="sh">"</span> <span class="k">else</span> <span class="nc">CPUOffloadPolicy</span><span class="p">(</span><span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">fsdp_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">mesh</span><span class="sh">"</span><span class="p">:</span> <span class="n">fsdp_mesh</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">mp_policy</span><span class="sh">"</span><span class="p">:</span> <span class="n">mp_policy</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">offload_policy</span><span class="sh">"</span><span class="p">:</span> <span class="n">cpu_offload</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">reshard_after_forward</span><span class="sh">"</span><span class="p">:</span> <span class="n">fsdp_config</span><span class="p">.</span><span class="n">reshard_after_forward</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># Apply FSDP2 and load state dict
</span><span class="n">full_state</span> <span class="o">=</span> <span class="n">actor_module</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
<span class="nf">apply_fsdp2</span><span class="p">(</span><span class="n">actor_module</span><span class="p">,</span> <span class="n">fsdp_kwargs</span><span class="p">,</span> <span class="n">fsdp_config</span><span class="p">)</span>
<span class="nf">fsdp2_load_full_state_dict</span><span class="p">(</span><span class="n">actor_module</span><span class="p">,</span> <span class="n">full_state</span><span class="p">,</span> <span class="n">fsdp_mesh</span><span class="p">,</span> <span class="n">cpu_offload</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>Optimizer</strong>: Standard AdamW is used:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">actor_optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span>
    <span class="n">actor_module_fsdp</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">lr</span><span class="o">=</span><span class="n">optim_config</span><span class="p">.</span><span class="n">lr</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="n">optim_config</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">betas</span><span class="sh">"</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)),</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="n">optim_config</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">weight_decay</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">),</span>
<span class="p">)</span>
</pre></table></code></div></div><h4 id="rollout-setup-with-vllm"><span class="me-2">Rollout Setup with vLLM</span><a href="#rollout-setup-with-vllm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">_build_rollout</code> method sets up the inference engine. The core components are <code class="language-plaintext highlighter-rouge">vLLMRollout</code> and <code class="language-plaintext highlighter-rouge">FSDPVLLMShardingManager</code>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="c1"># Initialize vLLM rollout engine (sync or async)
</span><span class="n">vllm_rollout_cls</span> <span class="o">=</span> <span class="n">vLLMRollout</span> <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="n">mode</span> <span class="o">==</span> <span class="sh">"</span><span class="s">sync</span><span class="sh">"</span> <span class="k">else</span> <span class="n">vLLMAsyncRollout</span>
<span class="n">rollout</span> <span class="o">=</span> <span class="nf">vllm_rollout_cls</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">local_path</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">model_hf_config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_model_config</span><span class="p">,</span>
    <span class="n">device_mesh</span><span class="o">=</span><span class="n">rollout_device_mesh</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
    <span class="o">**</span><span class="n">lora_kwargs</span><span class="p">,</span>  <span class="c1"># Optional LoRA configuration
</span><span class="p">)</span>

<span class="c1"># Create sharding manager to sync FSDP training params with vLLM inference params
</span><span class="n">rollout_sharding_manager</span> <span class="o">=</span> <span class="nc">FSDPVLLMShardingManager</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_module_fsdp</span><span class="p">,</span>
    <span class="n">inference_engine</span><span class="o">=</span><span class="n">rollout</span><span class="p">.</span><span class="n">inference_engine</span><span class="p">,</span>
    <span class="n">model_config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_model_config</span><span class="p">,</span>
    <span class="n">rollout_config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">,</span>
    <span class="n">full_params</span><span class="o">=</span><span class="n">full_params</span><span class="p">,</span>
    <span class="n">device_mesh</span><span class="o">=</span><span class="n">rollout_device_mesh</span><span class="p">,</span>
    <span class="n">offload_param</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">_is_offload_param</span><span class="p">,</span>
    <span class="n">load_format</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="n">load_format</span><span class="p">,</span>
    <span class="n">layered_summon</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">layered_summon</span><span class="sh">"</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>Important</strong>: The rollout device mesh differs from the actor training device mesh:</p><ul><li><strong>Actor training</strong>: All processes participate in FSDP parameter sharding<li><strong>Rollout inference</strong>: Uses tensor parallelism (TP) for fast inference, with data parallelism (DP) across TP groups</ul><p>This design optimizes for different objectives: memory efficiency during training vs. throughput during inference.</p><h4 id="actor-initialization"><span class="me-2">Actor Initialization</span><a href="#actor-initialization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>If the worker serves as an actor, it creates a <code class="language-plaintext highlighter-rouge">DataParallelPPOActor</code> as the actual trainer:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">_is_actor</span><span class="p">:</span>
    <span class="n">self</span><span class="p">.</span><span class="n">actor</span> <span class="o">=</span> <span class="nc">DataParallelPPOActor</span><span class="p">(</span>
        <span class="n">config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">actor</span><span class="p">,</span> 
        <span class="n">actor_module</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_module_fsdp</span><span class="p">,</span> 
        <span class="n">actor_optimizer</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_optimizer</span>
    <span class="p">)</span>
</pre></table></code></div></div><h3 id="dataparallelppoactor"><span class="me-2">DataParallelPPOActor</span><a href="#dataparallelppoactor" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">DataParallelPPOActor</code> serves as the actual trainer for the actor model. It can function as either an actor or a reference policy, depending on whether an optimizer is provided during initialization.</p><h4 id="forward-pass-computing-log-probabilities"><span class="me-2">Forward Pass: Computing Log Probabilities</span><a href="#forward-pass-computing-log-probabilities" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">_forward_micro_batch</code> method computes log probabilities for generated responses:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1"># Shift input_ids by 1 (standard autoregressive LM training)
# inplace_backward is a memory optimization - logits aren't needed after backward pass
</span><span class="n">log_probs</span> <span class="o">=</span> <span class="nf">logprobs_from_logits</span><span class="p">(</span>
    <span class="n">logits</span><span class="o">=</span><span class="n">logits_rmpad</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="n">input_ids_rmpad_rolled</span><span class="p">,</span>
    <span class="n">inplace_backward</span><span class="o">=</span><span class="n">inplace_backward</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Extract log probs for response tokens only
</span><span class="n">log_probs</span> <span class="o">=</span> <span class="n">full_log_probs</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[:,</span> <span class="o">-</span><span class="n">response_length</span> <span class="o">-</span> <span class="mi">1</span> <span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># (bsz, response_length)
</span></pre></table></code></div></div><p><strong>Flash Attention Optimization</strong>: When <code class="language-plaintext highlighter-rouge">use_remove_padding=True</code>, the attention mask is set to <code class="language-plaintext highlighter-rouge">None</code> and Flash Attention uses <code class="language-plaintext highlighter-rouge">cu_seqlens</code> for efficient computation. This is enabled via monkey patching:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nf">apply_monkey_patch</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">actor_module</span><span class="p">,</span>
    <span class="n">use_remove_padding</span><span class="o">=</span><span class="n">use_remove_padding</span><span class="p">,</span>
    <span class="n">ulysses_sp_size</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">ulysses_sequence_parallel_size</span><span class="p">,</span>
    <span class="n">use_fused_kernels</span><span class="o">=</span><span class="n">use_fused_kernels</span><span class="p">,</span>
    <span class="n">fused_kernels_backend</span><span class="o">=</span><span class="n">fused_kernels_backend</span><span class="p">,</span>
<span class="p">)</span>
</pre></table></code></div></div><h4 id="dynamic-batching-for-load-balancing"><span class="me-2">Dynamic Batching for Load Balancing</span><a href="#dynamic-batching-for-load-balancing" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">compute_log_prob</code> method uses dynamic batching to balance computational workload. The core logic is in <code class="language-plaintext highlighter-rouge">verl.utils.seqlen_balancing.rearrange_micro_batches</code>:</p><p><strong>Key features</strong>:</p><ol><li><strong>Token-based splitting</strong>: Split batch into micro-batches by total token count (not just batch size)<li><strong>Karmarkar-Karp Algorithm</strong>: Balance sequence lengths across micro-batches<li><strong>Workload approximation</strong>: Sort by sum of squared sequence lengths (approximates attention $O(n^2)$ complexity)<li><strong>DP synchronization</strong>: Ensure all DP ranks use the same number of micro-batches</ol><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="c1"># Calculate effective sequence lengths from attention mask (due to padding)
</span><span class="n">seq_len_effective</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">attention_mask</span><span class="sh">"</span><span class="p">].</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">total_seqlen</span> <span class="o">=</span> <span class="n">seq_len_effective</span><span class="p">.</span><span class="nf">sum</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>

<span class="c1"># Determine number of micro-batches based on max_token_len
</span><span class="n">num_micro_batches</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">seq_len_effective</span><span class="p">),</span> <span class="nf">ceildiv</span><span class="p">(</span><span class="n">total_seqlen</span><span class="p">,</span> <span class="n">max_token_len</span><span class="p">))</span>

<span class="c1"># Synchronize across DP ranks to prevent out-of-sync
</span><span class="k">if</span> <span class="n">dist</span><span class="p">.</span><span class="nf">is_initialized</span><span class="p">()</span> <span class="ow">and</span> <span class="n">same_micro_num_in_dp</span><span class="p">:</span>
    <span class="n">num_micro_batches</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">num_micro_batches</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="nf">get_device_name</span><span class="p">())</span>
    <span class="n">dist</span><span class="p">.</span><span class="nf">all_reduce</span><span class="p">(</span><span class="n">num_micro_batches</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">MAX</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">dp_group</span><span class="p">)</span>

<span class="c1"># Use Karmarkar-Karp Algorithm for balanced partitioning
</span><span class="n">micro_bsz_idx</span> <span class="o">=</span> <span class="nf">get_seqlen_balanced_partitions</span><span class="p">(</span><span class="n">seq_len_effective</span><span class="p">,</span> <span class="n">num_micro_batches</span><span class="p">,</span> <span class="n">equal_size</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Sort by computational workload (sum of squared sequence lengths approximates O(n²) attention)
</span><span class="k">if</span> <span class="n">use_dynamic_bsz_balance</span><span class="p">:</span>
    <span class="n">micro_bsz_idx</span><span class="p">.</span><span class="nf">sort</span><span class="p">(</span>
        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">partition</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">seq_len_effective</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">partition</span><span class="p">),</span>
        <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="p">)</span>

<span class="c1"># Create micro-batches by reassembling samples
</span><span class="k">for</span> <span class="n">partition</span> <span class="ow">in</span> <span class="n">micro_bsz_idx</span><span class="p">:</span>
    <span class="n">curr_micro_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">batch</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">partition</span><span class="p">])</span>
    <span class="n">micro_batches</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">curr_micro_batch</span><span class="p">)</span>
</pre></table></code></div></div><p>After batching, <code class="language-plaintext highlighter-rouge">_forward_micro_batch</code> performs the actual computation.</p><h4 id="policy-update-ppo-training-loop"><span class="me-2">Policy Update: PPO Training Loop</span><a href="#policy-update-ppo-training-loop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">update_policy</code> method implements the core PPO algorithm training logic:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="c1"># Split batch into mini-batches for multiple PPO epochs
# See PPO paper: https://arxiv.org/abs/1707.06347
</span><span class="n">mini_batches</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">ppo_mini_batch_size</span><span class="p">)</span>

<span class="k">for</span> <span class="n">mini_batch</span> <span class="ow">in</span> <span class="n">mini_batches</span><span class="p">:</span>
    <span class="c1"># Further split mini-batch into micro-batches for gradient accumulation
</span>    <span class="n">micro_batches</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">ppo_micro_batch_size_per_gpu</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">micro_batch</span> <span class="ow">in</span> <span class="n">micro_batches</span><span class="p">:</span>
        <span class="c1"># 1. Compute policy loss (PPO clipped objective)
</span>        <span class="n">pg_loss</span><span class="p">,</span> <span class="n">clipfrac</span><span class="p">,</span> <span class="n">ppo_kl</span><span class="p">,</span> <span class="n">clipfrac_lower</span> <span class="o">=</span> <span class="nf">compute_policy_loss</span><span class="p">(...)</span>
        
        <span class="c1"># 2. Compute KL penalty from reference policy
</span>        <span class="n">kl_penalty</span> <span class="o">=</span> <span class="nf">compute_kl_penalty</span><span class="p">(...)</span>
        
        <span class="c1"># 3. Total loss and backward
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">pg_loss</span> <span class="o">+</span> <span class="n">kl_penalty</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    
    <span class="c1"># 4. Optimizer step after accumulating gradients
</span>    <span class="n">self</span><span class="p">.</span><span class="nf">_optimizer_step</span><span class="p">()</span>
</pre></table></code></div></div><h4 id="ppo-loss-computation"><span class="me-2">PPO Loss Computation</span><a href="#ppo-loss-computation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The most challenging aspect of RL is understanding the loss formulations. For PPO, the implementation is in <code class="language-plaintext highlighter-rouge">verl.trainer.ppo.core_algos.compute_policy_loss</code>:</p><p><strong>Mathematical Formulation</strong>:</p><p>The PPO loss with dual clipping is:</p>\[L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t)]\]<p>where:</p><ul><li>$r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_\text{old}}(a_t \mid s_t)}$ is the probability ratio<li>$A_t$ is the advantage estimate<li>$\epsilon$ is the clip range (typically 0.2)</ul><p>For dual-clip PPO (to maintain exploration), an additional constraint is added:</p>\[L^{DUAL}(\theta) = \mathbb{E}_t[\max(L^{CLIP}(\theta), c \cdot A_t)] \text{ when } A_t &lt; 0\]<p><strong>Implementation</strong>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="c1"># Compute probability ratio
</span><span class="n">negative_approx_kl</span> <span class="o">=</span> <span class="n">log_prob</span> <span class="o">-</span> <span class="n">old_log_prob</span>
<span class="n">negative_approx_kl</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">negative_approx_kl</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">20.0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">20.0</span><span class="p">)</span>  <span class="c1"># Stability
</span><span class="n">ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">negative_approx_kl</span><span class="p">)</span>

<span class="c1"># Standard PPO clipping
</span><span class="n">pg_losses1</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">ratio</span>  <span class="c1"># Unclipped objective
</span><span class="n">pg_losses2</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">cliprange_low</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">cliprange_high</span><span class="p">)</span>  <span class="c1"># Clipped
</span><span class="n">clip_pg_losses1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">pg_losses1</span><span class="p">,</span> <span class="n">pg_losses2</span><span class="p">)</span>

<span class="c1"># Dual-clip PPO: Prevent overly small ratio for negative advantages
# This maintains exploration by preventing the policy from becoming too deterministic
</span><span class="n">pg_losses3</span> <span class="o">=</span> <span class="o">-</span><span class="n">advantages</span> <span class="o">*</span> <span class="n">clip_ratio_c</span>  <span class="c1"># clip_ratio_c typically = 3.0
</span><span class="n">clip_pg_losses2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">pg_losses3</span><span class="p">,</span> <span class="n">clip_pg_losses1</span><span class="p">)</span>

<span class="c1"># Apply dual-clip only for negative advantages
</span><span class="n">pg_losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">advantages</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">clip_pg_losses2</span><span class="p">,</span> <span class="n">clip_pg_losses1</span><span class="p">)</span>
<span class="n">pg_loss</span> <span class="o">=</span> <span class="nf">agg_loss</span><span class="p">(</span><span class="n">loss_mat</span><span class="o">=</span><span class="n">pg_losses</span><span class="p">,</span> <span class="n">loss_mask</span><span class="o">=</span><span class="n">response_mask</span><span class="p">,</span> <span class="n">loss_agg_mode</span><span class="o">=</span><span class="n">loss_agg_mode</span><span class="p">)</span>

<span class="k">return</span> <span class="n">pg_loss</span><span class="p">,</span> <span class="n">pg_clipfrac</span><span class="p">,</span> <span class="n">ppo_kl</span><span class="p">,</span> <span class="n">pg_clipfrac_lower</span>
</pre></table></code></div></div><p><strong>Key insights</strong>:</p><ul><li>Standard PPO clipping prevents large policy updates<li>Dual-clip PPO (for negative advantages) prevents policy from becoming too deterministic<li>The <code class="language-plaintext highlighter-rouge">response_mask</code> ensures only response tokens contribute to the loss (not prompt tokens)</ul><h3 id="rayppotrainer"><span class="me-2">RayPPOTrainer</span><a href="#rayppotrainer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">RayPPOTrainer</code> orchestrates the entire PPO training process. It manages resource allocation, coordinates different worker groups (actor, critic, reward), and implements the main training loop.</p><h4 id="resource-management"><span class="me-2">Resource Management</span><a href="#resource-management" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><code class="language-plaintext highlighter-rouge">RayResourcePool</code> manages worker scheduling and allocation across GPU nodes using Ray Placement Groups. This ensures:</p><ul><li>Different actors get appropriate resources<li>Co-location of compatible roles for efficiency (multiple roles can share the same resource pool)</ul><h4 id="main-training-loop"><span class="me-2">Main Training Loop</span><a href="#main-training-loop" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">fit</code> method implements the core training loop:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="n">self</span><span class="p">.</span><span class="nf">_load_checkpoint</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">train_dataloader</span><span class="p">:</span>
        <span class="c1"># 1. Generate sequences using current policy
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">generate_sequence</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        
        <span class="c1"># 2. Compute rewards
</span>        <span class="n">reward_tensor</span><span class="p">,</span> <span class="n">reward_extra_infos</span> <span class="o">=</span> <span class="nf">compute_reward</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">reward_fn</span><span class="p">)</span>
        
        <span class="c1"># 3. Compute log probabilities from old and reference policies
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">compute_log_prob</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>  <span class="c1"># old policy log probs
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">compute_ref_log_prob</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>  <span class="c1"># ref policy log probs
</span>        
        <span class="c1"># 4. Compute advantages using GAE or other estimators
</span>        <span class="n">batch</span> <span class="o">=</span> <span class="nf">compute_advantage</span><span class="p">(</span>
            <span class="n">batch</span><span class="p">,</span>
            <span class="n">adv_estimator</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">algorithm</span><span class="p">.</span><span class="n">adv_estimator</span><span class="p">,</span>
            <span class="n">gamma</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">algorithm</span><span class="p">.</span><span class="n">gamma</span><span class="p">,</span>
            <span class="n">lam</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">algorithm</span><span class="p">.</span><span class="n">lam</span><span class="p">,</span>
        <span class="p">)</span>
        
        <span class="c1"># 5. Update critic (if using value function)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">use_critic</span><span class="p">:</span>
            <span class="n">critic_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">critic_wg</span><span class="p">.</span><span class="nf">update_critic</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        
        <span class="c1"># 6. Update actor (after critic warmup)
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">global_steps</span> <span class="o">&gt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">trainer</span><span class="p">.</span><span class="n">critic_warmup</span><span class="p">:</span>
            <span class="n">actor_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">actor_rollout_wg</span><span class="p">.</span><span class="nf">update_actor</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</pre></table></code></div></div><p>The trainer elegantly separates concerns through the <code class="language-plaintext highlighter-rouge">RayWorkerGroup</code> abstraction, making the training loop clean and maintainable.</p><h3 id="vllmrollout"><span class="me-2">vLLMRollout</span><a href="#vllmrollout" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">vLLMRollout</code> integrates vLLM as the inference engine for efficient sequence generation during rollouts.</p><h4 id="initialization"><span class="me-2">Initialization</span><a href="#initialization" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>For tensor parallelism (TP), vLLM setup varies by backend:</p><p><strong>Megatron backend</strong>: Reuses existing process groups</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">vllm.distributed</span> <span class="kn">import</span> <span class="n">parallel_state</span> <span class="k">as</span> <span class="n">vllm_ps</span>

<span class="k">if</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">train_tp</span><span class="sh">"</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Reuse the process group already created by Megatron
</span>    <span class="n">vllm_ps</span><span class="p">.</span><span class="nf">initialize_model_parallel</span><span class="p">(</span><span class="n">tensor_model_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>FSDP backend</strong>: Delegates to vLLM’s <code class="language-plaintext highlighter-rouge">LLM</code> class for process group initialization</p><h4 id="sequence-generation"><span class="me-2">Sequence Generation</span><a href="#sequence-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">generate_sequences</code> method is the main entry point for rollout generation:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inference_engine</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">prompts</span><span class="o">=</span><span class="n">vllm_inputs</span><span class="p">,</span>  <span class="c1"># Already converted to token IDs
</span>    <span class="n">sampling_params</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sampling_params</span><span class="p">,</span>
    <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_requests</span><span class="p">,</span>
    <span class="n">use_tqdm</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>Important padding convention</strong>:</p><ul><li><strong>Prompts</strong>: Left-padded (for batch inference efficiency)<li><strong>Responses</strong>: Right-padded (standard autoregressive generation)</ul><h3 id="fsdpvllmshardingmanager"><span class="me-2">FSDPVLLMShardingManager</span><a href="#fsdpvllmshardingmanager" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The sharding manager is responsible for synchronizing FSDP training parameters with vLLM inference parameters. This is crucial in RL training because:</p><ol><li><strong>FSDP parameters are distributed</strong> across all ranks (each rank only has a shard)<li><strong>vLLM needs full or TP-sharded parameters</strong> for inference<li><strong>Synchronization timing</strong> affects both correctness and efficiency</ol><h4 id="parameter-collection"><span class="me-2">Parameter Collection</span><a href="#parameter-collection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The manager collects parameters from FSDP, handling both full model and LoRA cases:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="c1"># Extract the actual model from FSDP wrapper
</span><span class="n">peft_model</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">module</span><span class="p">,</span> <span class="sh">"</span><span class="s">_fsdp_wrapped_module</span><span class="sh">"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">module</span><span class="p">)</span>

<span class="c1"># Check if using LoRA (Parameter-Efficient Fine-Tuning)
</span><span class="k">if</span> <span class="nf">hasattr</span><span class="p">(</span><span class="n">peft_model</span><span class="p">,</span> <span class="sh">"</span><span class="s">peft_config</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">peft_config</span> <span class="o">=</span> <span class="n">peft_model</span><span class="p">.</span><span class="n">peft_config</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">default</span><span class="sh">"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="nf">__collect_lora_params</span><span class="p">()</span>  <span class="c1"># Only collect LoRA adapter weights
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">module</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>  <span class="c1"># Collect full model weights
</span>
<span class="c1"># Convert weight keys to match vLLM's expected format
</span><span class="n">params</span> <span class="o">=</span> <span class="nf">convert_weight_keys</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">peft_model</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="parameter-update"><span class="me-2">Parameter Update</span><a href="#parameter-update" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <code class="language-plaintext highlighter-rouge">update_params</code> method loads parameters into vLLM’s inference engine. The key challenge is <strong>gathering sharded parameters</strong>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">model_runner</span><span class="p">.</span><span class="n">model</span>

<span class="c1"># DTensor.full_tensor() gathers sharded parameters from all ranks
</span><span class="n">loaded_params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">load_weights</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">param</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">full_tensor</span><span class="p">()</span> <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">DTensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">param</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">updated_params</span><span class="p">.</span><span class="nf">items</span><span class="p">()</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>Why DTensor?</strong> In FSDP2, parameters are represented as <code class="language-plaintext highlighter-rouge">DTensor</code> (distributed tensor). Calling <code class="language-plaintext highlighter-rouge">.full_tensor()</code> performs an all-gather to reconstruct the full parameter, which vLLM needs for inference.</p><p><strong>Performance consideration</strong>: The all-gather is expensive but necessary. VERL optimizes this with:</p><ul><li><strong>Layered summoning</strong>: Gather parameters layer-by-layer to reduce peak memory<li><strong>Selective updates</strong>: Only update changed parameters when possible</ul><h3 id="onerecactorrolloutrefworker"><span class="me-2">OneRecActorRolloutRefWorker</span><a href="#onerecactorrolloutrefworker" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>This is the key part how OpenOneRec implement RL for their post training. This could be used as a reference how to adopt RL for recommendation task.</p><p>OpenOneRec’s customized worker extends <code class="language-plaintext highlighter-rouge">ActorRolloutRefWorker</code> to integrate the two-stage generation approach. The main change is overriding <code class="language-plaintext highlighter-rouge">_build_rollout</code>:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_build_rollout</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="c1"># Override to use OneRecvLLMRollout instead of standard vLLMRollout
</span>    <span class="n">rollout</span> <span class="o">=</span> <span class="nc">OneRecvLLMRollout</span><span class="p">(</span>
        <span class="n">model_path</span><span class="o">=</span><span class="n">local_path</span><span class="p">,</span>
        <span class="n">config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">rollout</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">model_hf_config</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">actor_model_config</span><span class="p">,</span>
        <span class="n">device_mesh</span><span class="o">=</span><span class="n">rollout_device_mesh</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="o">=</span><span class="n">trust_remote_code</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">rollout</span>
</pre></table></code></div></div><p>This clean extension demonstrates VERL’s extensibility - you can customize inference behavior without modifying the core training infrastructure.</p><h3 id="onerecvllmrollout-two-stage-generation"><span class="me-2">OneRecvLLMRollout: Two-Stage Generation</span><a href="#onerecvllmrollout-two-stage-generation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><code class="language-plaintext highlighter-rouge">OneRecvLLMRollout</code> implements OpenOneRec’s two-stage generation approach:</p><ol><li><strong>Stage 1</strong>: Generate Chain-of-Thought (CoT) reasoning<li><strong>Stage 2</strong>: Generate item ID sequences using beam search</ol><p>This approach allows the model to first reason about the recommendation task before producing the actual item IDs, potentially improving recommendation quality.</p><h4 id="two-stage-generation-flow"><span class="me-2">Two-Stage Generation Flow</span><a href="#two-stage-generation-flow" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre>┌──────────────────────┐                    ┌─────────────┐
│ OneRecvLLMRollout    │                    │ vLLM Engine │
└──────────┬───────────┘                    └──────┬──────┘
           │                                       │
           │ ═══════ Stage 1: CoT Generation  ═════│
           │                                       │
      1.   │─── generate<span class="o">(</span>prompt, <span class="nv">stop</span><span class="o">=</span><span class="s2">"&lt;/think&gt;"</span><span class="o">)</span> ─&gt;
           │                                       │
      2.   │&lt;──────── CoT reasoning tokens ────────│
           │                                       │
           │                                       │
           │ ═══════ Prepare Stage 2 Prompt  ══════│
           │                                       │
      3.   │─ prompt + CoT + <span class="s2">"</span><span class="se">\n</span><span class="s2">&lt;|sid_begin|&gt;"</span>     │
           │                                       │
           │                                       │
           │ ═══ Stage 2: Item ID Generation  ═════│
           │                                       │
      4.   │─── beam_search<span class="o">(</span>stage2_prompt, N<span class="o">)</span>  ───&gt;│
           │                                       │
      5.   │&lt;──── N candidate item sequences ──────│
           │                                       │
      6.   │─ Select and format output             │
           │                                       │
           ▼                                       ▼
</pre></table></code></div></div><h4 id="stage-1-cot-sampling"><span class="me-2">Stage 1: CoT Sampling</span><a href="#stage-1-cot-sampling" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The first stage generates reasoning with a stop token:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="c1"># Configure sampling for CoT generation
</span><span class="n">stage1_max_tokens</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">stage1_max_tokens</span><span class="sh">"</span><span class="p">,</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">config</span><span class="p">,</span> <span class="sh">"</span><span class="s">stage1_max_tokens</span><span class="sh">"</span><span class="p">,</span> <span class="mi">1024</span><span class="p">))</span>

<span class="n">cot_sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span>
    <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Generate 1 CoT per prompt
</span>    <span class="n">temperature</span><span class="o">=</span><span class="n">kwargs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="n">top_p</span><span class="o">=</span><span class="n">kwargs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">top_p</span><span class="sh">"</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
    <span class="n">top_k</span><span class="o">=</span><span class="n">kwargs</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">top_k</span><span class="sh">"</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">stage1_max_tokens</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">&lt;/think&gt;</span><span class="sh">"</span><span class="p">],</span>  <span class="c1"># Stop when reaching end of reasoning
</span>    <span class="n">include_stop_str_in_output</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># Keep the stop token
</span><span class="p">)</span>

<span class="c1"># Generate CoT reasoning
</span><span class="n">cot_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inference_engine</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span>
    <span class="n">prompts</span><span class="o">=</span><span class="n">vllm_inputs</span><span class="p">,</span>
    <span class="n">sampling_params</span><span class="o">=</span><span class="n">cot_sampling_params</span><span class="p">,</span>
    <span class="n">lora_request</span><span class="o">=</span><span class="n">lora_requests</span><span class="p">,</span>
    <span class="n">use_tqdm</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>Key features</strong>:</p><ul><li><strong>Stop token</strong>: <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code> marks the end of reasoning<li><strong>Configurable length</strong>: Stage 1 can have different max_tokens than Stage 2<li><strong>Sampling</strong>: Uses temperature/top_p for diverse reasoning</ul><h4 id="preparing-stage-2-input"><span class="me-2">Preparing Stage 2 Input</span><a href="#preparing-stage-2-input" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>After CoT generation, construct the Stage 2 prompt:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inference_engine</span><span class="p">.</span><span class="nf">get_tokenizer</span><span class="p">()</span>
<span class="c1"># Prefix marks the transition from reasoning to item ID generation
</span><span class="n">prefix_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">&lt;|sid_begin|&gt;</span><span class="sh">"</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">cot_outputs</span><span class="p">):</span>
    <span class="n">cot_token_ids</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">token_ids</span><span class="p">)</span>
    
    <span class="c1"># Filter out-of-vocabulary (OOV) tokens
</span>    <span class="c1"># This can happen if vLLM generates special internal tokens
</span>    <span class="n">cot_token_ids_filtered</span> <span class="o">=</span> <span class="p">[</span><span class="n">tid</span> <span class="k">for</span> <span class="n">tid</span> <span class="ow">in</span> <span class="n">cot_token_ids</span> <span class="k">if</span> <span class="n">tid</span> <span class="o">&lt;</span> <span class="n">vocab_size</span><span class="p">]</span>
    
    <span class="n">cot_responses</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cot_token_ids_filtered</span><span class="p">)</span>
    
    <span class="c1"># Construct Stage 2 prompt: [Original Prompt] + [CoT] + [Transition Prefix]
</span>    <span class="n">original_prompt_ids</span> <span class="o">=</span> <span class="n">vllm_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">new_prompt_ids</span> <span class="o">=</span> <span class="n">original_prompt_ids</span> <span class="o">+</span> <span class="n">cot_token_ids_filtered</span> <span class="o">+</span> <span class="n">prefix_ids</span>
    
    <span class="n">stage2_input</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">:</span> <span class="n">new_prompt_ids</span><span class="p">}</span>
    <span class="c1"># Preserve multi-modal data if present (e.g., images)
</span>    <span class="k">if</span> <span class="sh">"</span><span class="s">multi_modal_data</span><span class="sh">"</span> <span class="ow">in</span> <span class="n">vllm_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">stage2_input</span><span class="p">[</span><span class="sh">"</span><span class="s">multi_modal_data</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">vllm_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">"</span><span class="s">multi_modal_data</span><span class="sh">"</span><span class="p">]</span>
    
    <span class="n">stage2_inputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">stage2_input</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>Important steps</strong>:</p><ol><li><strong>OOV filtering</strong>: Remove invalid token IDs that may be generated<li><strong>Prompt composition</strong>: Concatenate original prompt + CoT + transition marker<li><strong>Multi-modal preservation</strong>: Keep any images or other modalities</ol><h4 id="stage-2-beam-search-for-item-ids"><span class="me-2">Stage 2: Beam Search for Item IDs</span><a href="#stage-2-beam-search-for-item-ids" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The second stage uses beam search to generate high-quality item sequences:</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
</pre><td class="rouge-code"><pre><span class="n">beam_params</span> <span class="o">=</span> <span class="nc">BeamSearchParams</span><span class="p">(</span>
    <span class="n">beam_width</span><span class="o">=</span><span class="n">beam_width</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens_item</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Call beam search (aligned with standard implementation)
</span><span class="n">item_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inference_engine</span><span class="p">.</span><span class="nf">beam_search</span><span class="p">(</span>
    <span class="n">prompts</span><span class="o">=</span><span class="n">stage2_inputs</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="n">beam_params</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># a relative complex post processing logic for the beam search result
</span><span class="k">if</span> <span class="n">return_all_beams</span><span class="p">:</span>
    <span class="c1"># Return all beams, expand output
</span>    <span class="c1"># Output will be exactly batch_size * n_beams_to_return (pad if needed)
</span>    <span class="n">expanded_idx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">beam_indices</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Track which beam index within each prompt
</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">item_outputs</span><span class="p">):</span>
        <span class="c1"># Prompt length including CoT + Prefix
</span>        <span class="n">stage2_prompt_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">stage2_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">original_prompt_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vllm_inputs</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">"</span><span class="s">prompt_token_ids</span><span class="sh">"</span><span class="p">])</span>

        <span class="c1"># Get top n beams for this prompt, pad if not enough
</span>        <span class="n">num_seqs</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">sequences</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">seq_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_beams_to_return</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">seq_idx</span> <span class="o">&lt;</span> <span class="n">num_seqs</span><span class="p">:</span>
                <span class="c1"># this naming is not good, it is just get the beam_id's response
</span>                <span class="n">best_seq</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">sequences</span><span class="p">[</span><span class="n">seq_idx</span><span class="p">]</span>
                <span class="n">full_seq</span> <span class="o">=</span> <span class="n">best_seq</span><span class="p">.</span><span class="n">tokens</span>
                <span class="c1"># Response = full_seq - original_prompt (not stage2_prompt!)
</span>                <span class="n">response_ids</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[</span><span class="n">original_prompt_len</span><span class="p">:]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Pad with first beam's result if not enough beams
</span>                <span class="n">best_seq</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">full_seq</span> <span class="o">=</span> <span class="n">best_seq</span><span class="p">.</span><span class="n">tokens</span>
                <span class="n">response_ids</span> <span class="o">=</span> <span class="n">full_seq</span><span class="p">[</span><span class="n">original_prompt_len</span><span class="p">:]</span>
            <span class="n">response</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">response_ids</span><span class="p">)</span>
            <span class="n">expanded_idx</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
            <span class="n">beam_indices</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">seq_idx</span><span class="p">)</span>

    <span class="c1"># Expand idx, attention_mask, position_ids to match expanded output
</span>    <span class="n">idx</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="n">expanded_idx</span><span class="p">]</span>  <span class="c1"># (batch_size * n, prompt_length)
</span>    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="p">[</span><span class="n">expanded_idx</span><span class="p">]</span>
    <span class="n">position_ids</span> <span class="o">=</span> <span class="n">position_ids</span><span class="p">[</span><span class="n">expanded_idx</span><span class="p">]</span>

    <span class="c1"># Expand non_tensor_batch to match expanded output
</span>    <span class="n">expanded_non_tensor_batch</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">non_tensor_batch</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
            <span class="n">expanded_non_tensor_batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span><span class="p">[</span><span class="n">expanded_idx</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">expanded_non_tensor_batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">val</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">expanded_idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">expanded_non_tensor_batch</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
    <span class="n">non_tensor_batch</span> <span class="o">=</span> <span class="n">expanded_non_tensor_batch</span>

    <span class="c1"># Store beam indices for reference
</span>    <span class="n">non_tensor_batch</span><span class="p">[</span><span class="sh">"</span><span class="s">_beam_indices</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">beam_indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>  <span class="c1"># Update batch_size
</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[TwoStage] Expanded output: original_bs=</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">item_outputs</span><span class="p">)</span><span class="si">}</span><span class="s">, expanded_bs=</span><span class="si">{</span><span class="n">batch_size</span><span class="si">}</span><span class="s">, n_beams=</span><span class="si">{</span><span class="n">n_beams_to_return</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="bp">...</span>
    
    <span class="n">seq</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">idx</span><span class="p">,</span> <span class="n">response</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><p><strong>Key considerations</strong>:</p><ul><li><strong>Response extraction</strong>: Use original prompt length, not stage2 prompt (which includes CoT)<li><strong>Padding</strong>: If fewer beams than requested, replicate the best beam<li><strong>Metadata</strong>: Track beam indices for reward computation or reranking</ul><p>This two-stage approach enables:</p><ol><li><strong>Better interpretability</strong>: CoT reasoning explains the recommendation<li><strong>Higher quality</strong>: Beam search explores multiple item sequence candidates<li><strong>Flexibility</strong>: Can train rewards on both reasoning quality and recommendation accuracy</ol><h3 id="onerec_recipepy"><span class="me-2">onerec_recipe.py</span><a href="#onerec_recipepy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>This module contains domain-specific components for the recommendation task:</p><ul><li><strong>Dataloader</strong>: Loads user interaction histories, candidate items, and formats prompts<li><strong>Reward computation</strong>: Calculates rewards based on relevance, diversity, and other recommendation metrics</ul><p>These components are task-specific and can be customized for different recommendation scenarios.</p><hr /><h2 id="summary"><span class="me-2">Summary</span><a href="#summary" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>VERL provides a powerful and flexible infrastructure for distributed RL training through clean abstractions:</p><ol><li><strong>RayWorkerGroup</strong>: Simplifies Ray actor management with automatic coordination, smart data distribution, and placement group handling<li><strong>@register decorator</strong>: Enables declarative distributed method execution with automatic dispatch/collect patterns<li><strong>Hybrid workers</strong>: ActorRolloutRefWorker supports multiple roles (actor/rollout/ref) for resource efficiency<li><strong>FSDP2 + vLLM integration</strong>: Seamlessly transitions between distributed training and efficient inference<li><strong>PPO implementation</strong>: Complete with dynamic batching, dual-clip loss, and advantage estimation<li><strong>RayPPOTrainer</strong>: Orchestrates the entire training process across multiple worker groups</ol><p>OpenOneRec demonstrates how to extend VERL for domain-specific needs with minimal code changes, showcasing the framework’s extensibility. The two-stage generation approach (CoT + beam search) illustrates how advanced generation strategies can be integrated into the RL training loop.</p><h2 id="references"><span class="me-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li><a href="https://github.com/volcengine/verl">VERL GitHub Repository</a><li><a href="https://github.com/pyemma/OpenOneRec">OpenOneRec GitHub Repository</a><li><a href="https://arxiv.org/abs/1707.06347">PPO Paper</a><li><a href="https://arxiv.org/pdf/1912.09729">Dual-Clip PPO Paper</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/machine-learning/">Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/verl/" class="post-tag no-text-decoration" >verl</a> <a href="/tags/rl-infra/" class="post-tag no-text-decoration" >rl-infra</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Learning%20VERL%20Part%201%20-%20A%20Perspective%20from%20OpenOneRec%20-%20Coding%20Monkey&url=https%3A%2F%2Fpyemma.github.io%2FOpenOneRec-RL%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Learning%20VERL%20Part%201%20-%20A%20Perspective%20from%20OpenOneRec%20-%20Coding%20Monkey&u=https%3A%2F%2Fpyemma.github.io%2FOpenOneRec-RL%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpyemma.github.io%2FOpenOneRec-RL%2F&text=Learning%20VERL%20Part%201%20-%20A%20Perspective%20from%20OpenOneRec%20-%20Coding%20Monkey" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 mb-5 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/OpenOneRec-RL/">Learning VERL Part 1 - A Perspective from OpenOneRec</a><li class="text-truncate lh-lg"> <a href="/Recommendation-Paper-2025-Review/">My 2025 Recommendation System Paper Summary</a><li class="text-truncate lh-lg"> <a href="/FSDP2-Code-Walk/">FSDP2 Under the Hood - A Deep Dive into PyTorch's Fully Sharded Data Parallel Implementation</a><li class="text-truncate lh-lg"> <a href="/A-Random-Walk-Down-Recsys/">A Random Walk Down Recsys - Part 1</a><li class="text-truncate lh-lg"> <a href="/Recsys-2025-Paper-Summary/">Recsys 2025 Paper Summary</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine-learning-design</a></div></section></div><section id="toc-wrapper" class="d-none ps-0 pe-4"><h2 class="panel-heading ps-3 mb-2">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/A-Random-Walk-Down-Recsys/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1768723200" data-df="ll" > Jan 18, 2026 </time><h4 class="pt-0 my-2">A Random Walk Down Recsys - Part 1</h4><div class="text-muted"><p>This is a new series of blog beyond my conference paper reading blog, in which I would summarize the paper that I found interesting form Arxiv IR section and share my learnings. In this first blog...</p></div></div></a></article><article class="col"> <a href="/Recommendation-Paper-2025-Review/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1767340800" data-df="ll" > Jan 2, 2026 </time><h4 class="pt-0 my-2">My 2025 Recommendation System Paper Summary</h4><div class="text-muted"><p>In this post, I would like to share some insights from the paper I have read in year 2025 and summarize some trends over the year. The One The best work I enjoyed this year is the One-series from...</p></div></div></a></article><article class="col"> <a href="/Recsys-2025-Paper-Summary/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1764230400" data-df="ll" > Nov 27, 2025 </time><h4 class="pt-0 my-2">Recsys 2025 Paper Summary</h4><div class="text-muted"><p>In this post, I would like to summary the paper from Recsys 2025 and share some of my learnings. We would cover several topics such as sequence modeling, cross domain learning as well as LLM integr...</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/A-Random-Walk-Down-Recsys/" class="btn btn-outline-primary" aria-label="Older" ><p>A Random Walk Down Recsys - Part 1</p></a><div class="btn btn-outline-primary disabled" aria-label="Newer"><p>-</p></div></nav><div id="disqus_thread"><p class="text-center text-muted small">Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script type="text/javascript"> var disqus_config = function () { this.page.url = 'https://pyemma.github.io/OpenOneRec-RL/'; this.page.identifier = '/OpenOneRec-RL/'; };var disqus_observer = new IntersectionObserver( function (entries) { if (entries[0].isIntersecting) { (function () { var d = document, s = d.createElement('script'); s.src = 'https://pyemma.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); disqus_observer.disconnect(); } }, { threshold: [0] } ); disqus_observer.observe(document.getElementById('disqus_thread'));function reloadDisqus() { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) {if (typeof DISQUS === 'undefined') { return; } if (document.readyState == 'complete') { DISQUS.reset({ reload: true, config: disqus_config }); } } } if (document.getElementById('mode-toggle')) { window.addEventListener('message', reloadDisqus); } </script><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://github.com/pyemma">Coding Monkey</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.1.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine learning design</a> <a class="post-tag btn btn-outline-primary" href="/tags/recommendation-system/">recommendation-system</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm4rec/">llm4rec</a> <a class="post-tag btn btn-outline-primary" href="/tags/system-design/">system design</a> <a class="post-tag btn btn-outline-primary" href="/tags/user-sequence-modeling/">user-sequence-modeling</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-system/">distributed system</a> <a class="post-tag btn btn-outline-primary" href="/tags/distributed-training/">distributed-training</a> <a class="post-tag btn btn-outline-primary" href="/tags/embeddings/">embeddings</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">llm</a> <a class="post-tag btn btn-outline-primary" href="/tags/machine-learning-design/">machine-learning-design</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.13/dayjs.min.js,npm/dayjs@1.11.13/locale/en.js,npm/dayjs@1.11.13/plugin/relativeTime.js,npm/dayjs@1.11.13/plugin/localizedFormat.js,npm/tocbot@4.29.0/dist/tocbot.min.js"></script> <script src="/assets/js/dist/post.min.js"></script> <script src="/assets/js/data/mathjax.js"></script> <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-M1GM2SJR6M"></script> <script> document.addEventListener('DOMContentLoaded', function (event) { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-M1GM2SJR6M'); }); </script> <script>SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{snippet}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
